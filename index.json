[{"content":"はじめに 本記事のカバー画像は Charles Deluvio による Unsplash からの画像です。\n今年は良くも悪くも変化のあった年でした。特に 1 月から暗雲立ち込めすぎて終わった\u0026hellip;と思ってたことを思い出しました。。ｗ 来年の自分が今を振り返れるように、今年始めからの記憶を引っ張り出しながら総括しました。ただ今年の初めの方の記憶については大分薄れていて覚えてる限り記す感じで。。\n今年問わず作ったものは Tech ページに、技術記事については RSS Feeds にまとめてあります。\n出来事 1 月  400 万をドブに捨てることとなる (詳細についてはいつかアウトプットしたい)  諸々人生を真剣に考え始めるキッカケとなる ただそんな中でも自費で M1 Mac mini を購入していた\u0026hellip;w   Rust 入門のため Actix Web で Gyazo の利用者向けウェブアプリ作ってた  Bloggimg ってやつで知見を 記事 として残してた Bloggimg をデプロイした際に得た知見は この記事 で残してた 得た知見は記事として残す行動が習慣化してきた    2 月  テックブログを Vercel 上に Team Blog Hub 使って開設した  その際の知見を この記事 でまとめてた    3 月  仕事で初めて Gatling を使った  その際の知見を まとめた記事 当時を思い出すと手触りは悪くなかったし、出力されるレポートも見やすかったけど k6 + Grafana のほうが慣れればカスタマイズ性も含めて良い印象   Zenn の記事を DEV に共有するための GitHub Actions を開発してた -\u0026gt; sync-zenn-with-dev-action  DEV のアカウント作ってたけど全く使っていなかったので有効活用のためクロスポストできるようにしたいがモチベだった気がする   負荷テスト対策のためのアレコレを記事にまとめていた  [Tips] Jest で private readonly な値をモックする方法 - Qiita 📔 PlayFab の API 制限に引っかかった - Qiita   Azure + PlayFab で初めて開発したけど、それなりに落とし穴にハマったりコンソールが個人的に見辛かったりしてマイクロソフトェ\u0026hellip; になってた記憶\u0026hellip;w  4 月  Twitter DM や Lapras でちゃんとしたメッセージをいただけるようになり転職を意識し始める  自分の技術者としての能力が外の世界で本当に通用するのか、試したくなってきた時期だった気がする 個人的にも気になっていた企業様からメッセージいただけたりした感じで嬉しかった   技術教材を作ってみないかのお誘いが Twitter DM で来てた  めちゃくちゃしっかりと記事を見てくださって感想をいただけたのと、ドキュメント作成能力は今後絶対に向上させたいスキルの 1 つだったので、受けさせていただくことに決めた   SecretSky を VR で見てめっちゃ盛り上がってた 人生初の履歴書作成が数年分の経歴が溜まっていたので、めちゃくちゃ大変すぎて疲れ果てていた  転職の意思がなかったとしても定期的に経歴の欄を更新していくことを決意    5 月  GameCI で Unity の CI 環境構築してみてた  この記事 に知見まとめてた 大規模プロジェクトでは GitHub Actions で GameCI 使うときは、セルフホストランナー 必須だなと感じた   転職することを決意し、転職先が決まり 8 月から働けることになる  転職のための作業に時間を割きたくなかったことと、最初から興味がそそられるオファーを色々いただけたのでスグに決められた    6 月  転職することを会社にご報告した  大学生の頃からの関わりも含めると 10 年近く期間があるのでめちゃくちゃエモくなった   諸々仕事も落ち着いた段階だったけど、引き継ぎ項目が多かったので 2 ヶ月後の 8 月から転職先で働かせていただくことになる  7 月  エンダーリリーズ にハマり倒してプラチナをゲットする 有給休暇で 1 ヶ月休んでたけど、パートナーに開発の勉強を教えててほぼ自分の時間はなかった\u0026hellip;  あと長期休暇を取ってみて、今度もしまたそういった機会があったら半年くらい休みたいなと思った    8 月  転職先で仕事が始まる Unity + OAuth2.0 関連をやり始める モバイルアプリでセキュアに情報を保存するための知見を 記事 で残してた  9 月  API インフラバックエンド関連をやり始める ステートフルなインフラバックエンドをやり始める  大規模なステートフルなインフラバックエンド関連の経験が皆無だったので新鮮で楽しいになる   ロストジャッジメントにハマり倒す。ストーリーもめっちゃ好きでプラチナを取る  10 月  Unity + OAuth2.0 関連が一旦落ち着く とにかくステートフルなインフラバックエンド関連やってた  想定利用者数が想像より桁違いに多かったので色々焦りながら勉強しまくりの日々を過ごす 早々に負荷テストのクラウド環境を整備して開発の度に回してパフォーマンス計測したりしてた インフラのみならずアプリケーション開発も並行して担当していたため、シームレスに開発が進められたのは良かったが、やること多すぎワロタ状態に突入    11 月  我武者羅にステートフルなインフラバックエンド関連やってた  おかげでスケールイン・アウトから監視、諸々含むシステムの安定性向上がようやく安心できるレベルまで持っていけた 知見をメモる時間ももったいないになってしまったので、とりあえず諸々 Notion に書きなぐってた    12 月  今まで培った知見を何もアウトプットしていないことに気づき焦る  来年から頑張ろうになる (えっ   とりあえずアドベントカレンダーには参戦した  ECS Fargate のメトリクスを Prometheus Agent 使って AMP に送って Grafana で監視する   4 月から進めていた技術教材の初稿ができて推敲しまくりステータスに入る  おわりに 今年は割と波 \u0026amp; 緩急のあった 1 年でした。来年は今年の変化を形にしつつものにしていく感じで行きたいです。記事にできそうな技術ネタもめっさ溜まってるので放出しまくりたいです。強めのエモネタもいくつかあるので、良さげなタイミングで放出したいです。\nこの記事を書いている人物のプロフィールは Profile からご確認いただけます。何かございましたら Contact からお気軽にご連絡くださいませ。\nそれではみなさま良いお年を！！😆\n","permalink":"https://nikaera.com/archives/summarize-2021/","summary":"はじめに 本記事のカバー画像は Charles Deluvio による Unsplash からの画像です。\n今年は良くも悪くも変化のあった年でした。特に 1 月から暗雲立ち込めすぎて終わった\u0026hellip;と思ってたことを思い出しました。。ｗ 来年の自分が今を振り返れるように、今年始めからの記憶を引っ張り出しながら総括しました。ただ今年の初めの方の記憶については大分薄れていて覚えてる限り記す感じで。。\n今年問わず作ったものは Tech ページに、技術記事については RSS Feeds にまとめてあります。\n出来事 1 月  400 万をドブに捨てることとなる (詳細についてはいつかアウトプットしたい)  諸々人生を真剣に考え始めるキッカケとなる ただそんな中でも自費で M1 Mac mini を購入していた\u0026hellip;w   Rust 入門のため Actix Web で Gyazo の利用者向けウェブアプリ作ってた  Bloggimg ってやつで知見を 記事 として残してた Bloggimg をデプロイした際に得た知見は この記事 で残してた 得た知見は記事として残す行動が習慣化してきた    2 月  テックブログを Vercel 上に Team Blog Hub 使って開設した  その際の知見を この記事 でまとめてた    3 月  仕事で初めて Gatling を使った  その際の知見を まとめた記事 当時を思い出すと手触りは悪くなかったし、出力されるレポートも見やすかったけど k6 + Grafana のほうが慣れればカスタマイズ性も含めて良い印象   Zenn の記事を DEV に共有するための GitHub Actions を開発してた -\u0026gt; sync-zenn-with-dev-action  DEV のアカウント作ってたけど全く使っていなかったので有効活用のためクロスポストできるようにしたいがモチベだった気がする   負荷テスト対策のためのアレコレを記事にまとめていた  [Tips] Jest で private readonly な値をモックする方法 - Qiita 📔 PlayFab の API 制限に引っかかった - Qiita   Azure + PlayFab で初めて開発したけど、それなりに落とし穴にハマったりコンソールが個人的に見辛かったりしてマイクロソフトェ\u0026hellip; になってた記憶\u0026hellip;w  4 月  Twitter DM や Lapras でちゃんとしたメッセージをいただけるようになり転職を意識し始める  自分の技術者としての能力が外の世界で本当に通用するのか、試したくなってきた時期だった気がする 個人的にも気になっていた企業様からメッセージいただけたりした感じで嬉しかった   技術教材を作ってみないかのお誘いが Twitter DM で来てた  めちゃくちゃしっかりと記事を見てくださって感想をいただけたのと、ドキュメント作成能力は今後絶対に向上させたいスキルの 1 つだったので、受けさせていただくことに決めた   SecretSky を VR で見てめっちゃ盛り上がってた 人生初の履歴書作成が数年分の経歴が溜まっていたので、めちゃくちゃ大変すぎて疲れ果てていた  転職の意思がなかったとしても定期的に経歴の欄を更新していくことを決意    5 月  GameCI で Unity の CI 環境構築してみてた  この記事 に知見まとめてた 大規模プロジェクトでは GitHub Actions で GameCI 使うときは、セルフホストランナー 必須だなと感じた   転職することを決意し、転職先が決まり 8 月から働けることになる  転職のための作業に時間を割きたくなかったことと、最初から興味がそそられるオファーを色々いただけたのでスグに決められた    6 月  転職することを会社にご報告した  大学生の頃からの関わりも含めると 10 年近く期間があるのでめちゃくちゃエモくなった   諸々仕事も落ち着いた段階だったけど、引き継ぎ項目が多かったので 2 ヶ月後の 8 月から転職先で働かせていただくことになる  7 月  エンダーリリーズ にハマり倒してプラチナをゲットする 有給休暇で 1 ヶ月休んでたけど、パートナーに開発の勉強を教えててほぼ自分の時間はなかった\u0026hellip;  あと長期休暇を取ってみて、今度もしまたそういった機会があったら半年くらい休みたいなと思った    8 月  転職先で仕事が始まる Unity + OAuth2.","title":"📔 2021年の振り返り"},{"content":"はじめに この記事は AWS Advent Calendar 2021 の 5 日目の記事です。\nFargate で Node.js アプリのメトリクスを Prometheus Agent をサイドカーコンテナとして動かして、Amazon Managed Service for Prometheus (AMP) に送信して Grafana で見られるようにしてみました。\nちなみに Promethus Agent はまだ 実験的な機能 なため、実務での利用は推奨しません。\n本記事の環境構築には AWS CDK を利用しています。\n動作環境  Node.js v16.13.0 AWS CDK 2.0.0 (build 4b6ce31) Prometheus 2.32.1  環境構築 早速環境構築を進めていきます。まだ AMP については CDK から操作できないようでしたので、ワークスペースの作成については AWS コンソールから手動で行います。(2021/12/06)\naws-aps を利用することで AWS CDK からでも Amazon Managed Service for Prometheus のワークスペースを作成すること確認できましたので、そちらの利用を推奨いたします\u0026hellip; 🙇🙇\nlib/prometheus-agent-test-stack.ts のコードも修正済みで AWS CDK で Amazon Managed Service for Prometheus のワークスペースを作成するように編集しました。(2021/12/18 追記)\n手動で AMP のワークスペースを作成する手順 まず、AMP のコンソール画面 に遷移してワークスペースを作成します。\n1. AMP のコンソール画面 からワークスペース作成画面に遷移する\n2. AMP のワークスペースを作成する\n3. AMP のワークスペース作成完了を確認すると同時に設定値を控えておく\nAMP ワークスペースの エンドポイント - リモート書き込み URL は、Prometheus Agent で AMP にデータ送信する際や、Grafana でデータソースを登録する際などに必要となるため控えておきます。\nAWS CDK で環境構築する CDK で構築作業を進めます。まずは下記コマンドで CDK プロジェクトを作成します。使用言語は TypeScript を選択します。\nmkdir prometheus-agent-test \u0026amp;\u0026amp; cd prometheus-agent-test cdk init --language typescript まず CDK でインフラ構築を進めていく前に、メトリクス収集テスト用の Node.js アプリを準備します。\nECS Fargate で動かす Node.js アプリを準備する prom-client を利用して、Node.js のメトリクスが取得できるだけの Node.js アプリを準備します。prometheus-agent-test フォルダで下記コマンドを実行します。\nmkdir metrics-app \u0026amp;\u0026amp; cd metrics-app npm init -y npm install --save prom-client 次に metrics-app フォルダ内に index.js を作成して下記を編集します。\n// metrics-app/index.js \u0026#34;use strict\u0026#34;; const http = require(\u0026#34;http\u0026#34;); const server = http.createServer(); const client = require(\u0026#34;prom-client\u0026#34;); const register = new client.Registry(); // 5秒間隔でメトリクスを取得する client.collectDefaultMetrics({ register, timeout: 5 * 1000 }); server.on(\u0026#34;request\u0026#34;, async function (req, res) { // /metrics にアクセスしたら、Prometheus のレポートを返す  if (req.url === \u0026#34;/metrics\u0026#34;) { res.setHeader(\u0026#34;Content-Type\u0026#34;, register.contentType); const metrics = await register.metrics(); return res.end(metrics); } else { return res.writeHead(404, { \u0026#34;Content-Type\u0026#34;: \u0026#34;text/plain\u0026#34; }); } }); server.listen(8080); node index.js コマンドを実行して http://localhost:8080/metrics にアクセスしてみます。下記のように各種メトリクスが出力されている様子が確認できれば OK です。\nPrometheus のレポートが正常に出力されている様子\n今回は ECS 上で Node.js アプリを動作させるため、Dockerfile も作成します。\n# metrics-app/DockerfileFROMpublic.ecr.aws/docker/library/node:16-alpine3.12 AS builderEXPOSE8080WORKDIR/usr/src/appCOPY package*.json ./RUN npm install --max-old-space-size=4096COPY . .CMD [ \u0026#34;node\u0026#34;, \u0026#34;index.js\u0026#34; ]上記 Dockerfile 作成後、再び動作検証のため下記コマンドを実行してから、http://localhost:8080/metrics にアクセスしてみます。\ndocker build -t prometheus-agent-test/metrics-app . docker run -p 8080:8080 prometheus-agent-test/metrics-app:latest 先ほどと同様に http://localhost:8080/metrics アクセス時に各種メトリクスが出力されている様子を確認できれば OK です。\nNode.js アプリを監視する Prometheus Agent を準備する まずは Prometheus 関連ファイルを配置するためのフォルダを作成します。prometheus-agent-test フォルダ内で下記コマンドを実行します。\nmkdir prometheus-agent \u0026amp;\u0026amp; cd prometheus-agent 次に Prometheus の設定テンプレートファイルを作成します。テンプレートファイルは sed を利用して中身の __TASK_ID__ および __REMOTE_WRITE_URL__ を書き換えて利用します。\n# prometheus-agent/prometheus.tmpl.yml global: scrape_interval: 5s external_labels: monitor: \u0026#34;prometheus\u0026#34; scrape_configs: - job_name: \u0026#34;prometheus-agent-test\u0026#34; static_configs: - targets: [\u0026#34;localhost:8080\u0026#34;] labels: # デフォルトの localhost:8080 がインスタンスとして利用されると、 # メトリクスの判別がしづらくなるため ECS Task の ID を利用する instance: \u0026#34;__TASK_ID__\u0026#34; remote_write: # AMP ワークスペース作成時に控えておいた、 # `エンドポイント - リモート書き込み URL` を設定する箇所 - url: \u0026#34;__REMOTE_WRITE_URL__\u0026#34; sigv4: region: ap-northeast-1 queue_config: max_samples_per_send: 1000 max_shards: 200 capacity: 2500 設定ファイルの作成が完了したら、テンプレートファイルを利用して Prometheus の設定ファイルを作成し、Prometheus Agent を起動させるためのシェルスクリプトを作成します。\n# prometheus-agent/docker-entrypoint.sh #!/bin/sh while [ -z \u0026#34;$taskId\u0026#34; ] do # ECS Fargate で起動したタスク ID を取得する taskId=$(curl --silent ${ECS_CONTAINER_METADATA_URI}/task | jq -r \u0026#39;.TaskARN | split(\u0026#34;/\u0026#34;) | .[-1]\u0026#39;) echo \u0026#34;waiting...\u0026#34; sleep 1 done echo \u0026#34;taskId: ${taskId}\u0026#34; echo \u0026#34;remoteWriteUrl: ${REMOTE_WRITE_URL}\u0026#34; # タスク ID `taskId` および、環境変数 `REMOTE_WRITE_URL` で、 # Prometheus のテンプレートファイル `prometheus.tmpl.yml` の内容を書き換え、 # その結果を `/etc/prometheus/prometheus.yml` に出力する cat /etc/prometheus/prometheus.tmpl.yml | \\  sed \u0026#34;s/__TASK_ID__/${taskId}/g\u0026#34; | \\  sed \u0026#34;s\u0026gt;__REMOTE_WRITE_URL__\u0026gt;${REMOTE_WRITE_URL}\u0026gt;g\u0026#34; \u0026gt; /etc/prometheus/prometheus.yml # --enable-feature=agent で Prometheus を Agent モードで起動する # Prometheus のコンフィグファイルには上記で出力した `/etc/prometheus/prometheus.yml` を利用する /usr/local/bin/prometheus \\  --enable-feature=agent \\  --config.file=/etc/prometheus/prometheus.yml \\  --web.console.libraries=/etc/prometheus/console_libraries \\  --web.console.templates=/etc/prometheus/consoles これで Prometheus Agent 起動のための準備は整ったため、最後に Dockerfile を準備します。ちなみに Prometheus Agent は v2.32.0 以降で利用可能です。本記事では v2.32.1 を利用します。\n# prometheus-agent/DockerfileFROM--platform=arm64 alpine:3.15ADD prometheus.tmpl.yml /etc/prometheus/RUN apk add --update --no-cache jq sed curl# ARM64 で動作する Prometheus v2.32.1 を curl でダウンロード展開するRUN curl -sL -O https://github.com/prometheus/prometheus/releases/download/v2.32.1/prometheus-2.32.1.linux-arm64.tar.gzRUN tar -zxvf prometheus-2.32.1.linux-arm64.tar.gz \u0026amp;\u0026amp; rm prometheus-2.32.1.linux-arm64.tar.gz# `prometheus` コマンドを `/usr/local/bin/prometheus` に移動するRUN mv prometheus-2.32.1.linux-arm64/prometheus /usr/local/bin/prometheusCOPY ./docker-entrypoint.sh /RUN chmod +x /docker-entrypoint.shCMD [\u0026#34;/docker-entrypoint.sh\u0026#34;]ここまでで CDK でインフラ整備を進めていくための下準備は完了です。\nECS Fargate 上で Node.js アプリおよび Prometheus Agent を動作させる あとは CDK で ECS Fargate 上で Node.js アプリおよび Prometheus Agent、Grafana を動作させるための環境を整備していきます。\nlib/prometheus-agent-test-stack.ts の内容を書き換えます。\n// lib/prometheus-agent-test-stack.ts import { Construct } from \u0026#34;constructs\u0026#34;; import { Stack, StackProps, aws_ecs as ecs, aws_logs as logs, aws_aps as aps, aws_ecs_patterns as ecs_patterns, aws_iam as iam, aws_elasticloadbalancingv2 as elbv2, Duration, CfnOutput, } from \u0026#34;aws-cdk-lib\u0026#34;; export class PrometheusAgentTestStack extends Stack { constructor(scope: Construct, id: string, props?: StackProps) { super(scope, id, props); // Node.js アプリに ecs_patterns.ApplicationLoadBalancedFargateService を利用して ALB 経由でアクセス可能にする  const projectName = \u0026#34;prometheus-agent-test\u0026#34;; const fargateService = new ecs_patterns.ApplicationLoadBalancedFargateService( this, `${projectName}-fargate-service`, { serviceName: `${projectName}-fargate-service`, cpu: 256, desiredCount: 3, listenerPort: 80, taskImageOptions: { family: `${projectName}-taskdef`, image: ecs.ContainerImage.fromAsset(\u0026#34;metrics-app\u0026#34;), containerPort: 8080, logDriver: ecs.LogDrivers.awsLogs({ streamPrefix: `/${projectName}/metrics-app`, logRetention: logs.RetentionDays.ONE_DAY, }), }, cluster: new ecs.Cluster(this, `${projectName}-cluster`, { clusterName: `${projectName}-cluster`, }), memoryLimitMiB: 512, } ); fargateService.targetGroup.configureHealthCheck({ path: \u0026#34;/metrics\u0026#34;, timeout: Duration.seconds(8), interval: Duration.seconds(10), healthyThresholdCount: 2, unhealthyThresholdCount: 4, healthyHttpCodes: \u0026#34;200\u0026#34;, }); // 本質ではないが、Gravition2 で動作させたいために RuntimePlatform のプロパティを上書きしている  const fargateServiceTaskdef = fargateService.taskDefinition.node .defaultChild as ecs.CfnTaskDefinition; fargateServiceTaskdef.addPropertyOverride(\u0026#34;RuntimePlatform\u0026#34;, { CpuArchitecture: \u0026#34;ARM64\u0026#34;, OperatingSystemFamily: \u0026#34;LINUX\u0026#34;, }); // AMP への書き込み権限を付与する  fargateService.taskDefinition.taskRole.addManagedPolicy( iam.ManagedPolicy.fromAwsManagedPolicyName( \u0026#34;AmazonPrometheusRemoteWriteAccess\u0026#34; ) ); // (2021/12/18) Amazon Managed Service for Prometheus のワークスペースを作成して、Prometheus の remote-write URL を取得する  const apsWorkspace = new aps.CfnWorkspace( this, `${projectName}-prom-workspace`, { alias: `${projectName}-prom-workspace`, } ); const apsWorkspaceRemoteUrl = `${apsWorkspace.attrPrometheusEndpoint}api/v1/remote_write`; // (2021/12/18) 本記事で頻出する \u0026#34;エンドポイント - リモート書き込み URL\u0026#34; をコンソールに出力する  new CfnOutput(this, \u0026#34;prom-remote-write-url\u0026#34;, { value: apsWorkspaceRemoteUrl, description: \u0026#34;Prometheus Workspace の remote-write URL\u0026#34;, exportName: \u0026#34;PromRemoteWriteURL\u0026#34;, }); // AMP へメトリクス情報を送信するための Prometheus Agent コンテナを追加する  const containerName = `${projectName}-prometheus-agent`; fargateService.taskDefinition.addContainer(containerName, { containerName, image: ecs.ContainerImage.fromAsset(\u0026#34;prometheus-agent\u0026#34;), memoryReservationMiB: 128, environment: { // (2021/12/18) CDK 経由で作成した Prometheus の remote-write URL を設定する  REMOTE_WRITE_URL: apsWorkspaceRemoteUrl, }, logging: new ecs.AwsLogDriver({ streamPrefix: `/${projectName}/prometheus-agent`, logRetention: logs.RetentionDays.ONE_DAY, }), }); // Grafana のタスク定義を作成する  const grafanaDashboardTaskDefinition = new ecs.FargateTaskDefinition( this, `${projectName}-grafana-taskdef`, { family: `${projectName}-grafana-taskdef`, } ); // Grafana のタスクが Prometheus Query を叩けるように権限付与する  grafanaDashboardTaskDefinition.taskRole.addManagedPolicy( iam.ManagedPolicy.fromAwsManagedPolicyName(\u0026#34;AmazonPrometheusQueryAccess\u0026#34;) ); // Grafana のコンテナを追加する。パスプレフィクスには dashboard を設定する  const grafanaDashboardContainerName = `${projectName}-grafana-dashboard`; grafanaDashboardTaskDefinition.addContainer(grafanaDashboardContainerName, { containerName: grafanaDashboardContainerName, image: ecs.ContainerImage.fromRegistry(\u0026#34;public.ecr.aws/ubuntu/grafana\u0026#34;), environment: { AWS_SDK_LOAD_CONFIG: \u0026#34;true\u0026#34;, GF_AUTH_SIGV4_AUTH_ENABLED: \u0026#34;true\u0026#34;, GF_SERVER_SERVE_FROM_SUB_PATH: \u0026#34;true\u0026#34;, GF_SERVER_ROOT_URL: \u0026#34;%(protocol)s://%(domain)s/dashboard\u0026#34;, }, portMappings: [{ containerPort: 3000 }], memoryLimitMiB: 512, logging: new ecs.AwsLogDriver({ streamPrefix: `/${projectName}/grafana-dashboard`, logRetention: logs.RetentionDays.ONE_DAY, }), }); const grafanaDashboardServiceName = `${projectName}-grafana-dashboard-service`; const grafanaDashboardService = new ecs.FargateService( this, grafanaDashboardServiceName, { serviceName: grafanaDashboardServiceName, cluster: fargateService.cluster, taskDefinition: grafanaDashboardTaskDefinition, desiredCount: 1, } ); // Grafana のタスクを ALB のターゲットグループに紐づける  fargateService.listener.addTargets( `${projectName}-grafana-dashboard-target`, { priority: 1, conditions: [elbv2.ListenerCondition.pathPatterns([\u0026#34;/dashboard/*\u0026#34;])], healthCheck: { path: \u0026#34;/dashboard/login\u0026#34;, interval: Duration.seconds(10), timeout: Duration.seconds(8), healthyThresholdCount: 2, unhealthyThresholdCount: 3, healthyHttpCodes: \u0026#34;200\u0026#34;, }, port: 3000, protocol: elbv2.ApplicationProtocol.HTTP, targets: [grafanaDashboardService], } ); } } その後、cdk deploy でインフラを構築します。\nCDK によるインフラ構築が正常に実行された時の様子\nデプロイが正常に完了したのを確認したら、Outputs に出力されている PrometheusAgentTestStack.prometheusagenttestfargateserviceServiceURL\u0026lt;識別子\u0026gt; の URL 末尾に /metrics を付与してアクセスしてみます。 出力されている URL のフォーマットは http://\u0026lt;識別子\u0026gt;.ap-northeast-1.elb.amazonaws.com になります。\nつまり、http://\u0026lt;識別子\u0026gt;.ap-northeast-1.elb.amazonaws.com/metrics にアクセスします。\nALB 経由で Node.js アプリにアクセス可能なことを確認する\nまた、Outputs に出力されている PrometheusAgentTestStack.promremotewriteurl は後に利用する エンドポイント - リモート書き込み URL で使用するので控えておきます。\nここまでで AWS CDK でのインフラ構築作業は完了しました。最後に Grafana で AMP のメトリクスを可視化するための作業を進めていきます。\nGrafana で Prometheus (AMP) のメトリクスを可視化する 先ほどの /metrics パスへのアクセス同様、Outputs に出力されている URL の末尾に /dashboard/login を付与してアクセスします。Grafana の初期ユーザおよびパスワードは admin となります。\nつまり、http://\u0026lt;識別子\u0026gt;.ap-northeast-1.elb.amazonaws.com/dashboard/login にアクセスしてみます。\nログイン情報が正しければ、新しいパスワードを設定する画面に遷移するので新たなパスワードを入力してログインを終えます。ログイン後は、Prometheus (AMP) をデータソースとして追加するために下記の操作を行います。\n1. 歯車アイコンをクリックして Data sources をクリックする\n2. Add data source ボタンをクリックする\n3. データソースとして Prometheus を選択する\n4. Prometheus をデータソースとして追加する\nPrometheus (AMP) に送信したメトリクスを Grafana で可視化するための準備が整ったので、実際に Grafana のダッシュボードでメトリクスを可視化してみます。手っ取り早くメトリクスを可視化するため、ダッシュボードには NodeJS Application Dashboard を利用します。\n1. + アイコンをクリックして、Import をクリックする\n2. NodeJS Application Dashboard の ID を入力して Load ボタンをクリックする\n3. 必要な情報を入力して NodeJS Application Dashboard のインポートを完了する\n4. ダッシュボードから Node.js アプリのメトリクスが確認できる\nここまでの手順でメトリクスの可視化は完了しましたが、負荷に応じて実際にメトリクスが変化する様子も確認してみます。Vegeta を利用して、実際に負荷をかけてみます。下記コマンドを実行します。\necho \u0026#39;GET http://\u0026lt;識別子\u0026gt;.ap-northeast-1.elb.amazonaws.com/metrics\u0026#39; | vegeta attack -duration=5s | vegeta report その後、再び Grafana のダッシュボードを見にいきます。負荷をかけた時間帯のみグラフに変化があることを確認できるはずです。\nダッシュボードの CPU 使用率のグラフに変化があったことを確認できる\nおわりに 今回は ECS Fargate のメトリクスを Prometheus Agent で Amazon Managed Service for Prometheus (AMP) に送信し、それを Grafana で可視化する方法について紹介しました。\nECS のサービスでタスクを実行する場合は サービスディスカバリ の利用が可能なため、Prometheus の サービスディスカバリの設定 を行うことで、単一の Prometheus で全てのコンテナのメトリクスを扱うことも可能です。\nまた Node.js アプリを作成する際に利用した prom-client で カスタムメトリクス を作成することで、監視したい項目を自由に増やすことも可能です。\n本記事が ECS Fargate を監視する際の検討材料の 1 つとなれたら幸いです。\n参考リンク  AWS Fargate（サーバーやクラスターの管理が不要なコンテナの使用）| AWS Introducing Prometheus Agent Mode, an Efficient and Cloud-Native Way for Metric Forwarding | Prometheus Amazon Managed Service for Prometheus | フルマネージド Prometheus | Amazon Web Services Grafana: The open observability platform | Grafana Labs AWS クラウド開発キット – アマゾン ウェブ サービス siimon/prom-client: Prometheus client for node.js tsenart/vegeta: HTTP load testing tool and library. It\u0026rsquo;s over 9000! NodeJS Application Dashboard dashboard for Grafana | Grafana Labs  ","permalink":"https://nikaera.com/archives/aws-ecs-fargate-amp-grafana/","summary":"はじめに この記事は AWS Advent Calendar 2021 の 5 日目の記事です。\nFargate で Node.js アプリのメトリクスを Prometheus Agent をサイドカーコンテナとして動かして、Amazon Managed Service for Prometheus (AMP) に送信して Grafana で見られるようにしてみました。\nちなみに Promethus Agent はまだ 実験的な機能 なため、実務での利用は推奨しません。\n本記事の環境構築には AWS CDK を利用しています。\n動作環境  Node.js v16.13.0 AWS CDK 2.0.0 (build 4b6ce31) Prometheus 2.32.1  環境構築 早速環境構築を進めていきます。まだ AMP については CDK から操作できないようでしたので、ワークスペースの作成については AWS コンソールから手動で行います。(2021/12/06)\naws-aps を利用することで AWS CDK からでも Amazon Managed Service for Prometheus のワークスペースを作成すること確認できましたので、そちらの利用を推奨いたします\u0026hellip; 🙇🙇\nlib/prometheus-agent-test-stack.ts のコードも修正済みで AWS CDK で Amazon Managed Service for Prometheus のワークスペースを作成するように編集しました。(2021/12/18 追記)","title":"📔 ECS Fargate のメトリクスを Prometheus Agent 使って AMP に送って Grafana で監視する"},{"content":"はじめに iOS/Android でユーザーの情報をセキュアに扱う必要があったので、調査したところ Android には EncryptedSharedPreferences が存在することを知りました。iOS には Keychain Services が存在します。\n今回は Unity の iOS/Android プラットフォーム上で設定値を保存するための実装を行う必要があったので、Unity から扱えるようネイティブプラグインを作成しました。今後もこういった要望はありそうでしたので、記事として手順や内容を書き記しておくことにしました。\n本記事内で紹介しているコードは下記にアップ済みです。\nhttps://github.com/nikaera/Unity-iOS-Android-SecretManager-Sample\n動作環境  MacBook Air (M1, 2020) Unity 2020.3.15f2 Android 6.0 以上  EncryptedSharedPreferences が使用可能なバージョン    Android のネイティブプラグインを作成する Android 環境ではまず External Dependency Manager for Unity を利用して、Unity の Android ネイティブプラグインで EncryptedSharedPreferences 利用可能にします。\n(追記) Gradle を利用したライブラリのインストール方法 shiena さんにご教授いただいたのですが、こちらの記事のように Gradle を利用することでも簡易にライブラリの取り込みが可能なようでした。\n手順は上記の記事をご参照いただくとして、Gradle を利用する方法で外部ライブラリを取り込む際の Assets/Plugins/Android/mainTemplate.gradle および Assets/Plugins/Android/gradleTemplate.properties は下記になります。\ndependencies { implementation fileTree(dir: \u0026#39;libs\u0026#39;, include: [\u0026#39;*.jar\u0026#39;]) + implementation \u0026#39;androidx.security:security-crypto:1.1.0-alpha03\u0026#39; **DEPS**} android { org.gradle.jvmargs=-Xmx**JVM_HEAP_SIZE**M org.gradle.parallel=true android.enableR8=**MINIFY_WITH_R_EIGHT** + android.useAndroidX=true unityStreamingAssets=.unity3d**STREAMING_ASSETS** **ADDITIONAL_PROPERTIES** Gradle を利用した方法でライブラリを利用される際は、次の External Dependency Manager for Unity で必要なパッケージをインストールする の手順はスキップ可能です。EncryptedSharedPreferences を利用するためのネイティブコードを追加する のステップから進めてください。\nExternal Dependency Manager for Unity を利用する方法だと、取り込み先プロジェクト内でライブラリの競合が発生する恐れがあります。Gradle を利用する方法であれば回避が可能です。1\nExternal Dependency Manager for Unity で必要なパッケージをインストールする External Dependency Manager for Unity をインポートするため unitypackage をダウンロードして、EncryptedSharedPreferences を導入したい Unity プロジェクトを開いてから unitypackage をクリックすることで、External Dependency Manager for Unity を Unity プロジェクトにインポートします。\nUnity プロジェクトの Build Settings からプラットフォームは Android に切り替えておきます。Enable Android Auto-resolution? というダイアログの選択肢はどちらを選んでも構いません。2\nExternal Dependency Manager for Unity で各種パッケージを管理する方法は README に記載がある通り、*Dependencies.xml というファイルを Editor フォルダに配置することで可能になります。\n今回は EncryptedSharedPreferences を導入するため、下記の xml ファイルを Editor フォルダ内に配置します。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;androidPackages\u0026gt; \u0026lt;!-- 本記事ではバージョン 1.1.0-alpha03 を利用している --\u0026gt; \u0026lt;androidPackage spec=\u0026#34;androidx.security:security-crypto:1.1.0-alpha03\u0026#34;\u0026gt; \u0026lt;androidSdkPackageIds\u0026gt; \u0026lt;!-- Google の Maven リポジトリからインストールするため、 extra-google-m2repository を指定する --\u0026gt; \u0026lt;androidSdkPackageId\u0026gt;extra-google-m2repository\u0026lt;/androidSdkPackageId\u0026gt; \u0026lt;/androidSdkPackageIds\u0026gt; \u0026lt;/androidPackage\u0026gt; \u0026lt;/androidPackages\u0026gt; \u0026lt;/dependencies\u0026gt; その後、Unity メニューから Assets -\u0026gt; External Dependency Manager -\u0026gt; Android Resolver -\u0026gt; Force Resolve を選択して、Assets/Editor/AndroidPluginDependencies.xml の内容を元に EncryptedSharedPreferences を利用するのに必要なパッケージを自動で Assets/Plugins/Android フォルダにダウンロードします。\n1. Unity メニューから Assets -\u0026gt; External Dependency Manager -\u0026gt; Android Resolver -\u0026gt; Force Resolve を選択する\n2. 実行に成功すると EncryptedSharedPreferences を利用するのに必要なライブラリ群が Assets/Plugins/Android フォルダに配置される\nここまで来ればあとは Android ネイティブコードを Assets/Plugins/Android フォルダ内に配置して Unity 側から叩けるようにするだけです。\nEncryptedSharedPreferences を利用するためのネイティブコードを追加する 早速下記の Android ネイティブコードを Assets/Plugins/Android/SecretManager.java に配置します。\npackage com.nikaera; import com.unity3d.player.UnityPlayerActivity; import java.lang.Exception; // External Dependency Manager for Unity によって、 // 必要な jar が含まれているため EncryptedSharedPreferences の利用が可能になる import androidx.security.crypto.EncryptedSharedPreferences; import androidx.security.crypto.MasterKey; import android.content.Context; import android.content.SharedPreferences; import android.content.SharedPreferences.Editor; import android.os.Bundle; import android.util.Log; public class SecretManager { private SharedPreferences sharedPreferences; public SecretManager(Context context) { try { // EncryptedSharedPreferences で設定値を保存する際に用いる、  // 暗号鍵を扱うためのラッパークラスをデフォルト設定で作成する  MasterKey masterKey = new MasterKey.Builder(context) .setKeyScheme(MasterKey.KeyScheme.AES256_GCM) .build(); // EncryptedSharedPreferences のインスタンスを生成する  // コンストラクタで作成した masterKey を指定している  this.sharedPreferences = EncryptedSharedPreferences.create( context, context.getPackageName(), masterKey, EncryptedSharedPreferences.PrefKeyEncryptionScheme.AES256_SIV, EncryptedSharedPreferences.PrefValueEncryptionScheme.AES256_GCM ); } catch (Exception e) { e.printStackTrace(); } } /** * 指定したキーで値を保存する関数 * @param key 値を保存する際に用いるキー * @param value 保存したい値 * @return boolean 値の保存に成功したかどうか */ public boolean put(String key, String value) { SharedPreferences.Editor editor = sharedPreferences.edit(); editor.putString(key, value); return editor.commit(); } /** * 指定したキーで保存した値を取得する関数 * `put` 関数で保存した値を取得するのに利用する * @param key 取得したい値のキー * @return string キーに紐づく値、存在しなければ空文字が返却される */ public String get(String key) { return sharedPreferences.getString(key, null); } /** * 指定したキーで値を削除する関数 * @param key 削除したい値のキー * @return boolean 値の削除に成功したかどうか */ public boolean delete(String key) { SharedPreferences.Editor editor = sharedPreferences.edit(); editor.remove(key); return editor.commit(); } } その後、上記を Unity スクリプトから実行可能にするための C# クラスを作成します。本記事ではファイルを Assets/Scripts/EncryptedSharedPreferences.cs に配置します。\nusing UnityEngine; /// \u0026lt;summary\u0026gt; /// 利用するネイティブコードは \u0026lt;c\u0026gt;Assets/Plugins/Android/SecretManager.java\u0026lt;/c\u0026gt; に記載 /// \u0026lt;/summary\u0026gt; /// \u0026lt;remarks\u0026gt; /// \u0026lt;a href=\u0026#34;https://developer.android.com/reference/androidx/security/crypto/EncryptedSharedPreferences\u0026#34;\u0026gt;EncryptedSharedPreferences\u0026lt;/a\u0026gt; /// \u0026lt;/remarks\u0026gt; class EncryptedSharedPreferences { private readonly AndroidJavaObject _secretManager; public EncryptedSharedPreferences() { // コンストラクタで com.nikaera.SecretManager のインスタンス生成を行う  var activity = new AndroidJavaClass(\u0026#34;com.unity3d.player.UnityPlayer\u0026#34;) .GetStatic\u0026lt;AndroidJavaObject\u0026gt;(\u0026#34;currentActivity\u0026#34;); var context = activity.Call\u0026lt;AndroidJavaObject\u0026gt;(\u0026#34;getApplicationContext\u0026#34;); _secretManager = new AndroidJavaObject(\u0026#34;com.nikaera.SecretManager\u0026#34;, context); } public bool Put(string key, string value) { return _secretManager.Call\u0026lt;bool\u0026gt;(\u0026#34;put\u0026#34;, key, value); } public string Get(string key) { return _secretManager.Call\u0026lt;string\u0026gt;(\u0026#34;get\u0026#34;, key); } public bool Delete(string key) { return _secretManager.Call\u0026lt;bool\u0026gt;(\u0026#34;delete\u0026#34;, key); } } あとは用途に応じて下記のようなコードで設定値の保存や取得などを行えます。\n// ... var _sharedPreferences = new EncryptedSharedPreferences(); // name をキーとして値を nikaera で保存する _sharedPreferences.Put(\u0026#34;name\u0026#34;, \u0026#34;nikaera\u0026#34;); // name をキーとして値を取得する var name = _sharedPreferences.Get(\u0026#34;name\u0026#34;); // \u0026#34;nikaera\u0026#34; が出力される Debug.Log(name); //　name をキーとして値を削除する _sharedPreferences.Delete(\u0026#34;name\u0026#34;); // ... iOS のネイティブプラグインを作成する iOS の場合は外部ライブラリを利用しないため、External Dependency Manager for Unity は利用しません。本来であれば Swift で信頼できる外部フレームワークを取り込み利用できると良さそうですが、今回は Objective-C でネイティブプラグインを書いていきます。3\nKeychain Services を利用するためのネイティブコードを追加する 早速下記の iOS ネイティブコードを Assets/Plugins/iOS/KeychainService.mm に配置します。\n// Keychain Services を利用するために Security フレームワークを利用する #import \u0026lt;Security/Security.h\u0026gt; extern \u0026quot;C\u0026quot; { // 指定したキーで値を保存する関数 // - param // - dataType: 値を保存する際に用いるキー // - value: 保存したい値 // - return // - 保存時のステータスコードを返却する (0 以外は失敗) int addItem(const char *dataType, const char *value) { NSMutableDictionary* attributes = nil; NSMutableDictionary* query = [NSMutableDictionary dictionary]; NSData* sata = [[NSString stringWithCString:value encoding:NSUTF8StringEncoding] dataUsingEncoding:NSUTF8StringEncoding]; [query setObject:(id)kSecClassGenericPassword forKey:(id)kSecClass]; [query setObject:(id)[NSString stringWithCString:dataType encoding:NSUTF8StringEncoding] forKey:(id)kSecAttrAccount]; OSStatus err = SecItemCopyMatching((CFDictionaryRef)query, NULL); if (err == noErr) { attributes = [NSMutableDictionary dictionary]; [attributes setObject:sata forKey:(id)kSecValueData]; [attributes setObject:[NSDate date] forKey:(id)kSecAttrModificationDate]; err = SecItemUpdate((CFDictionaryRef)query, (CFDictionaryRef)attributes); return (int)err; } else if (err == errSecItemNotFound) { attributes = [NSMutableDictionary dictionary]; [attributes setObject:(id)kSecClassGenericPassword forKey:(id)kSecClass]; [attributes setObject:(id)[NSString stringWithCString:dataType encoding:NSUTF8StringEncoding] forKey:(id)kSecAttrAccount]; [attributes setObject:sata forKey:(id)kSecValueData]; [attributes setObject:[NSDate date] forKey:(id)kSecAttrCreationDate]; [attributes setObject:[NSDate date] forKey:(id)kSecAttrModificationDate]; err = SecItemAdd((CFDictionaryRef)attributes, NULL); return (int)err; } else { return (int)err; } } // 指定したキーで値を取得する関数 // - param // - dataType: 値を取得する際に用いるキー // - return // - キーに紐づく値、存在しなければ空文字が返却される char* getItem(const char *dataType) { NSMutableDictionary* query = [NSMutableDictionary dictionary]; [query setObject:(id)kSecClassGenericPassword forKey:(id)kSecClass]; [query setObject:(id)[NSString stringWithCString:dataType encoding:NSUTF8StringEncoding] forKey:(id)kSecAttrAccount]; [query setObject:(id)kCFBooleanTrue forKey:(id)kSecReturnData]; CFDataRef cfresult = NULL; OSStatus err = SecItemCopyMatching((CFDictionaryRef)query, (CFTypeRef*)\u0026amp;cfresult); if (err == noErr) { NSData* passwordData = (__bridge_transfer NSData *)cfresult; const char* value = [[[NSString alloc] initWithData:passwordData encoding:NSUTF8StringEncoding] UTF8String]; char *str = strdup(value); return str; } else { return NULL; } } // 指定したキーで値を削除する関数 // - param // - dataType: 値を削除する際に用いるキー // - return // - 保存時のステータスコードを返却する (0 以外は失敗) int deleteItem(const char *dataType) { NSMutableDictionary* query = [NSMutableDictionary dictionary]; [query setObject:(id)kSecClassGenericPassword forKey:(id)kSecClass]; [query setObject:(id)[NSString stringWithCString:dataType encoding:NSUTF8StringEncoding] forKey:(id)kSecAttrAccount]; OSStatus err = SecItemDelete((CFDictionaryRef)query); if (err == noErr) { return 0; } else { return (int)err; } } } Keychain Services は Security フレームワークを利用するため、KeychainService.mm に対して Security フレームワークの依存関係を設定する必要があります。\nKeychainService.mm で Security フレームワークの利用を可能にする\nその後、上記を Unity スクリプトから実行可能にするための C# クラスを作成します。本記事ではファイルを Assets/Scripts/KeychainService.cs に配置します。\nusing System.Runtime.InteropServices; /// \u0026lt;summary\u0026gt; /// 実装は \u0026lt;c\u0026gt;Assets/Plugins/iOS/KeychainService.mm\u0026lt;/c\u0026gt; に記載 /// \u0026lt;/summary\u0026gt; /// \u0026lt;remarks\u0026gt; /// \u0026lt;a href=\u0026#34;https://developer.apple.com/documentation/security/keychain_services\u0026#34;\u0026gt;Keychain Services\u0026lt;/a\u0026gt; /// \u0026lt;/remarks\u0026gt; class KeychainService { #if UNITY_IOS [DllImport(\u0026#34;__Internal\u0026#34;)] private static extern int addItem(string dataType, string value); [DllImport(\u0026#34;__Internal\u0026#34;)] private static extern string getItem(string dataType); [DllImport(\u0026#34;__Internal\u0026#34;)] private static extern int deleteItem(string dataType); #endif  public bool Put(string key, string value) { #if UNITY_IOS  // 返却されるステータスが 0 なら成功  return addItem(key, value) == 0; #endif  } public string Get(string key) { #if UNITY_IOS  return getItem(key); #else  return null; #endif  } public bool Delete(string key) { #if UNITY_IOS  // 返却されるステータスが 0 なら成功  return deleteItem(key) == 0; #endif  } } あとは用途に応じて下記のようなコードで設定値の保存や取得などを行えます。\n// ... var _keychainService = new KeychainService(); // name をキーとして値を nikaera で保存する _keychainService.Put(\u0026#34;name\u0026#34;, \u0026#34;nikaera\u0026#34;); // name をキーとして値を取得する var name = _keychainService.Get(\u0026#34;name\u0026#34;); // \u0026#34;nikaera\u0026#34; が出力される Debug.Log(name); //　name をキーとして値を削除する _keychainService.Delete(\u0026#34;name\u0026#34;); // ... (余談) インターフェースで iOS/Android のふるまいを共通化する このままだとプラットフォームを切り替える毎にコードを書き直さないとならないので、インターフェースを利用して共通化を行います。\npublic interface ISecretManager { /// \u0026lt;summary\u0026gt;  /// 指定したキーで値を保存する関数  /// \u0026lt;/summary\u0026gt;  /// \u0026lt;param name=\u0026#34;key\u0026#34;\u0026gt;キー\u0026lt;/param\u0026gt;  /// \u0026lt;param name=\u0026#34;value\u0026#34;\u0026gt;値\u0026lt;/param\u0026gt;  /// \u0026lt;returns\u0026gt;保存に成功したかどうか\u0026lt;/returns\u0026gt;  bool Put(string key, string value); /// \u0026lt;summary\u0026gt;  /// 指定したキーの値を取得する関数  /// \u0026lt;/summary\u0026gt;  /// \u0026lt;param name=\u0026#34;key\u0026#34;\u0026gt;キー\u0026lt;/param\u0026gt;  /// \u0026lt;returns\u0026gt;指定したキーで設定された値、無ければ null\u0026lt;/returns\u0026gt;  string Get(string key); /// \u0026lt;summary\u0026gt;  /// 指定したキーの値を削除する関数  /// \u0026lt;/summary\u0026gt;  /// \u0026lt;param name=\u0026#34;key\u0026#34;\u0026gt;キー\u0026lt;/param\u0026gt;  /// \u0026lt;returns\u0026gt;削除に成功したかどうか\u0026lt;/returns\u0026gt;  bool Delete(string key); } その後、Assets/Scripts/EncryptedSharedPreferences.cs および Assets/Scripts/KeychainService.cs を下記の通り ISecretManager の実装に紐付けます。\nusing UnityEngine; /// \u0026lt;summary\u0026gt; /// 利用するネイティブコードは \u0026lt;c\u0026gt;Assets/Plugins/Android/SecretManager.java\u0026lt;/c\u0026gt; に記載 /// \u0026lt;/summary\u0026gt; /// \u0026lt;remarks\u0026gt; /// \u0026lt;a href=\u0026#34;https://developer.android.com/reference/androidx/security/crypto/EncryptedSharedPreferences\u0026#34;\u0026gt;EncryptedSharedPreferences\u0026lt;/a\u0026gt; /// \u0026lt;/remarks\u0026gt; class EncryptedSharedPreferences: ISecretManager { private readonly AndroidJavaObject _secretManager; public EncryptedSharedPreferences() { var activity = new AndroidJavaClass(\u0026#34;com.unity3d.player.UnityPlayer\u0026#34;) .GetStatic\u0026lt;AndroidJavaObject\u0026gt;(\u0026#34;currentActivity\u0026#34;); var context = activity.Call\u0026lt;AndroidJavaObject\u0026gt;(\u0026#34;getApplicationContext\u0026#34;); _secretManager = new AndroidJavaObject(\u0026#34;com.nikaera.SecretManager\u0026#34;, context); } #region ISecretManager  public bool Put(string key, string value) { return _secretManager.Call\u0026lt;bool\u0026gt;(\u0026#34;put\u0026#34;, key, value); } public string Get(string key) { return _secretManager.Call\u0026lt;string\u0026gt;(\u0026#34;get\u0026#34;, key); } public bool Delete(string key) { return _secretManager.Call\u0026lt;bool\u0026gt;(\u0026#34;delete\u0026#34;, key); } #endregion } using System.Runtime.InteropServices; /// \u0026lt;summary\u0026gt; /// 実装は \u0026lt;c\u0026gt;Assets/Plugins/iOS/KeychainService.mm\u0026lt;/c\u0026gt; に記載 /// \u0026lt;/summary\u0026gt; /// \u0026lt;remarks\u0026gt; /// \u0026lt;a href=\u0026#34;https://developer.apple.com/documentation/security/keychain_services\u0026#34;\u0026gt;Keychain Services\u0026lt;/a\u0026gt; /// \u0026lt;/remarks\u0026gt; class KeychainService: ISecretManager { #if UNITY_IOS [DllImport(\u0026#34;__Internal\u0026#34;)] private static extern int addItem(string dataType, string value); [DllImport(\u0026#34;__Internal\u0026#34;)] private static extern string getItem(string dataType); [DllImport(\u0026#34;__Internal\u0026#34;)] private static extern int deleteItem(string dataType); #endif  // KeychainService.mm に定義した関数を呼び出す  #region ISecretManager  public bool Put(string key, string value) { #if UNITY_IOS  return addItem(key, value) == 0; #else  return false; #endif  } public string Get(string key) { #if UNITY_IOS  return getItem(key); #else  return null; #endif  } public bool Delete(string key) { #if UNITY_IOS  return deleteItem(key) == 0; #else  return false; #endif  } #endregion } あとは上記をよしなに利用可能な SecretManager クラスを作成します。\nusing UnityEngine; /// \u0026lt;summary\u0026gt; /// \u0026lt;em\u0026gt;Editor 利用時のみ PlayerPrefs を利用する\u0026lt;/em\u0026gt; /// \u0026lt;/summary\u0026gt; /// \u0026lt;remarks\u0026gt;\u0026lt;see cref=\u0026#34;KeychainService\u0026#34; /\u0026gt;, \u0026lt;see cref=\u0026#34;EncryptedSharedPreferences\u0026#34; /\u0026gt;\u0026lt;/remarks\u0026gt; public static class SecretManager { #if UNITY_EDITOR #elif UNITY_ANDROID  private static ISecretManager _instance = new EncryptedSharedPreferences(); #elif UNITY_IOS  private static ISecretManager _instance = new KeychainService(); #endif  public static bool Put(string key, string value) { #if UNITY_EDITOR  PlayerPrefs.SetString(key, value); PlayerPrefs.Save(); return true; #elif UNITY_IOS || UNITY_ANDROID  return _instance.Put(key, value); #else  Debug.Log(\u0026#34;Not Implemented.\u0026#34;); return false; #endif  } public static string Get(string key) { #if UNITY_EDITOR  return PlayerPrefs.GetString(key); #elif UNITY_IOS || UNITY_ANDROID  return _instance.Get(key); #else  Debug.Log(\u0026#34;Not Implemented.\u0026#34;); return null; #endif  } public static bool Delete(string key) { #if UNITY_EDITOR  PlayerPrefs.DeleteKey(key); PlayerPrefs.Save(); return true; #elif UNITY_IOS || UNITY_ANDROID  return _instance.Delete(key); #else  Debug.Log(\u0026#34;Not Implemented.\u0026#34;); return false; #endif  } } これでプラットフォーム間の実装差異を気にすることなく、下記のような記述で設定値の保存や取得などを行えます。iOS/Android 以外のプラットフォームで追加実装したい場合は プラットフォーム依存コンパイル と ISecretManager の実装クラスを新たに作成することで簡単に追加できます。\n// ... // name をキーとして値を nikaera で保存する SecretManager.Put(\u0026#34;name\u0026#34;, \u0026#34;nikaera\u0026#34;); // name をキーとして値を取得する var name = SecretManager.Get(\u0026#34;name\u0026#34;); // \u0026#34;nikaera\u0026#34; が出力される Debug.Log(name); //　name をキーとして値を削除する SecretManager.Delete(\u0026#34;name\u0026#34;); // ... おわりに 今回は iOS/Android で設定値をセキュアに扱うための方法についてまとめてみました。実際は Keychain Services 周りは実装が大変なので、External Dependency Manager for Unity とか使って KeychainAccess のような外部ライブラリを利用する構成のほうが良いと思われます。\n本記事の内容に誤りがあったり、実際にはセキュアな実装ができていない等々あれば是非コメントでご指摘いただけますと幸いです。\n参考リンク  Android デベロッパー | Android Developers EncryptedSharedPreferences | Android デベロッパー | Android Developers Keychain Services | Apple Developer Documentation SharedPreferences を自前で難読化するのはもう古い?これからは EncryptedSharedPrefenreces を使おう - Qiita iOS のキーチェーンについて - Qiita Unity で IOS にセキュアに値を保存するには KeyChain を使おう - Qiita googlesamples/unity-jar-resolver: Unity plugin which resolves Android \u0026amp; iOS dependencies and performs version management    逆に External Dependency Manager for Unity を利用する方法のメリットは、UnityPackage などでライブラリとして配布する際に、ライブラリを動作させるのに必要な外部パッケージも同梱した状態で配布が可能になるなどがあります。(当然ライセンスには気を付ける必要がありますが\u0026hellip;) \u0026#x21a9;\u0026#xfe0e;\n パッケージの依存関係を自動で解決するかどうかという選択肢になります。本記事では明示的に Resolve を実行するため Disable でも Enable でも進行上の問題はありません。 \u0026#x21a9;\u0026#xfe0e;\n CocoaPods もサポートされているようなので、iOS でも Android 同様、外部ライブラリを取り込むのは簡単にできそうでした。例えば KeychainAccess とか使いたい。 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nikaera.com/archives/unity-ios-android-secret-manager/","summary":"はじめに iOS/Android でユーザーの情報をセキュアに扱う必要があったので、調査したところ Android には EncryptedSharedPreferences が存在することを知りました。iOS には Keychain Services が存在します。\n今回は Unity の iOS/Android プラットフォーム上で設定値を保存するための実装を行う必要があったので、Unity から扱えるようネイティブプラグインを作成しました。今後もこういった要望はありそうでしたので、記事として手順や内容を書き記しておくことにしました。\n本記事内で紹介しているコードは下記にアップ済みです。\nhttps://github.com/nikaera/Unity-iOS-Android-SecretManager-Sample\n動作環境  MacBook Air (M1, 2020) Unity 2020.3.15f2 Android 6.0 以上  EncryptedSharedPreferences が使用可能なバージョン    Android のネイティブプラグインを作成する Android 環境ではまず External Dependency Manager for Unity を利用して、Unity の Android ネイティブプラグインで EncryptedSharedPreferences 利用可能にします。\n(追記) Gradle を利用したライブラリのインストール方法 shiena さんにご教授いただいたのですが、こちらの記事のように Gradle を利用することでも簡易にライブラリの取り込みが可能なようでした。\n手順は上記の記事をご参照いただくとして、Gradle を利用する方法で外部ライブラリを取り込む際の Assets/Plugins/Android/mainTemplate.gradle および Assets/Plugins/Android/gradleTemplate.properties は下記になります。\ndependencies { implementation fileTree(dir: \u0026#39;libs\u0026#39;, include: [\u0026#39;*.","title":"📔 Unity で iOS/Android アプリの設定値をセキュアに扱う方法"},{"content":"MongoDB 用 API で Azure CosmosDB 向けの開発を行っていたのですが、sort 実行時にエラーが発生してしまいリソースが取得できなくなる問題が発生してしまいました。\n結論から言ってしまうと、この Stack Overflow の回答 通り対処すれば解決可能なのですが、簡易的に日本語でも解決策を記しておきます。\nまた、本記事内容の問題に遭遇したときに見つけたのですが、事前に Azure CosmosDB の Docker イメージ を利用しておけば、CosmosDB 特有の挙動に気づけるようになるかもしれません。私は MongoDB の Docker イメージ を利用して開発や動作検証を行っておりました。\n 公式サイトでの MongoDB 用 API でのインデックス管理 についての記事を見ていくと下記の文言が出てきます。\n クエリに並べ替えを適用するには、並べ替え操作で使用されるフィールドに対してインデックスを作成する必要があります。\n そのため、例えば MongoDB の ORM である Mongoose の利用例でいうと、スキーマを定義する際に下記のようにソートに利用したいキーに対してインデックスを指定する必要があります。\n@Schema({ timestamps: { createdAt: \u0026#34;created_at\u0026#34;, updatedAt: \u0026#34;updated_at\u0026#34; } }) export class TestSchema1 extends Document { _id: string; // ソートしたいキーには index を付ける  @Prop({ index: true }) sort!: number; } const schema = SchemaFactory.createForClass(TestSchema1); export const TestSchema1 = schema; また、範囲検索を行いたい場合は各キーにインデックスを作成することで可能になります。\n@Schema({ timestamps: { createdAt: \u0026#34;created_at\u0026#34;, updatedAt: \u0026#34;updated_at\u0026#34; } }) export class TestSchema2 extends Document { _id: string; @Prop() start_at: Date; @Prop() end_at: Date; } // start_at と end_at で範囲検索を行いたいため、 // それぞれに unique インデックスを作成している const schema = SchemaFactory.createForClass(TestSchema2) .index({ start_at: 1 }, { unique: true }) .index({ end_at: 1 }, { unique: true }); export const TestSchema2 = schema; 地味な Tips のような記事ですが、割とハマりました。。この記事が同じ轍を踏んでしまっている方の参考になれれば幸いです 😇\n","permalink":"https://nikaera.com/archives/azure-cosmos-db-sort/","summary":"MongoDB 用 API で Azure CosmosDB 向けの開発を行っていたのですが、sort 実行時にエラーが発生してしまいリソースが取得できなくなる問題が発生してしまいました。\n結論から言ってしまうと、この Stack Overflow の回答 通り対処すれば解決可能なのですが、簡易的に日本語でも解決策を記しておきます。\nまた、本記事内容の問題に遭遇したときに見つけたのですが、事前に Azure CosmosDB の Docker イメージ を利用しておけば、CosmosDB 特有の挙動に気づけるようになるかもしれません。私は MongoDB の Docker イメージ を利用して開発や動作検証を行っておりました。\n 公式サイトでの MongoDB 用 API でのインデックス管理 についての記事を見ていくと下記の文言が出てきます。\n クエリに並べ替えを適用するには、並べ替え操作で使用されるフィールドに対してインデックスを作成する必要があります。\n そのため、例えば MongoDB の ORM である Mongoose の利用例でいうと、スキーマを定義する際に下記のようにソートに利用したいキーに対してインデックスを指定する必要があります。\n@Schema({ timestamps: { createdAt: \u0026#34;created_at\u0026#34;, updatedAt: \u0026#34;updated_at\u0026#34; } }) export class TestSchema1 extends Document { _id: string; // ソートしたいキーには index を付ける  @Prop({ index: true }) sort!: number; } const schema = SchemaFactory.","title":"📝 Azure Cosmos DB でソートしようとするとフリーズする"},{"content":"はじめに 本記事のカバー画像は tookapic from Pixabay の画像です。\n技術研鑽のための行動が習慣化して確立してきたので、また困り始めた時に参照するための備忘録的な感じで、習慣化に至るまでの流れを記事化しておくことにしました。自分の中では努力していると一切感じず自然に技術研鑽及び技術者としてのプレゼンス向上のために行動できるサイクルができた印象です。\n昔から色々なやり方で上記の習慣化にはトライしていたものの、全てが長続きしなかった自分でも継続できるやり方なので、ある程度の再現性はあるかもしれません。(が、あくまでも自分のやり方にはなります。。🙃\nちなみに本記事の内容を推敲していたところ、最近読んだ心理的安全性のつくりかたに出てきた 「きっかけ・行動・みかえり」 のパターンになっていることに気づきました。\n技術研鑽のサイクル 自分で自分のモチベを保ちつつ、自然な技術研鑽サイクルとして定着したフローを説明していきます。\n「きっかけ」：実現したい開発アイデアが浮かぶ Image by Hans Braxmeier from Pixabay\n個人的に突発的にコレ作れたら面白そうとか、自分はコレがあったら便利だなっていうアイデアが浮かぶことがあるのですが、大体そのアイデアを本気で実現したいと本当に思えるピークは体感最長でも 3-4 日くらいです。 そのため、アイデアを具体化して開発に着手するまでの期間としては 3-4 日以内を目安に考えています。\nアイデアが思い浮かんでから長く期間が空いてしまい、具体化する気が無くなってしまった場合はメモアプリにアイデアをストックしておきます。 それらを見返すとより有益なアイデアが浮かんだりするので今後のきっかけ作りに有効活用できます。\nまた、 開発のアイデアについてですが、プライベートだけでなく仕事で開発しているシステムの新機能/改善案等もアイデアとして扱うことが可能です。 自身の仕事の質向上にもつながりますし、行動を起こすためのきっかけ作りとしても利用できて、技術検証や実装等は業務時間で行えるため割とオススメの方法です。1\n「行動」：開発アイデアを検証可能な形で実装する Image by Sasin Tipchai from Pixabay\n開発では、とにかく動作確認が可能な形で実装を行うことに注力します。細かい設計などは置いておいて仮説検証を優先していくイメージです。 そのため、技術選定は何基準でも問題ありませんが、アイデアの内容によってそこら辺の技術基準は変えていくのが良いと考えています。\n例えば、プラグインやライブラリの開発については使い慣れた言語や業務で利用している技術を選定すると、知識や知見を深めることに繋がりやすかったです。サービス開発については初期は触れる技術領域が広く浅くになりがちなので、新しい技術など使い慣れていない言語などを採用して知識や知見を広げることを意識すると最後まで楽しく開発できました。\n技術研鑽という名目で取り組む開発なので、完成を目指す必要はありません。しかし、成果物として完成させることを目指したいという場合は、モチベーションを絶やさないようベースとなる機能の実装を短期間で行うことを意識します。ベースとなる機能の実装というのは、一通りそのシステムの動作検証を他者が行える状態を示します。 システムの動作検証を他者が行える状態まで持っていければ、正しく成果物が評価できる状態になっているはずだからです。\nまとめると、モチベを保てるやり方を意識して技術選定を行い、具体的な成果物を意識してベースとなる機能の実装を短期間の細かいサイクルで進められると、満足感のある形で開発を進めることができました。\n「みかえり」：過程で得たもの全てをアウトプットする Photo by Greg Rakozy on Unsplash\n成果物の公開だけでなく、その開発を通して得た知見や知識及びソースコードなど全てアウトプットします。行動の途中だとしてもモチベが下がってきたら、みかえりのフェーズに移行します。 例えば、成果物で言うと、ストア公開するだけでなくソースコードも GitHub の公開リポジトリにアップします。知識や知見などは技術ブログで記事化してまとめて公開します。 他者の目に触れる場所へ公開することを意識すると、アウトプットの質を高めるモチベに繋がります。 承認欲求を満たすためではなく、あくまで自分のアウトプットの質を高める施策の一貫として考えます。\n上記を意識するとリファクタリングやベスプラに沿った開発ができているか等のチェックに繋がり、知識や知見をより深めることに繋がります。記事化も同様で、文章として知見や知識を残す過程で正しい内容なのか、本当に正しく動作するソースコードが書けているかに意識が向くため、誤った知識の修正や復習に繋がります。\n成果物だけでなく身につけた技術も含めて全てをアウトプットすることで、余すこと無く行動した結果を有効活用できます。このサイクルを何回か行い習慣化してくると、インプットした内容を全てアウトプットしきるための行動が自然と起こせるようになっていきました。 アウトプットする過程できっかけが生まれて更に行動することにつながっていくという流れも生まれました。\n具体例としては、シンプルなプラグインを作っていく過程で、ライブラリ化やモジュール化した方が良い機能がでてきたので、別リポジトリに切り出してパッケージマネージャーからインストール可能にしました。また、業務で特殊な事情で取り組んだ開発内容を新たな知見として記事化してアウトプットを増やすことができるようになりました。\n ちなみに私はアウトプットする手段としての記事化には重い腰が中々上がらない人間だったのですが、堤 修一さんの Youtube 動画 と同様の考え方に自然に至りました。\n全てが自分のやった成果として目に見える形で残っていくので、後から見返したときに自分の実績として実感が湧きやすく、作りっぱなしで終わっていた頃と比べると相当な達成感を味わうこともできます。 ゲームでいう様々な実績を解除してくような感覚に近いかもしれません。🎮\nおわりに 現状個人的には上記のサイクルが上手く機能していて、最近習慣化の軌道に乗った感じが自分の中にあったので知見として記事化しておくことにしました。今後、上記サイクルについては改善を繰り返しながらアップデートしていきますが、一旦現状の内容を後から見返せるようにしました。\nまた今回始めて記事内でイメージ画像をふんだんに使ってみましたが、画像探すの楽しいし記事のクオリティが上がったと錯覚できるので、今後もポエム記事については積極的に画像を利用していこうと思いました (違\nこの記事内容がどなたかの行動を起こすきっかけとなれれば幸いです！🙏\n  当たり前ですが NDA に触れるような内容を自身の成果として公開してしまうのは絶対にダメです。あくまでも一般的な技術知識のみを自身の得た知見として公開するというスタンスです。 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nikaera.com/archives/improve-skills-without-stress/","summary":"はじめに 本記事のカバー画像は tookapic from Pixabay の画像です。\n技術研鑽のための行動が習慣化して確立してきたので、また困り始めた時に参照するための備忘録的な感じで、習慣化に至るまでの流れを記事化しておくことにしました。自分の中では努力していると一切感じず自然に技術研鑽及び技術者としてのプレゼンス向上のために行動できるサイクルができた印象です。\n昔から色々なやり方で上記の習慣化にはトライしていたものの、全てが長続きしなかった自分でも継続できるやり方なので、ある程度の再現性はあるかもしれません。(が、あくまでも自分のやり方にはなります。。🙃\nちなみに本記事の内容を推敲していたところ、最近読んだ心理的安全性のつくりかたに出てきた 「きっかけ・行動・みかえり」 のパターンになっていることに気づきました。\n技術研鑽のサイクル 自分で自分のモチベを保ちつつ、自然な技術研鑽サイクルとして定着したフローを説明していきます。\n「きっかけ」：実現したい開発アイデアが浮かぶ Image by Hans Braxmeier from Pixabay\n個人的に突発的にコレ作れたら面白そうとか、自分はコレがあったら便利だなっていうアイデアが浮かぶことがあるのですが、大体そのアイデアを本気で実現したいと本当に思えるピークは体感最長でも 3-4 日くらいです。 そのため、アイデアを具体化して開発に着手するまでの期間としては 3-4 日以内を目安に考えています。\nアイデアが思い浮かんでから長く期間が空いてしまい、具体化する気が無くなってしまった場合はメモアプリにアイデアをストックしておきます。 それらを見返すとより有益なアイデアが浮かんだりするので今後のきっかけ作りに有効活用できます。\nまた、 開発のアイデアについてですが、プライベートだけでなく仕事で開発しているシステムの新機能/改善案等もアイデアとして扱うことが可能です。 自身の仕事の質向上にもつながりますし、行動を起こすためのきっかけ作りとしても利用できて、技術検証や実装等は業務時間で行えるため割とオススメの方法です。1\n「行動」：開発アイデアを検証可能な形で実装する Image by Sasin Tipchai from Pixabay\n開発では、とにかく動作確認が可能な形で実装を行うことに注力します。細かい設計などは置いておいて仮説検証を優先していくイメージです。 そのため、技術選定は何基準でも問題ありませんが、アイデアの内容によってそこら辺の技術基準は変えていくのが良いと考えています。\n例えば、プラグインやライブラリの開発については使い慣れた言語や業務で利用している技術を選定すると、知識や知見を深めることに繋がりやすかったです。サービス開発については初期は触れる技術領域が広く浅くになりがちなので、新しい技術など使い慣れていない言語などを採用して知識や知見を広げることを意識すると最後まで楽しく開発できました。\n技術研鑽という名目で取り組む開発なので、完成を目指す必要はありません。しかし、成果物として完成させることを目指したいという場合は、モチベーションを絶やさないようベースとなる機能の実装を短期間で行うことを意識します。ベースとなる機能の実装というのは、一通りそのシステムの動作検証を他者が行える状態を示します。 システムの動作検証を他者が行える状態まで持っていければ、正しく成果物が評価できる状態になっているはずだからです。\nまとめると、モチベを保てるやり方を意識して技術選定を行い、具体的な成果物を意識してベースとなる機能の実装を短期間の細かいサイクルで進められると、満足感のある形で開発を進めることができました。\n「みかえり」：過程で得たもの全てをアウトプットする Photo by Greg Rakozy on Unsplash\n成果物の公開だけでなく、その開発を通して得た知見や知識及びソースコードなど全てアウトプットします。行動の途中だとしてもモチベが下がってきたら、みかえりのフェーズに移行します。 例えば、成果物で言うと、ストア公開するだけでなくソースコードも GitHub の公開リポジトリにアップします。知識や知見などは技術ブログで記事化してまとめて公開します。 他者の目に触れる場所へ公開することを意識すると、アウトプットの質を高めるモチベに繋がります。 承認欲求を満たすためではなく、あくまで自分のアウトプットの質を高める施策の一貫として考えます。\n上記を意識するとリファクタリングやベスプラに沿った開発ができているか等のチェックに繋がり、知識や知見をより深めることに繋がります。記事化も同様で、文章として知見や知識を残す過程で正しい内容なのか、本当に正しく動作するソースコードが書けているかに意識が向くため、誤った知識の修正や復習に繋がります。\n成果物だけでなく身につけた技術も含めて全てをアウトプットすることで、余すこと無く行動した結果を有効活用できます。このサイクルを何回か行い習慣化してくると、インプットした内容を全てアウトプットしきるための行動が自然と起こせるようになっていきました。 アウトプットする過程できっかけが生まれて更に行動することにつながっていくという流れも生まれました。\n具体例としては、シンプルなプラグインを作っていく過程で、ライブラリ化やモジュール化した方が良い機能がでてきたので、別リポジトリに切り出してパッケージマネージャーからインストール可能にしました。また、業務で特殊な事情で取り組んだ開発内容を新たな知見として記事化してアウトプットを増やすことができるようになりました。\n ちなみに私はアウトプットする手段としての記事化には重い腰が中々上がらない人間だったのですが、堤 修一さんの Youtube 動画 と同様の考え方に自然に至りました。\n全てが自分のやった成果として目に見える形で残っていくので、後から見返したときに自分の実績として実感が湧きやすく、作りっぱなしで終わっていた頃と比べると相当な達成感を味わうこともできます。 ゲームでいう様々な実績を解除してくような感覚に近いかもしれません。🎮\nおわりに 現状個人的には上記のサイクルが上手く機能していて、最近習慣化の軌道に乗った感じが自分の中にあったので知見として記事化しておくことにしました。今後、上記サイクルについては改善を繰り返しながらアップデートしていきますが、一旦現状の内容を後から見返せるようにしました。","title":"📔 マイペースに技術研鑽を継続する方法"},{"content":"はじめに FastAPI と SQLAlchemy を利用して Web API 開発を行っていた際、SQLAlchemy のマイグレーションツールである alembic を利用していました。\nただ E2E テストを書こうとした際に、pytest 実行中に alembic でデータベースマイグレーションを行う方法が分からず模索していました。結果的にマイグレーションのやり方は分かったものの一応今後も利用するかもしれないため、その内容を記事として残しておくことにしました。\n本記事内で利用しているソースコードを含む FastAPI プロジェクトを GitHub リポジトリ上にアップしておいたので、詳細を確認されたい方がいればご参照くださいませ。\nalembic でマイグレーションを行う conftest.py にグローバルで利用するマイグレーション用の fixture を定義すれば OK です。\n# conftest.py import os import alembic.config import pytest from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker from sqlalchemy_utils import database_exists, create_database, drop_database # テスト用の初期データを定義した module を import する (必要があれば) # from .seed import users, contents # 指定したパラメータを用いて alembic によるデータベースマイグレーションを行う # 引数のデフォルト設定では全てのマイグレーションを実行するようになっている def migrate(migrations_path, alembic_ini_path=\u0026#39;alembic.ini\u0026#39;, connection=None, revision=\u0026#34;head\u0026#34;): config = alembic.config.Config(alembic_ini_path) config.set_main_option(\u0026#39;script_location\u0026#39;, migrations_path) if connection is not None: config.attributes[\u0026#39;connection\u0026#39;] = connection alembic.command.upgrade(config, revision) # テスト実行用にセットアップされたデータベースのセッション情報を扱う関数 # scope に session を指定することでテスト全体で一回だけ実行されるようにする @pytest.fixture(scope=\u0026#34;session\u0026#34;, autouse=True) def SessionLocal(): test_sqlalchemy_database_url = os.environ[\u0026#39;DATABASE_URL\u0026#39;] engine = create_engine(test_sqlalchemy_database_url) # 既にテスト用データベースが存在していたら破棄する if database_exists(test_sqlalchemy_database_url): drop_database(test_sqlalchemy_database_url) # テスト用データベースを作成する create_database(test_sqlalchemy_database_url) # 環境変数 DATABASE_URL で指定したデータベースに対して、 # マイグレーションを行いテスト実行に必要なテーブルを一括作成する # 第一引数に指定している alembic は `alembic init \u0026lt;環境名\u0026gt;` 実行時に指定した環境名を入力 with engine.begin() as connection: migrate(\u0026#34;alembic\u0026#34;, \u0026#39;alembic.ini\u0026#39;, connection) Base = declarative_base() Base.metadata.create_all(engine) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) # テスト用の各種データを追加する (必要があれば) # db_session = SessionLocal() # for user in users: # db_session.add(user) # db_session.commit() # for content in contents: # db_session.add(content) # db_session.commit() # db_session.close() # テスト用データ追加後のセットアップ済みの状態で # テスト用に利用する SessionLocal を返却する yield SessionLocal # テストが全て終わったら、テスト用データベースを破棄して、 # SQLAlchemy のセッションも切断する drop_database(test_sqlalchemy_database_url) engine.dispose() FastAPI の pytest への適用例 上記を関数を利用する方法は各自のテスト環境によって異なると思いますが、一応私が FastAPI のテストコードを書く際に利用したソースコードを元に参考例を載せておきます。\nconftest.py と同じディレクトリに client.py を作成します。\n# client.py from fastapi import Header, HTTPException, status from fastapi.testclient import TestClient from app.dependencies import get_database from app.main import app # conftest で定義した fixture の SessionLocal を元に、 # データベースセッションを作成するための override_get_db 関数を定義して、 # get_database の代わりに override_get_db を実行するよう差し替える def temp_db(f): def func(SessionLocal, *args, **kwargs): def override_get_db(): db = SessionLocal() try: yield db finally: db.close() app.dependency_overrides[get_database] = override_get_db f(*args, **kwargs) app.dependency_overrides[get_database] = get_database return func client = TestClient(app) あとは pytest のコード内で下記のような記述を行えば、FastAPI の内部でテスト用データベースを利用してくれるようになります。\nfrom fastapi import status from .client import client, temp_db # temp_db fixture を定義しておくことで、 # 関数の実行中は FastAPI の内部でテスト用データベースを利用する @temp_db def test_read_me_token_valid(): response = client.get(\u0026#34;/users/me\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer 1234567890\u0026#34;}) assert response.status_code == status.HTTP_200_OK #... 参考リンク  FastAPI でテスト用のクリーンな DB を作成して pytest で API の Unittest を行う - Qiita pytest：フィクスチャ(fixture)の使い方 - Qiita Dependencies - First Steps - FastAPI pytest: helps you write better programs — pytest documentation  ","permalink":"https://nikaera.com/archives/pytest-sqlalchemy-alembic/","summary":"はじめに FastAPI と SQLAlchemy を利用して Web API 開発を行っていた際、SQLAlchemy のマイグレーションツールである alembic を利用していました。\nただ E2E テストを書こうとした際に、pytest 実行中に alembic でデータベースマイグレーションを行う方法が分からず模索していました。結果的にマイグレーションのやり方は分かったものの一応今後も利用するかもしれないため、その内容を記事として残しておくことにしました。\n本記事内で利用しているソースコードを含む FastAPI プロジェクトを GitHub リポジトリ上にアップしておいたので、詳細を確認されたい方がいればご参照くださいませ。\nalembic でマイグレーションを行う conftest.py にグローバルで利用するマイグレーション用の fixture を定義すれば OK です。\n# conftest.py import os import alembic.config import pytest from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker from sqlalchemy_utils import database_exists, create_database, drop_database # テスト用の初期データを定義した module を import する (必要があれば) # from .seed import users, contents # 指定したパラメータを用いて alembic によるデータベースマイグレーションを行う # 引数のデフォルト設定では全てのマイグレーションを実行するようになっている def migrate(migrations_path, alembic_ini_path=\u0026#39;alembic.","title":"📝 pytest で alembic のマイグレーションを行う方法"},{"content":"Teemo という Chrome 拡張を開発したのですが、その際 1 つのウインドウを使い回す構成にしたいなと考えていました。\n例えば何も考えずに chrome.windows.create を Chrome 拡張を開くたびに呼び出すと、呼び出すたびにウインドウが新規作成されてしまいます。そうすると、都度画面に不要なウインドウが出てきて邪魔になるだけでなく、手動で不要なウインドウを消す作業をユーザーに強いることとなってしまいます。。🙃\n上記のような挙動が望まれるケースもあると思いますが、Teemo ではウインドウ間を頻繁に行き来するため、ショートカットを利用して拡張機能を呼び出すことを見込んでいました。そのため、ショートカットを利用して拡張機能を呼び出すたびにウインドウが新規作成され続ける挙動は望んでいませんでした。\n1 つのウインドウを使い回すためには、chrome.windows.create 時に作成される window の id を保持しておきます。その後、Chrome 拡張が呼び出されるたびに window が既に存在するかどうかを保持していた id を元にチェックします。既に window が存在していた場合はそれを使いまわします。存在していなかった場合は、chrome.windows.create で window を新規作成します。\n// window の id を保持しておくための変数 let vid = -1; chrome.browserAction.onClicked.addListener(function () { // vid の値を元に Chrome 拡張で開いた window の取得を試みる  chrome.windows.get(vid, function (chromeWindow) { // エラーが無く、既に window が存在している場合は、  // そのステータスを { focused: true } にすることで最前面に呼び出す  if (!chrome.runtime.lastError \u0026amp;\u0026amp; chromeWindow) { chrome.windows.update(vid, { focused: true }); return; } // 上記以外のパターンでは window を新規作成する  chrome.windows.create( { url: \u0026#34;index.html\u0026#34;, type: \u0026#34;popup\u0026#34;, }, function (window) { // 新規作成した window を使い回せるようにするため、  // vid 変数に window の id を保持しておく  vid = window.id; } ); }); }); 上記コードで 1 つのウインドウを使い回すことが出来るようになるはずです。✅\n","permalink":"https://nikaera.com/archives/chrome-extension-only-one-window/","summary":"Teemo という Chrome 拡張を開発したのですが、その際 1 つのウインドウを使い回す構成にしたいなと考えていました。\n例えば何も考えずに chrome.windows.create を Chrome 拡張を開くたびに呼び出すと、呼び出すたびにウインドウが新規作成されてしまいます。そうすると、都度画面に不要なウインドウが出てきて邪魔になるだけでなく、手動で不要なウインドウを消す作業をユーザーに強いることとなってしまいます。。🙃\n上記のような挙動が望まれるケースもあると思いますが、Teemo ではウインドウ間を頻繁に行き来するため、ショートカットを利用して拡張機能を呼び出すことを見込んでいました。そのため、ショートカットを利用して拡張機能を呼び出すたびにウインドウが新規作成され続ける挙動は望んでいませんでした。\n1 つのウインドウを使い回すためには、chrome.windows.create 時に作成される window の id を保持しておきます。その後、Chrome 拡張が呼び出されるたびに window が既に存在するかどうかを保持していた id を元にチェックします。既に window が存在していた場合はそれを使いまわします。存在していなかった場合は、chrome.windows.create で window を新規作成します。\n// window の id を保持しておくための変数 let vid = -1; chrome.browserAction.onClicked.addListener(function () { // vid の値を元に Chrome 拡張で開いた window の取得を試みる  chrome.windows.get(vid, function (chromeWindow) { // エラーが無く、既に window が存在している場合は、  // そのステータスを { focused: true } にすることで最前面に呼び出す  if (!chrome.runtime.lastError \u0026amp;\u0026amp; chromeWindow) { chrome.","title":"📝 Chrome 拡張で 1つのウインドウを使い回す方法"},{"content":"react-emoji-textarea の開発を行った際、リリースを作成したら自動的に Node.js パッケージにライブラリが公開される仕組みがほしいと考え、GitHub Actions でそれを実現することにしました。\nその際、公式サイトに 公開されている内容 を参考に GitHub Actions を作成したのですが、そのまま利用すると私の環境では下記のエラーが発生してしまいました。\nerror Couldn\u0026#39;t publish package: \u0026#34;https://registry.yarnpkg.com/@nikaera/react-emoji-textarea: You do not have permission to publish \\\u0026#34;react-emoji-textarea\\\u0026#34;. Are you logged in as the correct user?\u0026#34; 上記のエラーについて調査しながら改修したところ、最終的に下記の GitHub Actions で Node.js パッケージを公開できるようになりました。secrets.NPM_TOKEN には NPM Token を登録します。\nname: Node.js Package on: # workflow_dispatch を追加して手動でも実行できるよう改修 workflow_dispatch: release: types: [created] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v2 with: node-version: \u0026#34;14.x\u0026#34; registry-url: \u0026#34;https://registry.npmjs.org\u0026#34; # registry.npmjs.org へアクセスする際は必ず認証を試みるオプションを追加 always-auth: true # scope には自分のユーザ名を指定 scope: \u0026#34;@nikaera\u0026#34; # .npmrc に https://registry.npmjs.org アクセス時に利用する認証情報を記載する - run: echo \u0026#34;//registry.npmjs.org/:_authToken=${{ secrets.NPM_TOKEN }}\u0026#34; \u0026gt; ~/.npmrc - name: Build react-emoji-textarea 😆💖 run: |yarn install --frozen-lockfile yarn format yarn build - run: yarn publish --access public env: NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }} ","permalink":"https://nikaera.com/archives/yarn-publish-github-actions/","summary":"react-emoji-textarea の開発を行った際、リリースを作成したら自動的に Node.js パッケージにライブラリが公開される仕組みがほしいと考え、GitHub Actions でそれを実現することにしました。\nその際、公式サイトに 公開されている内容 を参考に GitHub Actions を作成したのですが、そのまま利用すると私の環境では下記のエラーが発生してしまいました。\nerror Couldn\u0026#39;t publish package: \u0026#34;https://registry.yarnpkg.com/@nikaera/react-emoji-textarea: You do not have permission to publish \\\u0026#34;react-emoji-textarea\\\u0026#34;. Are you logged in as the correct user?\u0026#34; 上記のエラーについて調査しながら改修したところ、最終的に下記の GitHub Actions で Node.js パッケージを公開できるようになりました。secrets.NPM_TOKEN には NPM Token を登録します。\nname: Node.js Package on: # workflow_dispatch を追加して手動でも実行できるよう改修 workflow_dispatch: release: types: [created] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v2 with: node-version: \u0026#34;14.x\u0026#34; registry-url: \u0026#34;https://registry.npmjs.org\u0026#34; # registry.","title":"📝 Node.js パッケージを公開するための GitHub Actions を構築する"},{"content":"はじめに 📝 最近とある事情により Twitter の DM を利用しているのですが、Slack などのように絵文字をショートカット入力できないことにフラストレーションが溜まってきていました。そのため、絵文字をショトカで入力可能にしてくれる Chrome 拡張機能を探したのですが見つけられませんでした。\nそこで、無いなら作ろうということで Teemo を開発しました。\nソースコードは GitHub 上で公開しています。 何かご要望等ございましたら PR や Issue 作成頂けますと喜びます。Teemo の実際の挙動については下記の動画で確認できます 🎥   考えていたこと 💭 今回 Teemo の開発を行うに当たり、考えていた点は下記になります。\n よくある : 入力からの絵文字ショートカットを導入する  Slack や GitHub、JIRA などではおなじみの入力方法 ⌨️   パレットから選択する際は半角英数字で検索できるようにしたい  Twitter では日本語で検索しないと絵文字が探せない 🔍 普段英数字で絵文字検索をするので目的の絵文字が見つけづらい 🕵️   拡張機能を利用することで文章入力の煩わしさが増加することは避けたい  コピペや文章クリアの機能等にもショトカ利用できるようにしたい 💨    プロトタイピングしながら友人に進捗をシェアしながら開発は進めていきました。本当は個人で利用する想定で進めていたのですが、割と評判が良かったため Chrome ウェブストアに公開するのを目標に動いていました。そして、Chrome ウェブストアで公開できるクオリティを目指して動いたことで満足のいく拡張機能が作れました。\n使い方 ⚒️ Teemo の使い方について紹介いたします。\nTeemo を Chrome 拡張機能として追加する 🛍️ まずは Chrome ウェブストア にアクセスして、Teemo を Chrome に追加する必要があります。\n1. Chrome ウェブストア にアクセスして右上の Chrome に追加 ボタンをクリックする\n2. 拡張機能として Teemo の追加が完了したら、ツールバーのアイコンをクリックするか Ctrl or Cmd + Shift + O を入力して Teemo のエディタが開くことを確認する\n上記確認できれば Teemo が正常に Chrome 拡張機能として追加できています。\nTeemo で絵文字を : でショートカット入力する 💨 Teemo のエディタでは : を利用することで文章入力を止めることなく、シームレスに絵文字入力が行えます。: を入力後、関連ワードを半角英数字で入力することで絵文字候補がウィンドウ下部に表示されます。入力したい絵文字をマウスかキーボードの矢印キーで選択して入力できます。\n: を入力後、関連ワードを半角英数字で入力することで絵文字候補がウィンドウ下部に表示される 💨\nTeemo で絵文字をパレットから選択して入力する 🎨 絵文字をパレットから選択して入力するには Ctrl or Cmd + E キーを入力するか、ウィンドウ下部の真ん中にある 🎨 ボタンをクリックしてパレットを表示する必要があります。ちなみに絵文字選択用のパレットには OSS の Emoji Mart を利用しています。\nパレットで関連ワードを入力することで絵文字検索ができる。絵文字をクリックすることで入力できる 🐱\nTeemo で入力した文章をクリップボードにコピーする 📋 入力した文章は Ctrl or Cmd + C キーを入力するか、ウィンドウ下部の右にある 📋 ボタンをクリックすることで全文コピーが可能です。わざわざ範囲選択を行わずとも文章を全文コピーすることができます。 コピーが正常にできるとウィンドウ下部の真ん中にある 📋 ボタンが 📋✔️ ボタンになります。\nTeemo で文章入力後 Ctrl + C で全文コピー後、ツイート編集画面にペーストした時の様子 🐦\nTeemo で入力した文章をクリアする 🧼 Teemo では単一のウィンドウを使い回すため、改めて文章を作成する際に一度入力した文章をクリアする必要があります。 そのため、Ctrl or Cmd + R キーを入力するか、ウィンドウ下部の左にある 🆕 ボタンをクリックすることで文章をクリアできるようにしています。\nTeemo の入力欄を 🆕 ボタンをクリックしてクリアした時の様子 🧼\nおわりに 🔚 個人的に Teemo のおかげで、Twitter での絵文字を利用した文章作成についてストレス無く出来るようになったので良かったです。副次的な効果として、どんなサイトでも絵文字を利用した文章作成が簡単に出来るようになったため、他に絵文字をショートカット入力できないサイトがあったとしても同様の困りごとは発生しなくなりました。☺️\nSNS 上などで文章を作成する際に絵文字を頻繁に利用する方には割とご満足いただける拡張機能になっているはずです多分 🙋\n","permalink":"https://nikaera.com/archives/teemo-first-release/","summary":"はじめに 📝 最近とある事情により Twitter の DM を利用しているのですが、Slack などのように絵文字をショートカット入力できないことにフラストレーションが溜まってきていました。そのため、絵文字をショトカで入力可能にしてくれる Chrome 拡張機能を探したのですが見つけられませんでした。\nそこで、無いなら作ろうということで Teemo を開発しました。\nソースコードは GitHub 上で公開しています。 何かご要望等ございましたら PR や Issue 作成頂けますと喜びます。Teemo の実際の挙動については下記の動画で確認できます 🎥   考えていたこと 💭 今回 Teemo の開発を行うに当たり、考えていた点は下記になります。\n よくある : 入力からの絵文字ショートカットを導入する  Slack や GitHub、JIRA などではおなじみの入力方法 ⌨️   パレットから選択する際は半角英数字で検索できるようにしたい  Twitter では日本語で検索しないと絵文字が探せない 🔍 普段英数字で絵文字検索をするので目的の絵文字が見つけづらい 🕵️   拡張機能を利用することで文章入力の煩わしさが増加することは避けたい  コピペや文章クリアの機能等にもショトカ利用できるようにしたい 💨    プロトタイピングしながら友人に進捗をシェアしながら開発は進めていきました。本当は個人で利用する想定で進めていたのですが、割と評判が良かったため Chrome ウェブストアに公開するのを目標に動いていました。そして、Chrome ウェブストアで公開できるクオリティを目指して動いたことで満足のいく拡張機能が作れました。\n使い方 ⚒️ Teemo の使い方について紹介いたします。\nTeemo を Chrome 拡張機能として追加する 🛍️ まずは Chrome ウェブストア にアクセスして、Teemo を Chrome に追加する必要があります。","title":"📔 チャットの短文作成に便利な Chrome 拡張機能を開発してみた"},{"content":"はじめに 先日同僚が Unity の CI 環境を構築するためのライブラリである GameCI について教えてくれました。早速 GameCI の GitHub Actions を利用して、サンプルプロジェクトで色々動作検証してみたところ、Unity の CI 環境を楽に構築できることが分かりました。\nもちろん、Unity Cloud Build を利用すれば CI 環境の構築は以前から楽にできました。しかし、選択肢の 1 つとして GameCI を持っておくことで、サクッと GitHub Actions に統合する形で Unity の CI 環境を導入できるのは他には無いメリットを感じました。\n本記事で紹介しているソースコード、及び検証時に利用したプロジェクトは GitHub にアップ済みですので、手っ取り早く内容を把握されたい方は下記をご参照くださいませ。\nhttps://github.com/nikaera/Unity-GameCI-Sample\n業務でも利用できそうなので、GameCI を利用して CI 環境を構築する手順を記事でまとめました。\nGameCI に備わっている機能紹介 GameCI には現状下記の GitHub Actions が用意されているようです。\n   機能 概要     Activation Unity ライセンスを任意の Unity バージョンで発行する   Test runner Unity の PlayMode 及び EditMode のテストを実行する (テスト結果の出力にも対応)   Builder 任意の Platform ビルドを実行する (アーティファクト 利用でダウンロードも可能)   Returning a license Unity ライセンスの返却ができる (Professional License のみ対応)   Remote builder GitHub Actions のスペックでは満足のいくビルドができない際に AWS 環境でハイスペックなマシンを用意してビルドできる。ビルドのためのインフラ構築には AWS CloudFormation を使用している (現在は AWS のみ対応。今後 GCP, Azure にも対応予定とのこと)   Deployment Unity ビルドを各種 Platform 向けにデプロイする (iOS 及び Android のみ記載あり。厳密に言うと Builder でビルド出力した内容を fastlane を用いてデプロイするためのワークフロー紹介になっている)    上記を見ると既に GameCI には開発者として Unity CI に欲しい機能は最低限揃っているように見受けられました。 また本記事では、今後機会があれば試してみたいと考えていますが Remote builder 及び Deployment については言及していません。\n今回は実例を交えながら Activation 及び Test runner、Builder、Returning a license の使用方法について紹介していきます。\nActivation: GameCI で必要となる Unity License のアクティベーションを行う GameCI で Unity ライセンスをアクティベートするには Activation を利用します。早速ドキュメントの手順に沿って作業を進めていきます。\nまず CI を導入したい GitHub 上の Unity プロジェクトの .github/workflows 内に Unity ライセンスアクティベート用のワークフローファイルを作成します。\nname: Acquire activation file on: workflow_dispatch: {} jobs: activation: name: Request manual activation file 🔑 runs-on: ubuntu-latest steps: # GameCI の Activation を利用して alf ファイルを発行する - name: Request manual activation file id: getManualLicenseFile uses: game-ci/unity-request-activation-file@v2 with: # Unity プロジェクトのバージョンを指定する # ProjectSettings/ProjectVersion.txt に記載されているバージョンを入力すれば OK unityVersion: 2020.3.5f1 # Upload artifact (Unity_v20XX.X.XXXX.alf) - name: Expose as artifact uses: actions/upload-artifact@v2 with: name: ${{ steps.getManualLicenseFile.outputs.filePath }} path: ${{ steps.getManualLicenseFile.outputs.filePath }} その後、デフォルトブランチ にプッシュして GitHub Actions で実行可能にしたら、下記手順に従い Unity ライセンスファイルのアクティベート及びダウンロードを行います。\n1. ブラウザから GitHub リポジトリにアクセスして、Unity ライセンスアクティベート用のワークフローを実行して alf ファイルを生成する\n2. ワークフローの実行に成功したら、該当項目をクリックして詳細画面に遷移する\n3. Artifacts の項目から alf ファイルをダウンロードする\n4. Unity license manual activation webpage からログインして alf ファイルをアップロードする\n5. Unity ライセンスの利用用途に応じて適切な選択肢を入力する (本記事では Personal ライセンスを選択)\n6. Download license ボタンをクリックして ulf ファイルをダウンロードする\nこれで Unity ライセンスファイルのアクティベートは完了です。次にアクティベートしたライセンスファイルを GitHub リポジトリの Secrets に登録して、GameCI で PlayMode 及び EditMode のテストが実行できるようにしていきます。\n::: message\nalf 拡張子のファイルがライセンスリクエストファイルを指していて、Unity ライセンスの発行に必要となるファイルです。ulf 拡張子のファイルが Unity ライセンスのファイルです。1\n:::\nTest runner: PlayMode 及び EditMode テストを実行して結果を参照する GitHub Actions 上でテストを実行するために、先ほどアクティベートした Unity ライセンスの情報を ワークフロー上で扱えるようにする必要があります。そのため、まずは Secrets に ulf ファイルの内容を登録することから始めます。\n1. Unity ライセンスの情報登録のため、Github リポジトリの Secrets 登録画面に遷移する\n2. GameCI はライセンス情報参照のため、デフォルト設定では Secrets の UNITY_LICENSE を参照する。そのため、Name が UNITY_LICENSE、Value には ulf ファイルの中身を登録する2\n上記作業で GameCI でテストやビルド実行を行える環境が整ったので、動作検証のためテスト実行用のワークフローファイルを作成します。\nname: Run EditMode and PlayMode Test on: workflow_dispatch: {} jobs: test: name: Run EditMode and PlayMode Test runs-on: ubuntu-latest steps: # actions/checkout@v2 を利用して作業ディレクトリに # Unity プロジェクトの中身をダウンロードしてくる - name: Check out my unity project. uses: actions/checkout@v2 # GameCI の Test runner を利用して # EditMode 及び PlayMode のテストを実行する - name: Run EditMode and PlayMode Test uses: game-ci/unity-test-runner@v2 env: # 2. の手順で Secrets に登録した Unity ライセンスの情報を指定する UNITY_LICENSE: ${{ secrets.UNITY_LICENSE }} # もし Professional license を使いたい場合は、 # メールアドレス、パスワード、シリアルナンバーを入力する必要がある # ref: https://game.ci/docs/github/test-runner#professional-license # UNITY_EMAIL: ${{ secrets.UNITY_EMAIL }} # UNITY_PASSWORD: ${{ secrets.UNITY_PASSWORD }} # UNITY_SERIAL: ${{ secrets.UNITY_SERIAL }} with: projectPath: . # テストの実行結果もみたい場合は githubToken を指定する # secrets.GITHUB_TOKEN は Secrets 未登録でも利用可能 githubToken: ${{ secrets.GITHUB_TOKEN }} # Unity プロジェクトのバージョンを指定する # ProjectSettings/ProjectVersion.txt に記載されているバージョンを入力すれば OK unityVersion: 2020.3.5f1 # 実行したいテストの種類を指定できる # 指定可能な値は All, PlayMode, EditMode # testMode: All # テスト実行時に利用したい Docker イメージを明示的に指定できる # customImage: \u0026#39;unityci/editor:2020.1.14f1-base-0\u0026#39; # テストの実行結果をアーティファクトにアップロードしてダウンロードして参照できるようにする - uses: actions/upload-artifact@v2 if: always() with: name: Test results path: artifacts 上記のワークフローファイルを GitHub Actions 上で動作検証する際の手順は下記になります。\n1. Unity のテストを実行するためのワークフローを選択して実行する\n2. ワークフローの実行が成功したら、詳細画面に遷移した後、Test Results の項目からテストの実行結果を確認する\nテスト実行用のワークフローファイルでは workflow_dispatch で実行可能にしていますが、pull_request を利用すればプルリク時にテストを実行させることが可能になります。\nBuilder: プロジェクトのビルドを実行して出力結果を確認する GameCI にはプロジェクトのビルドを行うための GitHub Actions も用意されています。実際に GameCI で WebGL ビルドを行いその内容を GitHub Pages で確認できるようにして動作検証していきます。\n早速 WebGL ビルドを行うためのワークフローファイルを作成していきます。\nname: Run the WebGL build on: workflow_dispatch: {} jobs: build: name: Run the WebGL build runs-on: ubuntu-latest steps: # actions/checkout@v2 を利用して作業ディレクトリに # Unity プロジェクトの中身をダウンロードしてくる - name: Check out my unity project. uses: actions/checkout@v2 # GameCI の Builder を利用して、 # Unity プロジェクトのビルドを実行する - name: Run the WebGL build uses: game-ci/unity-builder@v2 env: UNITY_LICENSE: ${{ secrets.UNITY_LICENSE }} with: # 今回は WebGL ビルドを行いたいため WebGL を指定する # WebGL 以外の指定可能な値は下記に記載の値が利用可能 # ref: https://docs.unity3d.com/ScriptReference/BuildTarget.html targetPlatform: WebGL unityVersion: 2020.3.5f1 # Builder で出力した WebGL ビルドを GitHub Pages にデプロイする - name: Deploy to GitHub Pages uses: JamesIves/github-pages-deploy-action@4.1.3 with: # GitHub Pages デプロイ用の Orphan ブランチ名を指定する branch: gh-pages # デプロイ用ビルドフォルダパスを指定する # GameCI の Builder はデフォルトでは build フォルダにビルド内容を出力する folder: build # Builder で出力した WebGL ビルドをアーティファクトでダウンロード可能にする - name: Upload the WebGL Build uses: actions/upload-artifact@v2 with: name: Build path: build 上記のワークフローファイルを GitHub Actions 上で動作検証する際の手順は下記になります。\n1. Unity の WebGL ビルドを実行するためのワークフローを実行する\n2. ワークフローの実行が成功したら、詳細画面に遷移した後、ビルド内容が正常そうか確認する\n3. ビルド内容を確認するための GitHub Pages の設定を Settings から行う\n4. GitHub Pages でブラウザから WebGL ビルドの動作確認をする\n上記のように Builder を利用することで WebGL ビルドの成否及び、最新のビルド内容を常に GitHub Pages で見られるようにできます。 すると WebGL ビルドが正常かどうかの確認が常に GitHub Pages を見れば把握できるようになるため、Unity1 週間ゲームジャム などに参加する際で便利に活用できそうです。\n:::message\nWebGL ビルドを行う際、Unity バージョンやアセットの対応状況によっては正しくブラウザ上で動作しないビルドが出力されます。ただし、ブラウザで発生するエラー内容によっては WebGL のビルド設定を見直すだけで解決できる場合があります。 例えば unityframework is not defined というエラーが発生した際は、この記事 のように WebGL の Build Settings を見直すことで解決できる場合があります。\n:::\n:::message\n私の環境では JamesIves/github-pages-deploy-action で GitHub Pages へのデプロイを行った際、デフォルトでは /WebGL/WebGL フォルダにビルド内容が出力されました。そのため、ブラウザから WebGL ビルドにアクセスする際、\u0026lt;GitHub Pages の URL\u0026gt;/WebGL/WebGL のような URL にアクセスする必要がありました。\n:::\nReturning a license: GameCI で利用している Unity License を返却する 通常利用することは無いと公式サイトにも書かれていますが、Professional License の返却も GameCI で行うことが可能です。 今回は Personal License を利用したため使用しませんでしたが、下記をワークフローのステップに組み込むことで Professional License を返却できるようです。\n# ... # どこかのタイミングでライセンスのアクティベートを行う - name: Activate Unity uses: game-ci/unity-activate@v1 env: UNITY_LICENSE: ${{ secrets.UNITY_LICENSE }} #... # ステップの最後などに game-ci/unity-return-license@v1 を呼び出して # アクティベート済みのライセンスを返却する - name: Return license uses: game-ci/unity-return-license@v1 if: always() おわりに 以前 Unity コマンドを駆使して自分で CI 環境を構築した経験があるのですが、 GameCI を利用した方が全然楽に Unity CI 環境構築を GitHub Actions 上で行えました。\nちなみに GameCI で利用されている Docker イメージ は以前からよく使われていた gableroux/unity3d が元になっているようでした。ってか GabLeRoux さんのホームページ を見たら、GameCI の開発を始めた方のようでした。すごい。\n本記事が GitHub Actions で Unity CI 環境構築を始めようとしている方の助けになれれば幸いです。\n参考リンク  GameCI - The fastest and easiest way to automatically test and build your game projects Services - Cloud Build - Unity AWS CloudFormation（テンプレートを使ったリソースのモデル化と管理）| AWS fastlane - App automation done right リポジトリのデフォルトブランチ名を管理する - GitHub Docs Unity でパーソナルライセンスのシリアルナンバーを発行する | Yucchiy\u0026rsquo;s Note Unity license manual activation webpage 暗号化されたシークレット - GitHub Docs Unity - Scripting API: BuildTarget Unity 1 週間ゲームジャム | フリーゲーム投稿サイト unityroom Unity2020 WebGL 9 割まで読み込めるがアプリが起動しない不具合の解決方法 - Qiita Deploy to GitHub Pages · Actions · GitHub Marketplace    alf ファイル及び ulf ファイルの実態は XML ファイルです。 \u0026#x21a9;\u0026#xfe0e;\n 適当なテキストエディタで ulf ファイルを開き全文をコピー \u0026amp; ペーストします。 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nikaera.com/archives/unity-gameci-github-actions/","summary":"はじめに 先日同僚が Unity の CI 環境を構築するためのライブラリである GameCI について教えてくれました。早速 GameCI の GitHub Actions を利用して、サンプルプロジェクトで色々動作検証してみたところ、Unity の CI 環境を楽に構築できることが分かりました。\nもちろん、Unity Cloud Build を利用すれば CI 環境の構築は以前から楽にできました。しかし、選択肢の 1 つとして GameCI を持っておくことで、サクッと GitHub Actions に統合する形で Unity の CI 環境を導入できるのは他には無いメリットを感じました。\n本記事で紹介しているソースコード、及び検証時に利用したプロジェクトは GitHub にアップ済みですので、手っ取り早く内容を把握されたい方は下記をご参照くださいませ。\nhttps://github.com/nikaera/Unity-GameCI-Sample\n業務でも利用できそうなので、GameCI を利用して CI 環境を構築する手順を記事でまとめました。\nGameCI に備わっている機能紹介 GameCI には現状下記の GitHub Actions が用意されているようです。\n   機能 概要     Activation Unity ライセンスを任意の Unity バージョンで発行する   Test runner Unity の PlayMode 及び EditMode のテストを実行する (テスト結果の出力にも対応)   Builder 任意の Platform ビルドを実行する (アーティファクト 利用でダウンロードも可能)   Returning a license Unity ライセンスの返却ができる (Professional License のみ対応)   Remote builder GitHub Actions のスペックでは満足のいくビルドができない際に AWS 環境でハイスペックなマシンを用意してビルドできる。ビルドのためのインフラ構築には AWS CloudFormation を使用している (現在は AWS のみ対応。今後 GCP, Azure にも対応予定とのこと)   Deployment Unity ビルドを各種 Platform 向けにデプロイする (iOS 及び Android のみ記載あり。厳密に言うと Builder でビルド出力した内容を fastlane を用いてデプロイするためのワークフロー紹介になっている)    上記を見ると既に GameCI には開発者として Unity CI に欲しい機能は最低限揃っているように見受けられました。 また本記事では、今後機会があれば試してみたいと考えていますが Remote builder 及び Deployment については言及していません。","title":"📔 GameCI で Unity の CI 環境を GitHub Actions で構築する"},{"content":"MySQL を利用する Rails プロジェクトを起動しようとしたところ、下記のエラーが発生しました。\nbin/rails s # データベースアダプターには mysql2 を選択している状態なのに postgresql で接続しようとしている Error loading the \u0026#39;postgresql\u0026#39; Active Record adapter. Missing a gem it depends on? pg is not part of the bundle. Add it to your Gemfile. (LoadError) # config/database.yml ファイルの中身一部抜粋 # mysql2 をデータベースアダプターとして利用しているため、 # PostgreSQL 接続のための pg ライブラリの追加を求めるエラーが発生しているのは何かおかしい。。 default: \u0026amp;default adapter: mysql2 #... 何でや、、と思い Rails のドキュメントを読んでいた所、公式サイトに Connection Preference に関する記述を見つけました。\n Since pool is not in the ENV[\u0026lsquo;DATABASE_URL\u0026rsquo;] provided connection information its information is merged in. Since adapter is duplicate, the ENV[\u0026lsquo;DATABASE_URL\u0026rsquo;] connection information wins.\n どうやら config/database.yml と ENV['DATABASE_URL'] の両方が設定されている場合、設定値に重複がある項目については ENV['DATABASE_URL'] の値が優先されるようでした。\n今回は DATABASE_URL に下記の PostgreSQL URL が設定されてしまっていたため、config/database.yml では MySQL を利用していたのにも関わらず、データベースアダプターに PostgreSQL が使われてしまっていたようです。\nenv | grep DATABASE_URL # 環境変数 DATABASE_URL に PostgreSQL の URL が設定されている DATABASE_URL=postgresql://user:password@localhost:5432/something_development そのため、DATABASE_URL の中身を空にすることで、本来の意図通りに config/database.yml の設定を反映させることができました。\n# 筆者は fish を利用しているので set -e で環境変数を空にした set -e DATABASE_URL # 何も出力されないことを確認し DATABASE_URL が空であることを確認する env | grep DATABASE_URL # Rails サーバーが正常に起動するか再度確認する bin/rails s #... # 無事アプリケーションサーバーが起動すること確認できれば OK =\u0026gt; Booting Puma =\u0026gt; Rails 6.0.3.6 application starting in development =\u0026gt; Run `rails server --help` for more startup options Puma starting in single mode... * Version 4.3.7 (ruby 2.6.1-p33), codename: Mysterious Traveller * Min threads: 5, max threads: 5 * Environment: development * Listening on tcp://127.0.0.1:3000 * Listening on tcp://[::1]:3000 Use Ctrl-C to stop 今回はたまたま別プロジェクトでゴニョゴニョ作業をしていた名残で環境変数 DATABASE_URL が設定されてしまっていて、かつそのままの流れで Rails プロジェクトで作業していたため遭遇してしまいました。。\nローカル環境で変数を設定する際は direnv や dotenv 等を利用して極力手動では環境変数をいじらないようにしようと思いました (所感)\n参考リンク  Configuring Rails Applications — Ruby on Rails Guides  ","permalink":"https://nikaera.com/archives/rails-database-url/","summary":"MySQL を利用する Rails プロジェクトを起動しようとしたところ、下記のエラーが発生しました。\nbin/rails s # データベースアダプターには mysql2 を選択している状態なのに postgresql で接続しようとしている Error loading the \u0026#39;postgresql\u0026#39; Active Record adapter. Missing a gem it depends on? pg is not part of the bundle. Add it to your Gemfile. (LoadError) # config/database.yml ファイルの中身一部抜粋 # mysql2 をデータベースアダプターとして利用しているため、 # PostgreSQL 接続のための pg ライブラリの追加を求めるエラーが発生しているのは何かおかしい。。 default: \u0026amp;default adapter: mysql2 #... 何でや、、と思い Rails のドキュメントを読んでいた所、公式サイトに Connection Preference に関する記述を見つけました。\n Since pool is not in the ENV[\u0026lsquo;DATABASE_URL\u0026rsquo;] provided connection information its information is merged in.","title":"📝 Rails で config/database.yml よりも ENV['DATABASE_URL'] の設定が優先される話"},{"content":"はじめに 縦書きテキストを画像に埋め込みたいと頼まれたので、\nPython 製の画像処理ライブラリ Pillow を使ってサクッと実装してみました。\n一応ソースコードは Gist にもアップ済みです ✍️\nhttps://gist.github.com/nikaera/c1049708ff548b06cab0ae377adc4ac7\n動作環境  Python 3.9.5 Pillow 8.2.0  画像に縦書きテキストを埋め込む まずは今回利用する Pillow を予めインストールしておきます。\npip install Pillow その後 main.py を作成して下記を入力します。\nテキストを埋め込みたい画像を main.py と同じフォルダに sample.jpeg という名前で配置しておきます。\n# Pillow の利用するモジュールのみをインポートする from PIL import Image, ImageDraw, ImageFont # 読み込みたいフォント情報を入力する font_name = \u0026#34;/System/Library/Fonts/ヒラギノ角ゴシック W0.ttc\u0026#34; font_size = 48 font = ImageFont.truetype(font_name, font_size) # テキストを埋め込みたい画像 sample.jpeg を読み込む im = Image.open(\u0026#39;sample.jpeg\u0026#39;) d = ImageDraw.Draw(im) # 画像に埋め込みたいテキスト情報を入力する # (後述するが、改行コードには未対応) text = \u0026#34;bifdLcFCKXtFJZmPZhzdefjhhYTtuJPAYsR\u0026#34; # 文章を改行するまでの文字数を入力する split_number = 11 # split_number で指定した文字数ごとに分割され配列に格納される # ref: https://qiita.com/yasunori/items/551a7c20ef9b81474e2a splits = [text[i: i+split_number] for i in range(0, len(text), split_number)] # 画像の width を読み込み、画像の右端の座標を取得する # top_right_margin には余白を設定する (描画領域の端が画像の端と被ってしまうため) w, _ = im.size top_right_margin = 13 right_edge = w - top_right_margin # テキストの入力領域に端が赤い四角形を描画する d.rectangle((right_edge, top_right_margin, right_edge - font_size * len(splits), font_size * split_number + top_right_margin), fill=(255, 255, 255), outline=(255, 0, 0)) # 分割した文章を上記四角形内に左にずらしながら縦書き入力する for index, item in enumerate(splits): d.text((right_edge - (font_size / 2) - font_size * index, top_right_margin), item, fill=\u0026#34;black\u0026#34;, anchor=\u0026#34;mt\u0026#34;, font=font, direction=\u0026#34;ttb\u0026#34;) # 縦書きテキストを埋め込んだ画像を test.png として出力する im.save(\u0026#34;test.png\u0026#34;) 上記ソースコード内で特筆すべき事項として d.text があります。1\nまず anchor オプション で文字を横中央に寄せて、縦を上端に寄せるよう設定しています。\n更に direction オプション を利用することで、文字列を縦に入力しています。縦に入力するためのオプションとして ttb を入力しています。\n実際に main.py を実行した際に生成される画像は下記のとおりです。\nテキストを埋め込む前の画像\nmain.py を実行してテキストを埋め込んだ画像\nおわりに 今回は Pillow を用いて縦書きテキストの画像埋め込みを実装しましたが、 ブラウザベースで埋め込みをやりたい場合は html2canvas からの png 出力ダウンロードで実装できそうでした。(ただその場合は各種ブラウザ対応とかモバイル対応が大変そう。。 👀)\n参考リンク  Pillow — Pillow (PIL Fork) 8.2.0 documentation Text anchors — Pillow (PIL Fork) 8.2.0 documentation ImageDraw Module — Pillow (PIL Fork) 8.2.0 documentation Pillow を日本語縦書きに対応させる - Qiita Embedding Japanese vertical writing characters into an image using Pillow.    当初は multiline_text 関数 を用いて改行にも対応した形でテキスト埋め込みを実装する予定でした。しかし、ValueError: ttb direction is unsupported for multiline text というエラーが発生してしまい multiline_text 関数の利用は断念しました。。😭 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nikaera.com/archives/pillow-vertical-writing/","summary":"はじめに 縦書きテキストを画像に埋め込みたいと頼まれたので、\nPython 製の画像処理ライブラリ Pillow を使ってサクッと実装してみました。\n一応ソースコードは Gist にもアップ済みです ✍️\nhttps://gist.github.com/nikaera/c1049708ff548b06cab0ae377adc4ac7\n動作環境  Python 3.9.5 Pillow 8.2.0  画像に縦書きテキストを埋め込む まずは今回利用する Pillow を予めインストールしておきます。\npip install Pillow その後 main.py を作成して下記を入力します。\nテキストを埋め込みたい画像を main.py と同じフォルダに sample.jpeg という名前で配置しておきます。\n# Pillow の利用するモジュールのみをインポートする from PIL import Image, ImageDraw, ImageFont # 読み込みたいフォント情報を入力する font_name = \u0026#34;/System/Library/Fonts/ヒラギノ角ゴシック W0.ttc\u0026#34; font_size = 48 font = ImageFont.truetype(font_name, font_size) # テキストを埋め込みたい画像 sample.jpeg を読み込む im = Image.open(\u0026#39;sample.jpeg\u0026#39;) d = ImageDraw.Draw(im) # 画像に埋め込みたいテキスト情報を入力する # (後述するが、改行コードには未対応) text = \u0026#34;bifdLcFCKXtFJZmPZhzdefjhhYTtuJPAYsR\u0026#34; # 文章を改行するまでの文字数を入力する split_number = 11 # split_number で指定した文字数ごとに分割され配列に格納される # ref: https://qiita.","title":"📝 Pillow を使って画像に縦書きテキストを埋め込む"},{"content":"はじめに 去年 DEV のアカウントを作成したものの、今まで全く有効活用出来ていませんでした。\nDEV には カノニカル URL を設定出来るので、常々 Zenn の記事を投稿する際にクロスポストしたいなと考えておりました。そこで、Zenn に記事を投稿したら、自動的に DEV にも記事を投稿 \u0026amp; 同期する GitHub Actions を作ってみました。\nhttps://github.com/nikaera/sync-zenn-with-dev-action\n今回初めて GitHub Actions を自作したのですが、その中で得た知見を残す形で記事を書くことにしました。また、GitHub Actions は TypeScript で作成しました。\n開発した GitHub Actions の概要 まずはザッとどのような GitHub Actions を作成したのか、概要について説明いたします。\nGitHub リポジトリで管理している Zenn の記事を DEV に同期して投稿する GitHub Actions を作成しました。 その際に DEV へ投稿する記事には Zenn の該当記事へのカノニカル URL も自動で設定できます。これにより DEV と Zenn へ記事をシームレスにクロスポストすることが可能となります。\n今回作成した GitHub Actions を利用するワークフローファイルの一例は下記となります。\n# .github/workflows/sync-zenn-with-dev-all.yml name: \u0026#34;Sync all Zenn articles to DEV\u0026#34; on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - name: checkout my project uses: actions/checkout@v2 - name: dev.to action step uses: nikaera/sync-zenn-with-dev-action@v1 # id を設定することで、後のジョブで Output で指定した値が参照可能になる id: dev-to with: # DEV の API キーを指定する api_key: ${{ secrets.api_key }} # (オプション) DEV に記事を投稿した際に Zenn のカノニカル URL を設定したい場合に指定する # username: nikaera # (オプション) 改行区切りで指定した articles フォルダ内のファイルパスを記載した txt ファイルを指定することで、記載された記事のみを同期するようになる。 # 他プラグインと組み合わせることで差分のみを txt ファイルに載せることが可能。詳細については後述の Outputs の項目に記載。 # added_modified_filepath: ./added_modified.txt # (オプション) Zenn の articles 以下全ての記事を常に DEV に同期するか指定する # update_all が true のときは added_modified_filepath は無視される。 # update_all: false # 上記アクション実行時に DEV に新規で同期する記事に関しては Zenn のマークダウンヘッダに # 該当する DEV の記事の ID が dev_article_id として記載されるようになる。 # 今後はその ID を元に同期するようになるため、該当する Zenn の記事をコミットする。 # 新規で同期する記事が無ければ、このジョブは実行しない。 - name: write article id of DEV to articles of Zenn. run: |git config user.name github-actions git config user.email github-actions@github.com git add ${{ steps.dev-to.outputs.newly-sync-articles }} git commit -m \u0026#34;sync: Zenn with DEV [skip ci]\u0026#34; git push if: steps.dev-to.outputs.newly-sync-articles # Outputs には DEV の記事情報 (title, url) が含まれるようになるため、 # 最後に出力して実行結果の内容を確認することもできる - name: Get the output articles. # dev-to という id が紐付いたジョブの Outputs を取得して echo で内容を出力する run: echo \u0026#34;${{ steps.dev-to.outputs.articles }}\u0026#34; 簡単に nikaera/sync-zenn-with-dev-action@v1 というジョブの内部処理について説明いたします。\n Inputs の update_all 及び added_modified_filepath で受け取った情報を元に、 どの記事を DEV に同期させるか判定する DEV に同期する記事のファイルパス一覧を取得して、 それぞれの記事のヘッダに dev_article_id の記載があるか判定する Inputs の api_key を利用して、Zenn の記事に dev_article_id が含まれていれば、 該当する DEV の記事を更新する。含まれていなければ DEV に新規で記事を作成する Inputs の username を利用して、 DEV の記事に該当する Zenn 記事のカノニカル URL を設定する DEV に新規で記事を作成した場合は、 Zenn の該当記事のヘッダに dev_article_id を書き込む DEV に記事を投稿する際、Zenn は対応しているが、 DEV では対応していない記述は削除する (::: 記法や一部のコード記法) 記事の公開ステータス及びタグなどについても DEV の記事に反映する1 新規で DEV に記事を同期した Zenn 記事のファイルパスを、 Outputs の newly-sync-articles に設定する (後のジョブで dev_article_id の含まれた記事をコミットしたいため) ワークフローで同期された記事情報は Outouts の articles に設定する  Inputs と Outputs の内容一覧については下記になります。\nInputs\n   キー 説明 必須     api_key DEV の API Key を設定する o   username Zenn の 自分のアカウント名 を設定する (DEV に同期する記事に Zenn のカノニカル URL を設定したい場合のみ) x   added_modified_filepath 改行区切りで指定した articles フォルダ内のファイルパスを記載した txt ファイルを指定することで、記載された記事のみを同期するようになる。PR やコミット差分のファイルのみを取得するための GitHub Actions jitterbit/get-changed-files@v1 と組み合わせることで、更新差分のあった記事のみを随時同期することも可能。2 更新差分のあった記事のみを随時同期するための実際のワークフローファイルはこちら x   update_all Zenn の全ての記事をどうきするかどうかを設定する。GitHub Actions 初回実行時のみ true にする使い方を想定している。デフォルトは true。added_modified_filepath よりも update_all が優先されるため added_modified_filepath を設定する場合は false を設定する必要あり` | x |     Outputs\n   キー 説明     articles 同期された DEV の記事のタイトル及び URL が格納された配列   newly-sync-articles DEV で新たに新規作成された Zenn 記事のファイルパスが格納された配列。実際のワークフローファイルの該当する記述のように、必ずコミットに含めるようにする必要がある (理由は後述)    Inputs 及び Outputs については公式サイトの説明をご参照ください。\nZenn の記事を DEV に同期するための仕組み Zenn の記事を新規で DEV に同期する際は、DEV に記事を新規作成する必要があります。その際に Zenn の記事と DEV の記事を紐付けるための何らかの仕組みが必要となります。そうしないと、今後 Zenn の記事内容を更新した際に、DEV のどの記事に内容を同期させればよいかが不明なためです。\nそこで、記事を同期するための仕組みとして、dev_article_id というフィールドを Zenn のマークダウンヘッダに追記することで DEV の同期すべき記事との紐付けを行うことにしました。dev_article_id には DEV の記事作成 API 実行時の返り値である id を設定します。\n一度 id を dev_article_id として Zenn の記事に紐付けてしまえば、次回以降に記事の同期を行う際は DEV の記事更新 API を利用できます。\nまた、Outputs の newly-sync-articles には新規で作成された DEV 記事の id である dev_article_id が追記された Zenn 記事のファイルパスが格納されています。そのため、nikaera/sync-zenn-with-dev-action@v1 実行後は、下記のように steps.dev-to.outputs.newly-sync-articles 内に格納されたファイル群をコミットに反映させる必要があります。\n# `nikaera/sync-zenn-with-dev-action@v1` 実行後に必ず定義すべきジョブ # DEV に新規に作成した記事がなければ実行しない (if: steps.dev-to.outputs.newly-sync-articles) - name: write article id of DEV to articles of Zenn. run: |git config user.name github-actions git config user.email github-actions@github.com git add ${{ steps.dev-to.outputs.newly-sync-articles }} git commit -m \u0026#34;sync: Zenn with DEV [skip ci]\u0026#34; git push if: steps.dev-to.outputs.newly-sync-articles 上記のジョブで newly-sync-articles に格納された dev_article_id が追記された Zenn 記事は随時コミットに反映しないと、Zenn の全ての記事が同期毎 DEV に新規作成され続けるという不具合を引き起こしてしまうので、ご注意ください\nGitHub Actions を開発する手順 サクッと開発に取り組みたかったため、Docker コンテナを利用する方法 ではなく、JavaScript を利用する方法 で開発を進めていくことにしました。\nTypeScript で GitHub Actions を作る GitHub 公式が TypeScript で GitHub Actions を作るための テンプレートプロジェクト を用意してくれています。今回はこのテンプレートプロジェクトを利用する形でプロジェクトを作成しました。\n (余談) GitHub Actions では Docker コンテナ を用いてワークフローを実行可能です。そのため、実行環境は自由に設定出来ます。(Go, Python, Ruby, etc.)\n 早速 TypeScript のテンプレートプロジェクトを元に自分の GitHub Actions プロジェクトを作成します。\n1. テンプレートプロジェクトを元に GitHub Actions の TypeScript プロジェクトを作成する\n2. プロジェクトの作成後 git clone してきて開発する準備を整える\nGitHub Actions プロジェクトの開発を進めるための準備を行う テンプレートプロジェクトを git clone したら、まずは action.yml の内容を変更します。 今回作成した GitHub Actions の action.yml は下記となっております。\n# action.yml # GitHub Actions のプロジェクト名 name: \u0026#34;Sync Zenn articles to DEV\u0026#34; # GitHub Actions のプロジェクト説明文 description: \u0026#34;Just sync Zenn articles to DEV.\u0026#34; # GitHub Actions の作者 author: \u0026#34;nikaera\u0026#34; # GitHub Actions に渡せる引数の値定義 inputs: api_key: # フィールドの指定が必須であれば true、必須でなければ false を設定する # DEV の API キーは同期を行う際に必須なため、true を設定している required: true # フィールドの説明文 description: \u0026#34;The api_key required to use the DEV API (https://docs.forem.com/api/#section/Authentication)\u0026#34; username: required: false description: \u0026#34;Zenn user\u0026#39;s account name. (Fields to be filled in if canonical url is set.)\u0026#34; articles: required: false description: \u0026#34;The directory where Zenn articles are stored.\u0026#34; # フィールドにはデフォルト値を指定することも可能 # Zenn の記事がデフォで格納されているフォルダ名を指定している default: articles update_all: require: false description: \u0026#34;Whether to synchronize all articles.\u0026#34; default: true added_modified_filepath: required: false description: |Synchronize only the articles in the file path divided by line breaks. You can use jitterbit/get-changed-files@v1 to get only the file paths of articles that have changed in the correct format. (https://github.com/jitterbit/get-changed-files) # GitHub Actions 実行後に参照可能になる値定義 outputs: articles: description: \u0026#34;A list of URLs of dev.to articles that have been created or updated\u0026#34; newly-sync-articles: description: \u0026#34;File path list of newly synchronized articles.\u0026#34; # GitHub Actions の実行環境 runs: using: \u0026#34;node12\u0026#34; # テンプレートプロジェクトでは コンパイル先が dist になるため `dist/index.js` を指定している main: \u0026#34;dist/index.js\u0026#34; TypeScript のテンプレートプロジェクトでは、バンドルツールとして ncc が採用されています。GitHub Actions 実行時に使用されるのは ncc によりコンパイルされた単一の JavaScript ファイル (dist/index.js) になります。\nあとは src フォルダ内でプログラムを書いて、npm run all \u0026amp;\u0026amp; node dist/index.js のようにコマンド実行しながら開発を進めていくだけです。\n (余談) GitHub Actions の開発ツールとして Docker を利用した act というものが存在するようです。ローカル環境で検証する際は 既知の問題 に対応する必要がありそうですが、GitHub Actions の開発で非常に有効活用できそうで気になっております。\n今回の開発では利用しなかったのですが、今後開発を進めていく中で利用する機会も出てきそうなので、その際は本記事内容を更新する形で知見を追記したいと考えております。\nGitHub Actions を実装する際に利用した機能 GitHub Actions を実装する際に利用した機能を、実際のコード内容を抜粋して簡単に説明していきます。下記で紹介する内容は GitHub Actions Toolkit の機能です。\n// main.ts  /** 下記の yml の with で指定した値は core.getInput で受け取ることが可能。 - name: dev.to action step uses: nikaera/sync-zenn-with-dev-action@v1 id: dev-to with: # DEV の API キーを指定する api_key: ${{ secrets.api_key }} */ core.getInput(\u0026#34;api_key\u0026#34;, { required: true }); core.getInput(\u0026#34;update_all\u0026#34;, { required: false }); /** 下記の yml の steps.\u0026lt;ジョブで指定した id\u0026gt;.outputs で参照可能な値をセットすることが可能。 セットする内容は文字列である必要がある。 - name: dev.to action step uses: nikaera/sync-zenn-with-dev-action@v1 id: dev-to - name: Get the output articles. run: echo \u0026#34;${{ steps.dev-to.outputs.articles }}\u0026#34; */ core.setOutput(\u0026#34;articles\u0026#34;, JSON.stringify(devtoArticles, undefined, 2)); core.setOutput(\u0026#34;newly-sync-articles\u0026#34;, newlySyncedArticles.join(\u0026#34; \u0026#34;)); /** GitHub Actions 実行時に出力されるログをレベルごとに出力することが可能 core.debug はローカル実行時のみに出力内容を確認することができる */ core.debug(\u0026#34;debug\u0026#34;); core.info(`update_all: ${updateAll}`); core.error(JSON.stringify(error)); 上記だけ把握してれば GitHub Actions の開発は問題なく行うことができました。\n作成した GitHub Actions を実際に GitHub 上で実行可能にする ローカル環境で一通り開発が完了したら、GitHub リポジトリに push した後タグ付けを行います。GitHub Actions はタグを設定しないと実行できないため必要な作業となります。 今回タグ付けは GitHub 上で行いました。\n1. タグの項目をクリックする\n2. Create a new release ボタンをクリックしてタグ作成画面に遷移する\n3. Publish release ボタンをクリックしてタグの作成を完了する\n上記の例では v1 というタグを作成したので nikaera/sync-zenn-with-dev-action@v1 のような記述で GitHub Actions を利用可能になりました。 私は Zenn の記事を zenn.dev というリポジトリで管理しているため、早速このリポジトリに GitHub Actions を導入してみます。\nZenn の全ての記事を DEV に同期するためのワークフロー 本記事の GitHub Actions では DEV の API キーを使用するため、事前にシークレットへ API_KEY という名称で値を登録しておきます。公式サイトの手順 に従い シークレットの登録が完了したら、該当リポジトリに .github/workflows/sync-zenn-with-dev-all.yml というワークフローファイルを作成します。\n# .github/workflows/sync-zenn-with-dev-all.yml name: \u0026#34;Sync-All Zenn with DEV\u0026#34; on: workflow_dispatch: jobs: build: runs-on: ubuntu-latest if: contains(github.event.head_commit.message, \u0026#39;[skip ci]\u0026#39;) == false steps: - name: setup node project uses: actions/checkout@v2 - name: dev.to action step uses: nikaera/sync-zenn-with-dev-action@v1 id: dev-to with: api_key: ${{ secrets.api_key }} # Zenn の自分のアカウント名を指定すると # DEV 記事のカノニカル URL に Zenn 記事の URL を指定できる # username: nikaera update_all: true - name: write article id of DEV to articles of Zenn. run: |git config user.name github-actions git config user.email github-actions@github.com git add ${{ steps.dev-to.outputs.newly-sync-articles }} git commit -m \u0026#34;sync: Zenn with DEV [skip ci]\u0026#34; git push if: steps.dev-to.outputs.newly-sync-articles - name: Get the output articles. run: echo \u0026#34;${{ steps.dev-to.outputs.articles }}\u0026#34; 上記は手動実行が可能な Zenn の記事を全て DEV に同期するためのワークフローファイルになります。作成が完了したらワークフローファイルを実行して、記事が正常に同期できているか確認してみます。\n1. Actions タブをクリックする\n2. 作成した Sync-All Zenn with DEV ワークフローファイルを選択する\n3. Run workflow ボタンを実行して、ワークフローを実行する\n4. ワークフローの実行が正常に完了していれば、ログに DEV の記事情報が出力される\n5. dev.to にアクセスして Zenn の記事情報が正常に同期されていることを確認する\n正常に Zenn の全ての記事が DEV に同期されていることが確認できたら、次は Zenn の記事が新規作成されたり、更新されたときのみに DEV に同期するためのワークフローファイルを作成します。\n更新差分のあった Zenn 記事のみを DEV に同期するためのワークフロー main ブランチが更新されたときのみ更新された記事のみを DEV に同期するためのワークフローファイルを作成します。該当リポジトリに .github/workflows/sync-zenn-with-dev.yml というワークフローファイルを作成します。\n# .github/workflows/sync-zenn-with-dev.yml name: \u0026#34;Sync Zenn with DEV\u0026#34; on: push: branches: - main jobs: build: runs-on: ubuntu-latest if: contains(github.event.head_commit.message, \u0026#39;[skip ci]\u0026#39;) == false steps: - name: setup node project uses: actions/checkout@v2 - name: get modified files id: files uses: jitterbit/get-changed-files@v1 - name: output modified files to text run: |for changed_file in ${{ steps.files.outputs.added_modified }}; do echo \u0026#34;${changed_file}\u0026#34; \u0026gt;\u0026gt; added_modified.txt done - name: dev.to action step uses: nikaera/sync-zenn-with-dev-action@v1 id: dev-to with: api_key: ${{ secrets.api_key }} # Zenn の自分のアカウント名を指定すると # DEV 記事のカノニカル URL に Zenn 記事の URL を指定できる # username: nikaera added_modified_filepath: ./added_modified.txt update_all: false - name: write article id of DEV to articles of Zenn. run: |git config user.name github-actions git config user.email github-actions@github.com git add ${{ steps.dev-to.outputs.newly-sync-articles }} git commit -m \u0026#34;sync: Zenn with DEV [skip ci]\u0026#34; git push if: steps.dev-to.outputs.newly-sync-articles - name: Get the output articles. run: echo \u0026#34;${{ steps.dev-to.outputs.articles }}\u0026#34; 上記ワークフローファイルの作成が完了したら、早速動作確認のために、まさに今執筆中の本記事内容をリポジトリに push してみます。\n1. git push 後に該当するワークフローの実行結果を確認する\n2. dev.to に執筆途中の内容で記事が同期されていることを確認する\nこれまで説明してきた 2 つのワークフローを利用することで、大抵のユースケースはカバーできるはずです。\nおわりに GitHub Actions の勉強のために取り組んだプロジェクトですが、思いの外楽しくて他にも色々な機能のアイデアがあるので随時実装していきたいと考えています。(英訳, タイトルフォーマット変更, etc.)\nDEV に Zenn の記事をクロスポストする GitHub Actions を公開することで、いつもお世話になっている Zenn というプラットフォームを海外の方に認知していただける機会を創出できたのかもと考えたらテンションが上がってきました。\nそれはさておき、Zenn の記事を他でも有効活用するための GitHub Actions を開発する際には、恐らく本記事で紹介した GitHub Actions のコードが参考になるはずです。\nまた、GitHub Actions の Marketplace というものが用意されているようなので、開発がある程度完了次第、こちらに申請するのも試してみたいと考えております。\n参考リンク  DEV Community 👩‍💻👨‍💻 dev.to supports canonical URLs so you can share content without impacting SEO... - DEV Community nikaera/sync-zenn-with-dev-action: Just sync Zenn articles to DEV. DEV API (beta) jitterbit/get-changed-files: Get all of the files changed/modified in a pull request or push\u0026rsquo;s commits. Docker コンテナのアクションを作成する - GitHub Docs JavaScript アクションを作成する - GitHub Docs actions/typescript-action: Create a TypeScript Action with tests, linting, workflow, publishing, and versioning vercel/ncc: Compile a Node.js project into a single file. Supports TypeScript, binary addons, dynamic requires. nektos/act: Run your GitHub Actions locally 🚀 actions/toolkit: The GitHub ToolKit for developing GitHub Actions. 暗号化されたシークレット - GitHub Docs DeepL Pro：テキストの他、Word などの文書をセキュアに翻訳 Qiita Crieit - プログラマー、クリエイターが何でも気軽に書けるコミュニティ    DEV (Forem) の仕様上、タグは最大でも 4 つまでしか設定できないため、Zenn で設定したタグの先頭 4 つまで DEV の記事には設定しています \u0026#x21a9;\u0026#xfe0e;\n 当然といえば当然ですが jitterbit/get-changed-files@v1 は workflow_dispatch でワークフローを手動実行した際や、force push 等でファイル差分を正確に特定できない操作には対応しておりませんので、その場合はエラーが発生します \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nikaera.com/archives/sync-zenn-with-dev-action/","summary":"はじめに 去年 DEV のアカウントを作成したものの、今まで全く有効活用出来ていませんでした。\nDEV には カノニカル URL を設定出来るので、常々 Zenn の記事を投稿する際にクロスポストしたいなと考えておりました。そこで、Zenn に記事を投稿したら、自動的に DEV にも記事を投稿 \u0026amp; 同期する GitHub Actions を作ってみました。\nhttps://github.com/nikaera/sync-zenn-with-dev-action\n今回初めて GitHub Actions を自作したのですが、その中で得た知見を残す形で記事を書くことにしました。また、GitHub Actions は TypeScript で作成しました。\n開発した GitHub Actions の概要 まずはザッとどのような GitHub Actions を作成したのか、概要について説明いたします。\nGitHub リポジトリで管理している Zenn の記事を DEV に同期して投稿する GitHub Actions を作成しました。 その際に DEV へ投稿する記事には Zenn の該当記事へのカノニカル URL も自動で設定できます。これにより DEV と Zenn へ記事をシームレスにクロスポストすることが可能となります。\n今回作成した GitHub Actions を利用するワークフローファイルの一例は下記となります。\n# .github/workflows/sync-zenn-with-dev-all.yml name: \u0026#34;Sync all Zenn articles to DEV\u0026#34; on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - name: checkout my project uses: actions/checkout@v2 - name: dev.","title":"📔 Zenn の記事を DEV に自動的に同期させる GitHub Actions 作ってみた"},{"content":"はじめに 今までは JMeter でしか負荷テストを行ったことなかったのですが、最近 PlayFab で CloudFunction の負荷テストを行う際に Gatling を初めて利用しました。\n今回の負荷テストでは、各ユーザ毎のレートリミットの制限等も考慮した実利用時を想定した形で行うことが要求されたため、単一ユーザの認証情報を使い回すことは望ましくないと考えました。そこで、複数の認証済みユーザの情報を元に PlayFab の CloudFunction の負荷テストを実施したのですが、若干実装に苦戦したため手順について記事として残しておくことにしました。\nまた、本記事では Gatling のセットアップから記載していますが、該当コードやその説明を早く見たいという方は 複数ユーザ認証を行うテストシナリオを実装する 項目をご参照ください。\n動作環境  macOS Big Sur Java OpenJDK 12.0.1  未インストールの方は事前に 公式サイトから OpenJDK をインストールしてください    Gatling の環境を整える Gatling には 2 種類のセットアップ方法が用意されています。 スタンドアローンなツールを直接公式サイトからダウンロードするか、Maven や sbt といったツール経由でダウンロードするか選択できます。\nどちらの方法でセットアップするかについてですが、新規でテストケースを Gatling で書いていく用途だと前者になり、既存のプロジェクトに Gatling を取り込む用途だと後者になるかと存じます。\n本記事では、前者のスタンドアローンなツールを直接公式サイトからダウンロードする方法で Gatling の環境をセットアップします。\n公式サイトから Gatling をダウンロードする Gatling のトップページ に遷移して、ページを 2 Ways to use Gatling の項目までスクロールした後、ダウンロードボタンをクリックします。\nDOWNLOAD GATLING'S BUNDLE にある DOWNLOAD NOW ボタンをクリックする\nファイルダウンロード後はダウンロードした zip ファイルを適当なフォルダに展開して配置します。 早速ターミナルで展開したフォルダ内にある ./bin/gatling.sh を実行して、正常にコマンドが実行できるか確認してみます。\nOS が Windows の場合は ./bin/gatling.bat を実行します。\n⊨ ./bin/gatling.sh ~/D/gatling-charts-highcharts-bundle-3.5.1 GATLING_HOME is set to /Users/nika/Desktop/gatling-charts-highcharts-bundle-3.5.1 Choose a simulation number: [0] computerdatabase.BasicSimulation [1] computerdatabase.advanced.AdvancedSimulationStep01 [2] computerdatabase.advanced.AdvancedSimulationStep02 [3] computerdatabase.advanced.AdvancedSimulationStep03 [4] computerdatabase.advanced.AdvancedSimulationStep04 [5] computerdatabase.advanced.AdvancedSimulationStep05 0 # 実行したいテストの番号を入力する、今回は適当に 0 を指定 Select run description (optional) # 実行するテストに関する説明文を入力する。何も入力する内容が無い or # 説明文の入力が完了したら Enter を入力して、実際にテストを実行する # 選択したテストの実行が開始する # (0 を入力したので computerdatabase.BasicSimulation が実行される) Simulation computerdatabase.BasicSimulation started... #... Simulation computerdatabase.BasicSimulation completed in 26 seconds Parsing log file(s)... Parsing log file(s) done Generating reports... # テストの実行が無事完了すると、結果が表示されレポートファイルが生成される。 # レポート生成先のファイルパスは実行結果の末尾に表示される。 # レポートファイルは HTML で生成されるため、適当なブラウザで開くことで内容を確認することが出来る。 ================================================================================ ---- Global Information -------------------------------------------------------- \u0026gt; request count 13 (OK=13 KO=0 ) \u0026gt; min response time 230 (OK=230 KO=- ) \u0026gt; max response time 483 (OK=483 KO=- ) \u0026gt; mean response time 324 (OK=324 KO=- ) \u0026gt; std deviation 98 (OK=98 KO=- ) \u0026gt; response time 50th percentile 259 (OK=259 KO=- ) \u0026gt; response time 75th percentile 415 (OK=415 KO=- ) \u0026gt; response time 95th percentile 476 (OK=476 KO=- ) \u0026gt; response time 99th percentile 482 (OK=482 KO=- ) \u0026gt; mean requests/sec 0.481 (OK=0.481 KO=- ) ---- Response Time Distribution ------------------------------------------------ \u0026gt; t \u0026lt; 800 ms 13 (100%) \u0026gt; 800 ms \u0026lt; t \u0026lt; 1200 ms 0 ( 0%) \u0026gt; t \u0026gt; 1200 ms 0 ( 0%) \u0026gt; failed 0 ( 0%) ================================================================================ Reports generated in 0s. Please open the following file: /Users/nika/Desktop/gatling-charts-highcharts-bundle-3.5.1/results/basicsimulation-20210314133324259/index.html ./bin/gatling.sh を実行した後、上記のような出力が確認できれば、問題なくスタンドアローン版の Gatling 環境のセットアップが完了しています。\nまた、負荷テストのレポートを確認したい場合は、出力結果にある Please open the following file: \u0026lt;レポートのファイルパス\u0026gt; に記載されているファイルをブラウザで開きます。 html 拡張子を開くアプリのデフォルトが何らかのブラウザになっているのであれば、ターミナルから open \u0026lt;レポートのファイルパス\u0026gt; を実行するのでも構いません。\nGatling でテストシナリオを実装する Gatling のテストシナリオを書く場所は ./user-files/simulations フォルダ内になります。 テストシナリオを Scala で書いた後、ファイルを ./user-files/simulations フォルダに配置します。すると、./bin/gatling.sh を実行した際の実行するテストシナリオのリストに出てくるようになります。\n簡単なテストシナリオを試しに書いてみる まずは試しに私のブログに対してのアクセス負荷を計測するためのテストを実装していきます。./user-files/simulations フォルダ内に nikaera.com フォルダを作成し、AccessSimulation.scala という負荷テストのシナリオファイルを作成します。\n// ./user-files/simulations/nikaera.com/AccessSimulation.scala  package com.nikaera import scala.concurrent.duration._ import io.gatling.core.Predef._ import io.gatling.http.Predef._ import scala.collection.mutable.ListBuffer import io.gatling.core.structure.PopulationBuilder // 1. Simulation クラスを継承してテストシナリオを実行するクラスを定義する class AccessSimulation extends Simulation { // 2. HTTP アクセスする際の設定値を入力する。  // 今回はアクセス先のベース URL を定義するための baseUrl のみ指定  val httpConf = http .baseUrl(\u0026#34;https://nikaera.com\u0026#34;) // 3. Scala の ListBuffer を用いて複数シナリオを格納する scenarios 変数を用意する  val scenarios = new ListBuffer[PopulationBuilder]() // 4. httpConf で設定した情報を元に / (https://nikaera.com) 及び  // /tech/ (https://nikaera.com/tech/) へ同時 10 アクセスするのを、  // 5秒毎に 3回実行するシナリオを作成して、scenarios 変数に格納する  val pollingApiScn = scenario(\u0026#34;Polling Simulation\u0026#34;) .exec( http(\u0026#34;Top Page\u0026#34;) .get(\u0026#34;/\u0026#34;) .check(status.is(200)) ) .exec( http(\u0026#34;Tech Page\u0026#34;) .get(\u0026#34;/tech/\u0026#34;) .check(status.is(200)) ) scenarios += pollingApiScn .inject( atOnceUsers(10), nothingFor(5 seconds), atOnceUsers(10), nothingFor(5 seconds), atOnceUsers(10) ) .protocols(httpConf); // 5. 4. で定義したシナリオを実行して https://nikaera.com のアクセス負荷を計測する  setUp( scenarios(0) ) } 再度 ./bin/gatling.sh を実行して、実際に負荷テストを行ってみます。\n⊨ ./bin/gatling.sh ~/D/gatling-charts-highcharts-bundle-3.5.1 GATLING_HOME is set to /Users/nika/Desktop/gatling-charts-highcharts-bundle-3.5.1 Choose a simulation number: [0] com.nikaera.AccessSimulation [1] computerdatabase.BasicSimulation [2] computerdatabase.advanced.AdvancedSimulationStep01 [3] computerdatabase.advanced.AdvancedSimulationStep02 [4] computerdatabase.advanced.AdvancedSimulationStep03 [5] computerdatabase.advanced.AdvancedSimulationStep04 [6] computerdatabase.advanced.AdvancedSimulationStep05 0 # 作成したテストシナリオである com.nikaera.AccessSimulation を選択して実行します。 Select run description (optional) Simulation com.nikaera.AccessSimulation started... #... Simulation com.nikaera.AccessSimulation completed in 10 seconds Parsing log file(s)... Parsing log file(s) done Generating reports... ================================================================================ ---- Global Information -------------------------------------------------------- \u0026gt; request count 60 (OK=60 KO=0 ) \u0026gt; min response time 11 (OK=11 KO=- ) \u0026gt; max response time 372 (OK=372 KO=- ) \u0026gt; mean response time 104 (OK=104 KO=- ) \u0026gt; std deviation 99 (OK=99 KO=- ) \u0026gt; response time 50th percentile 53 (OK=53 KO=- ) \u0026gt; response time 75th percentile 212 (OK=212 KO=- ) \u0026gt; response time 95th percentile 259 (OK=259 KO=- ) \u0026gt; response time 99th percentile 307 (OK=307 KO=- ) \u0026gt; mean requests/sec 5.455 (OK=5.455 KO=- ) ---- Response Time Distribution ------------------------------------------------ \u0026gt; t \u0026lt; 800 ms 60 (100%) \u0026gt; 800 ms \u0026lt; t \u0026lt; 1200 ms 0 ( 0%) \u0026gt; t \u0026gt; 1200 ms 0 ( 0%) \u0026gt; failed 0 ( 0%) ================================================================================ Reports generated in 0s. Please open the following file: /Users/nika/Desktop/gatling-charts-highcharts-bundle-3.5.1/results/accesssimulation-20210314144205502/index.html テストシナリオの実行に成功していることが確認できたら、複数ユーザ認証した情報を元に行うテストシナリオを書いていきます。\n複数ユーザ認証を行うテストシナリオを実装する 複数ユーザ認証するテストシナリオを実装していきます。PlayFab で複数ユーザの認証情報を元に CloudFunction の負荷テストを行うことを想定しています。1 テストシナリオを作成するにあたり、./user-files/simulations/ フォルダに新たに playfab.com フォルダを作成して、その中に TestCloudFunctionSimulation.scala ファイルを生成します。\n// ./user-files/simulations/playfab.com/TestCloudFunctionSimulation.scala  package com.playfab import java.io._ import java.net.{HttpURLConnection, URL} import io.gatling.core.Predef._ import io.gatling.http.Predef._ import scala.concurrent.duration._ import scala.util.Random import scala.util.parsing.json.JSON import scala.collection.mutable.ListBuffer import io.gatling.core.structure.PopulationBuilder class TestCloudFunctionSimulation extends Simulation { // PlayFab に登録されたユーザの ID 群  val playfabUsers = Array[String]( \u0026#34;user1\u0026#34;, \u0026#34;user2\u0026#34;, \u0026#34;user3\u0026#34;, \u0026#34;user4\u0026#34;, \u0026#34;user5\u0026#34;, \u0026#34;user6\u0026#34;, \u0026#34;user7\u0026#34;, \u0026#34;user8\u0026#34;, \u0026#34;user9\u0026#34;, \u0026#34;user10\u0026#34; ) // PlayFab の TitleId 及び Secret を変数に保持しておく  val playfabId = \u0026#34;XXXXX\u0026#34; val playfabSecret = \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; val playfabApiUrl = s\u0026#34;https://${playfabId}.playfabapi.com\u0026#34; // PlayFab の Login With Server Custom Id を利用して、  // ユーザの認証情報を取得するために用いる関数  def getPlayfabAuth(serverCustomId: String): Option[Any] = { val url = new URL( s\u0026#34;${playfabApiUrl}/Server/LoginWithServerCustomId\u0026#34; ) val con = url.openConnection().asInstanceOf[HttpURLConnection] HttpURLConnection.setFollowRedirects(false) con.setRequestMethod(\u0026#34;POST\u0026#34;) con.setRequestProperty(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) con.setRequestProperty(\u0026#34;X-SecretKey\u0026#34;, playfabSecret) con.setDoOutput(true) val out = new OutputStreamWriter(con.getOutputStream(), \u0026#34;UTF-8\u0026#34;) out.write(s\u0026#34;\u0026#34;\u0026#34;{ \u0026#34;CreateAccount\u0026#34;: false, \u0026#34;ServerCustomId\u0026#34;: \u0026#34;${serverCustomId}\u0026#34; }\u0026#34;\u0026#34;\u0026#34;) out.flush() out.close() con.connect() val in = con.getInputStream val br = new BufferedReader(new InputStreamReader(in, \u0026#34;UTF-8\u0026#34;)); val json = br.readLine() in.close() con.disconnect() return JSON.parseFull(json) } val httpConf = http .baseUrl(playfabApiUrl) .header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) // CloudFunction の Request Body を作成するために利用する関数  def cloudScriptDto(funcName: String, funcArgs: String): String = { return s\u0026#34;\u0026#34;\u0026#34;{ \u0026#34;FunctionName\u0026#34;: \u0026#34;${funcName}\u0026#34;, \u0026#34;FunctionParameter\u0026#34;: ${funcArgs}}\u0026#34;\u0026#34;\u0026#34; } val scenarios: ListBuffer[PopulationBuilder] = new ListBuffer[PopulationBuilder]() playfabUsers.foreach(userId =\u0026gt; { // playfabUsers で指定したユーザ ID 情報を元に、  // PlayFab の認証情報を取得利用することで、  val playfab = this.getPlayfabAuth(userId).get val map: Map[String, Option[Any]] = playfab.asInstanceOf[Map[String, Option[Any]]]; // 愚直に JSON オブジェクトのパースを行い、必要な情報を変数に取り出す。  val data = map.get(\u0026#34;data\u0026#34;).get.asInstanceOf[Map[String, Option[Any]]]; val entityTokenInfo = data.get(\u0026#34;EntityToken\u0026#34;).get.asInstanceOf[Map[String, Option[Any]]]; val entityToken = entityTokenInfo.get(\u0026#34;EntityToken\u0026#34;).get.asInstanceOf[String]; val entity = entityTokenInfo.get(\u0026#34;Entity\u0026#34;).get.asInstanceOf[Map[String, Option[Any]]]; val entityId = entity.get(\u0026#34;Id\u0026#34;).get.asInstanceOf[String]; // Test2 API については CloudFunction 実行時のパラメタを、  // ランダム指定したいため、Random を用いてパラメタを散らすようにする  val rand = new Random val values = Array( \u0026#34;value1\u0026#34;, \u0026#34;value2\u0026#34;, \u0026#34;value3\u0026#34;, \u0026#34;value4\u0026#34;, \u0026#34;value5\u0026#34; ) val values_length = values.length // アクセス負荷を調査したい CloudFunction API 群を実行する。  val pollingApiScn = scenario(s\u0026#34;PollingSimulation: ${entityId}\u0026#34;) .exec( http(\u0026#34;Test1 Api\u0026#34;) .post(\u0026#34;/CloudScript/ExecuteFunction\u0026#34;) .header(\u0026#34;X-EntityToken\u0026#34;, entityToken) .body(StringBody(cloudScriptDto(\u0026#34;Test1\u0026#34;, null))) .check(status.is(200)) ) .exec( http(\u0026#34;Test2 Api\u0026#34;) .post(\u0026#34;/CloudScript/ExecuteFunction\u0026#34;) .header(\u0026#34;X-EntityToken\u0026#34;, entityToken) .body( StringBody( cloudScriptDto( \u0026#34;Test2\u0026#34;, s\u0026#34;\u0026#34;\u0026#34;{\u0026#34;value\u0026#34;: \u0026#34;${values(rand.nextInt(values_length))}\u0026#34;}\u0026#34;\u0026#34;\u0026#34; ) ) ) .check(status.is(200)) ); // 1 ユーザあたり 3秒毎に 100回ずつ API 群を実行した際の  // 負荷テストのシナリオを scenarios 変数に格納する  scenarios += pollingApiScn .inject( atOnceUsers(100), nothingFor(10 seconds), atOnceUsers(100), nothingFor(10 seconds), atOnceUsers(100) ) .protocols(httpConf); }); // scenarios 変数に格納されたテストシナリオを並列実行する  setUp( scenarios(0), scenarios(1), scenarios(2), scenarios(3), scenarios(4), scenarios(5), scenarios(6), scenarios(7), scenarios(8), scenarios(9) ) } ザッとインラインコメントで説明を書きましたが、重要な点についてのみ補足します。 def getPlayfabAuth は PlayFab 認証するための関数となっていますが、適宜認証に用いるサービス毎で関数の内容を変更することで、他サービスで認証するための関数として利用可能です。 また playfabUsers.foreach 内で各ユーザが実行するテストシナリオを生成しつつ、それらを scenarios 変数に保持しています。そうすることで、最後に setUp 関数でシナリオをまとめてセットできるようになります。\nplayfabUsers.foreach で値を指定するのではなく CSV でテストに与えるフィードデータを定義する 方法もあります。認証部分も含めてテストシナリオを書きたい場合にも便利に利用できます。またアカウント情報を CSV ファイルに定義しておくと、テストユーザの情報を新規追加したい場合で管理が楽になります。\n上記テストシナリオのソースコードを応用することで、様々なサービスで複数ユーザ認証した情報を元に負荷テストを行うためのシナリオを作成することが可能となります。\nおわりに 今回は Gatling で複数ユーザ認証した情報を元に負荷テストする手順について書きました。\nGatling で生成されるレポートは見やすく、改善点を洗い出してコードの改善作業をするのにとても有用でした。 また、JMeter と比べて動作が軽いため気軽に実行しやすく、テストシナリオが全てコードで管理されているためシナリオの改変も素早く行うことが出来ました。\n個人的にはテストシナリオを全てコードで記述できる Gatling が気に入ったので今後も有効活用していきたいと感じました。ただ、Gatling 以外にも Python で書ける locust や JavaScript で書ける k6 など、他にも気になるツールがいくつかあったので今後試してみたいなと考えています。\n勝手に負荷テストは JMeter 一択だと思っていたのですが、負荷テストツールには色々あるようなのでプロジェクトや自分にあったものを選定していけるよう随時キャッチアップしていきたいと感じました。\n参考リンク  Apache JMeter - Apache JMeter™ Gatling Open-Source Load Testing – For DevOps and CI/CD Azure 関数を使用した PlayFab CloudScript - PlayFab | Microsoft Docs JDK Builds from Oracle Maven – Welcome to Apache Maven sbt - The interactive build tool Open Source – Gatling Open-Source Load Testing 負荷試験ツール・Gatling・CSV ファイルと Feeder を使ったテストケース Locust - A modern load testing framework Load testing for engineering teams | k6    コードの setUp でシナリオを複数指定する箇所についてですが、より良いやり方があれば是非ご教授いただけますと幸いです\u0026hellip; \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nikaera.com/archives/gatling-multiuser/","summary":"はじめに 今までは JMeter でしか負荷テストを行ったことなかったのですが、最近 PlayFab で CloudFunction の負荷テストを行う際に Gatling を初めて利用しました。\n今回の負荷テストでは、各ユーザ毎のレートリミットの制限等も考慮した実利用時を想定した形で行うことが要求されたため、単一ユーザの認証情報を使い回すことは望ましくないと考えました。そこで、複数の認証済みユーザの情報を元に PlayFab の CloudFunction の負荷テストを実施したのですが、若干実装に苦戦したため手順について記事として残しておくことにしました。\nまた、本記事では Gatling のセットアップから記載していますが、該当コードやその説明を早く見たいという方は 複数ユーザ認証を行うテストシナリオを実装する 項目をご参照ください。\n動作環境  macOS Big Sur Java OpenJDK 12.0.1  未インストールの方は事前に 公式サイトから OpenJDK をインストールしてください    Gatling の環境を整える Gatling には 2 種類のセットアップ方法が用意されています。 スタンドアローンなツールを直接公式サイトからダウンロードするか、Maven や sbt といったツール経由でダウンロードするか選択できます。\nどちらの方法でセットアップするかについてですが、新規でテストケースを Gatling で書いていく用途だと前者になり、既存のプロジェクトに Gatling を取り込む用途だと後者になるかと存じます。\n本記事では、前者のスタンドアローンなツールを直接公式サイトからダウンロードする方法で Gatling の環境をセットアップします。\n公式サイトから Gatling をダウンロードする Gatling のトップページ に遷移して、ページを 2 Ways to use Gatling の項目までスクロールした後、ダウンロードボタンをクリックします。\nDOWNLOAD GATLING'S BUNDLE にある DOWNLOAD NOW ボタンをクリックする","title":"📔 Gatling で複数ユーザ認証した情報を元に負荷テストする"},{"content":"はじめに PlayFab で CloudFunction を利用しているときに API 制限に引っかかってしまいました。負荷テストをした際に初めて気づいたのですが、公式ページにも言及が無かったため発覚が遅れてしまいました。そのため、PlayFab に依存していた機能を部分的に外す必要が出てきてしまい苦労しました。\n本記事では、上記のような事態に陥る方を減らすため、API 制限に気づくまでの軌跡を辿りながら、PlayFab の CloudFunction を利用する際の注意点について、記事として残しておきたいと思います。\nPlayFab の API 制限に引っかかった要因 PlayFab の CloudFunction を利用すると、PlayFab 経由で独自 Web API を実行することが可能になります。また、CloudFunction 経由で独自 Web API を実行すると、PlayFab ユーザ情報が含まれたパラメタが含まれた状態でリクエストが飛んでくるため、その情報を利用することでサーバーサイドで PlayFab の操作を行うことが出来るようになり大変便利です。\nそのため、あるプロジェクトでは PlayFab CloudFunction を Azure Function や AWS Lambda のような FaaS を使っている感じで利用しておりました。そして、その利用の仕方は誤りであったことに後々気づきます\u0026hellip;\n負荷テストを実装するフェーズで CloudFunction を大量に叩いてみる PlayFab の CloudFunction を実行するにあたり利用した PlayFab の API は Server-Side Cloud Script - Execute Function というものになります。\n同接 2000 人想定で負荷テストのシナリオを実装することが求められていたため、その通りシンプルに 2000 件同時に Server-Side Cloud Script - Execute Function を実行するシナリオを Gatling で組んでみました。すると、何回やっても数十件以上は必ずエラーが発生していることが分かりました。\n# Gatling で負荷テストを実行した際に 50件失敗している様子 ================================================================================ ---- Global Information -------------------------------------------------------- \u0026gt; request count 2000 (OK=1950 KO=50 ) \u0026gt; min response time 320 (OK=320 KO=354 ) \u0026gt; max response time 14459 (OK=9723 KO=14459 ) \u0026gt; mean response time 998 (OK=934 KO=3485 ) \u0026gt; std deviation 1510 (OK=1304 KO=4310 ) \u0026gt; response time 50th percentile 545 (OK=543 KO=656 ) \u0026gt; response time 75th percentile 1085 (OK=1077 KO=7209 ) \u0026gt; response time 95th percentile 2243 (OK=2029 KO=10210 ) \u0026gt; response time 99th percentile 7947 (OK=7775 KO=14353 ) \u0026gt; mean requests/sec 100 (OK=97.5 KO=2.5 ) ---- Response Time Distribution ------------------------------------------------ \u0026gt; t \u0026lt; 800 ms 1393 ( 70%) \u0026gt; 800 ms \u0026lt; t \u0026lt; 1200 ms 367 ( 18%) \u0026gt; t \u0026gt; 1200 ms 190 ( 10%) \u0026gt; failed 50 ( 3%) ---- Errors -------------------------------------------------------------------- \u0026gt; status.find.is(200), but actually found 400 50 (100.0%) ================================================================================ 正直 2000件程度の API アクセスであれば、何の問題もなく負荷テストが通ると考えていたので、この結果には驚きました。原因は何なのか調べたところ、Azure Function で PlayFab ユーザ認証を行うために利用していた Authentication - Validate Entity Token で 503 エラーが発生していることが分かりました。\n少ない API 実行件数で負荷テストを実行する場合は問題ないのですが、件数が一定数超えたタイミングで 503 エラーが返却されるようになってしまいます。しかし、たまに同じ件数を実行しているはずなのにスムーズに全件 API 実行に成功することもありました。これは何らかのレートリミット等に引っかかっているのかも知れないということで調査したところ、次の事実が判明しました。\nCloudFunction は FaaS の用途には適さない どうやら PlayFab 公式フォーラムの ある投稿 によると、PlayFab の Server API を呼び出す際は 10秒間に 1000回という制限があるようでした。 そして、この制限を突破するには商用のための契約をした後にインスタンス割当に関する交渉をすることで可能になるかも知れないとのことでした。\n Servers are rate limited to 1,000 calls per 10 seconds. What jital is highlighting that the per-player rate should be no more than a few times a minute. A server can call at a higher rate, as it is calling for a lot of users, potentially. If you need a higher limit than 1,000 per 10 seconds, you\u0026rsquo;ll want to talk to our sales team about getting on an Enterprise contract so that we can work with you on custom limits. There\u0026rsquo;s an option on the Contact Us form on the main site to message them, if you want to go that route.\n つまり、普通に PlayFab を利用している限りはプランをアップグレードしようと制限に引っかかるということが分かりました。 また、今年 1月に投稿された内容 を見るに 10秒以内に 5000以上のユーザーがログイン/登録できたとあり、もう API の 10秒間に 1000回呼び出し制限は撤廃されたのかを聞いているユーザがいたのですが、まだ撤廃されていないと返信されていたので偶然だったようでした。\nちなみに私も上記が気になったので、セッションごとにレートリミットのかかり方が変わるのか検証するために異なるユーザ情報を用いてリクエスト 2000件を並列に実行してみましたが、503 エラーは変わらず返却され続けていたので、少なくとも私の手元の環境では効果はなさそうでした。\n No, the rate limits on the Client and Server API calls has not changed. However, the rate limits are currently enforced on a per-server basis. And since the service runs a great many servers for load balancing, it is possible to exceed those limits from time to time.\n 原因の調査中 PlayFab は EC2 の us-west-2 リージョンでリクエストを受けていそうなことが分かったのですが、そのアクセス先のインスタンスがロードバランサによって分散されているため、レートリミットの制限がインスタンス先により、時と場合によって制限されるかどうかが決まってくるのかもしれないとのことでした。\n以上のことから、PlayFab の CloudFunction については Azure Function や AWS Lambda のような FaaS のような用途には使わず、あくまでもアプリケーションで補助的に利用するための独自スクリプトを動かす程度に留めて利用するのが正解なように感じました。\nPlayFab のユーザ認証情報である SessionTicket や EntityToken を利用することで、認証周りの実装部分を省くことが出来るかも知れないと思い期待していたのですが、それは別の BaaS を使うか IaaS で自前で作るのが良さそうでした。\nおわりに API の 10秒間に 1000回呼び出し制限については明示的にドキュメントに記載があるわけでもなかったため、気づくことが出来ずプロジェクト終盤で気づくという事故が起きてしまったのですが、私と似たような境遇に陥る人が少しでも減るようにと記事を書いてみました。\nとはいえ、少し調べれば出てくるような制限だったので純粋に調査不足だったなあと反省しました。。CloudFunction はとても便利ですが、利用する際は API の呼び出し制限等用法には十分お気をつけてご利用くださいませ。\nPlayFab が便利な BaaS であることに疑いの余地は無いので今後も利用すると思いますが、知見を貯めつつ効果的に使えるよう勉強していきたいと考えております。また何か知見を得たら随時ブログ記事に書き溜めていきたいと思います。\n参考リンク  Azure 関数を使用した PlayFab CloudScript - PlayFab | Microsoft Docs Server-Side Cloud Script - Execute Function (PlayFab CloudScript) | Microsoft Docs Authentication - Validate Entity Token (PlayFab Authentication) | Microsoft Docs Server API limitations - Playfab Community Server API limitations - Playfab Community  ","permalink":"https://nikaera.com/archives/playfab-api-call-limitation/","summary":"はじめに PlayFab で CloudFunction を利用しているときに API 制限に引っかかってしまいました。負荷テストをした際に初めて気づいたのですが、公式ページにも言及が無かったため発覚が遅れてしまいました。そのため、PlayFab に依存していた機能を部分的に外す必要が出てきてしまい苦労しました。\n本記事では、上記のような事態に陥る方を減らすため、API 制限に気づくまでの軌跡を辿りながら、PlayFab の CloudFunction を利用する際の注意点について、記事として残しておきたいと思います。\nPlayFab の API 制限に引っかかった要因 PlayFab の CloudFunction を利用すると、PlayFab 経由で独自 Web API を実行することが可能になります。また、CloudFunction 経由で独自 Web API を実行すると、PlayFab ユーザ情報が含まれたパラメタが含まれた状態でリクエストが飛んでくるため、その情報を利用することでサーバーサイドで PlayFab の操作を行うことが出来るようになり大変便利です。\nそのため、あるプロジェクトでは PlayFab CloudFunction を Azure Function や AWS Lambda のような FaaS を使っている感じで利用しておりました。そして、その利用の仕方は誤りであったことに後々気づきます\u0026hellip;\n負荷テストを実装するフェーズで CloudFunction を大量に叩いてみる PlayFab の CloudFunction を実行するにあたり利用した PlayFab の API は Server-Side Cloud Script - Execute Function というものになります。\n同接 2000 人想定で負荷テストのシナリオを実装することが求められていたため、その通りシンプルに 2000 件同時に Server-Side Cloud Script - Execute Function を実行するシナリオを Gatling で組んでみました。すると、何回やっても数十件以上は必ずエラーが発生していることが分かりました。","title":"📔 PlayFab の API 制限に引っかかった"},{"content":"Jest でクラスの private readonly な変数を差し替えたい時に若干引っかかったのでメモっておきます。タイトルでは Jest とありますが、本記事の内容は JavaScript でモックする際の有効な手法の 1 つとして利用することが可能です。\n結論から言うと変数を差し替えたい場合は下記のような記述になります。\nconst mockValue = \u0026#34;\u0026#34;; Object.defineProperty(service, \u0026#34;privateReadOnlyValue\u0026#34;, { value: mockValue, }); ちなみに関数を差し替えたい場合は下記のような記述になります。\nObject.defineProperty(service, \u0026#34;privateSumFunction\u0026#34;, { value: jest.fn((a, b) =\u0026gt; a + b), }); 各種テストケースで使いまわしているインスタンスの private readonly な変数をモックした場合、値をリストアしたいケースも出てきました。その場合の記述としては、下記が有効でした。\n// tmpService 変数に service インスタンスを clone して利用する const tmpService = Object.create(service); Object.defineProperty(tmpService, \u0026#34;privateReadOnlyValue\u0026#34;, { value: \u0026#34;\u0026#34;, }); 参考リンク  Object.create() - JavaScript | MDN Mocking read-only properties for a class · Issue #2227 · facebook/jest  ","permalink":"https://nikaera.com/archives/jest-private-readonly-mock/","summary":"Jest でクラスの private readonly な変数を差し替えたい時に若干引っかかったのでメモっておきます。タイトルでは Jest とありますが、本記事の内容は JavaScript でモックする際の有効な手法の 1 つとして利用することが可能です。\n結論から言うと変数を差し替えたい場合は下記のような記述になります。\nconst mockValue = \u0026#34;\u0026#34;; Object.defineProperty(service, \u0026#34;privateReadOnlyValue\u0026#34;, { value: mockValue, }); ちなみに関数を差し替えたい場合は下記のような記述になります。\nObject.defineProperty(service, \u0026#34;privateSumFunction\u0026#34;, { value: jest.fn((a, b) =\u0026gt; a + b), }); 各種テストケースで使いまわしているインスタンスの private readonly な変数をモックした場合、値をリストアしたいケースも出てきました。その場合の記述としては、下記が有効でした。\n// tmpService 変数に service インスタンスを clone して利用する const tmpService = Object.create(service); Object.defineProperty(tmpService, \u0026#34;privateReadOnlyValue\u0026#34;, { value: \u0026#34;\u0026#34;, }); 参考リンク  Object.create() - JavaScript | MDN Mocking read-only properties for a class · Issue #2227 · facebook/jest  ","title":"📝 Jest で private readonly な値をモックする方法"},{"content":"はじめに 最近 catnose99 さんの チーム個々人のテックブログを RSS で集約するサイトを作った（Next.js） を利用させていただく形で 会社のテックブログ を構築しました。\nhttps://tech.kadinche.com/\nTeam Blog Hub が Next.js で開発されているので、デプロイ先は Vercel に決めました。その際に記事を自動的に更新するために、GitHub Actions で定期的にビルドを回すようにしました。また、独自ドメインの設定も行いました。\n本記事では、Vercel プロジェクトを GitHub Actions でビルドしてデプロイするための手順についてまとめていきます。またおまけとして、Vercel での独自ドメインの設定方法、及び弊社の Team Blog Hub の運用方針についても書いていきます。\nなお、記事は下記の前提で書いておりますのでご注意くださいませ。\n Vercel でサインアップ済み  サインアップ済みでない方はこちらからサインアップ可能です   Vercel でプロジェクト作成済み  プロジェクト未作成の方はこちらから作成可能です   Vercel の GitHub 連携でプロジェクトへのリポジトリの紐付け済み  動作環境  Node.js v15.6.0 Vercel CLI 21.1.0  GitHub Actions で Vercel へデプロイする準備を行う まずは GitHub Actions で Vercel へのビルド \u0026amp; デプロイを行うための環境を整える必要があります。そのためには、各種必要となる情報を予め Vercel から取得しておく必要があります。\nVercel デプロイ時に必要となるトークンを取得する Vercel にログイン後、トップページ右上のアイコンからトークンを発行画面に遷移します。\nトップページ右上のアイコンから Settings メニューをクリックする\nトークンを発行して、その内容を控える (GitHub Actions での Vercel デプロイ時に必要となる)\n発行したトークンは GitHub Actions でのデプロイ時に利用します。\nVercel デプロイ時に必要となる情報を取得する Verce には公式が提供している CLI が存在するので、まずはそちらをインストールします。 インストールコマンドは下記になります。\n# npm の場合 npm i -g vercel # yarn の場合 yarn global add vercel その後、Vercel プロジェクトに紐付けた GitHub リポジトリのルートで vercel コマンドを実行して、手順に従って Vercel プロジェクトとの紐付けを行います。コマンドの実行に成功すると .vercel/project.json というファイルの生成が確認できるはずです。\n.vercel/project.json には GitHub Actions 経由でのデプロイ時に必要となる vercel-project-id と vercel-org-id の内容が記載されています。\n{ \u0026#34;projectId\u0026#34;: \u0026#34;\u0026lt;vercel-project-id の内容\u0026gt;\u0026#34;, \u0026#34;orgId\u0026#34;: \u0026#34;\u0026lt;vercel-org-id の内容\u0026gt;\u0026#34; } 上記の内容を GitHub リポジトリの Secrets に登録していきます。\nGitHub Actions で定期的に Vercel へデプロイする GitHub リポジトリの Secrets に .vercel/project.json の内容を登録します。今回は ORG_ID PROJECT_ID VERCEL_TOKEN を Secrets に登録します。\n   キー 値     ORG_ID .vercel/project.json の orgId   PROJECT_ID .vercel/project.json の projectId   VERCEL_TOKEN Vercel で発行したトークン    GitHub Actions 経由で Vercel へデプロイするのに必要な値を Secrets に登録しておく\nシークレットへ必要な情報が登録できたら、GitHub Actions のワークフローファイルを作成します。今回は有志で開発されている Vercel Action を利用します。\n# deploy_website.yml name: deploy website on: # 一応動確のために手動で GitHub Actions を実行可能にする # その際の引数として checkout 時の ref を渡している # default 部分はリポジトリに設定されているデフォルトブランチを指定する workflow_dispatch: inputs: ref: description: branch|tag|SHA to checkout default: \u0026#34;main\u0026#34; required: true # 毎日日本時間の 11時 に GitHub Actions が実行される (cron の時刻は UST) # 実行の際に参照されるブランチは上記の default で指定したものが使用される schedule: - cron: \u0026#34;0 2 * * *\u0026#34; jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: ref: ${{ github.event.inputs.ref }} - uses: actions/setup-node@v2 with: node-version: \u0026#34;15\u0026#34; # 投稿内容を更新するために npm run build:posts を走らせる - name: Recreate all posts shell: bash run: |npm install npm run build:posts - uses: amondnet/vercel-action@v20 with: # GitHub Actions の Secrets で作成した値を参照する形で # Vercel デプロイ時に必要となる各種パラメタを設定する vercel-token: ${{ secrets.VERCEL_TOKEN }} # Required vercel-args: \u0026#34;--prod\u0026#34; # Optional vercel-org-id: ${{ secrets.ORG_ID}} #Required vercel-project-id: ${{ secrets.PROJECT_ID}} #Required working-directory: ./ 上記ファイルを作成次第、GitHub リポジトリのデフォルトブランチにコミットして、GitHub Actions のワークフローを手動実行してみて正常にデプロイできそうか確認していきます。\nGitHub リポジトリのページに遷移後、実際に GitHub Actions のワークフローを実行してみる\nGitHub Actions のワークフローが実行されていそうか確認する\nGitHub Actions のワークフローが正常に実行完了していることを確認する\nGitHub Actions が正常に実行されていそうなことが確認できたら、Vercel 側で正常にデプロイが完了していそうかを確認します。Vercel のトップページに遷移後、該当するプロジェクトページに遷移して、 Deployments タブからデプロイ履歴を確認します。\nVercel で該当するプロジェクトページに遷移して、デプロイ履歴を確認する\nこれで一通りの作業は終了です。お疲れさまでした。\n(おまけ) 独自ドメインを紐付ける Vercel には Custom Domains という独自ドメインを紐付けるための機能が備わっています。こちらを利用すると独自ドメイン経由で Vercel プロジェクトにアクセスさせることが可能になります。\nルートドメインの場合は A レコードを、サブドメインの場合は CNAME レコードを独自ドメインの DNS レコード設定に追加するだけで、独自ドメイン経由で HTTPS アクセス可能になります。\nCustom Domains を設定して独自ドメイン経由でアクセスできるようにする (サブドメイン設定時の場合の設定方法)\nCustom Domains の状態が Valid Configuration になり次第、設定した独自ドメインへアクセスしたときに Vercel デプロイしたページが表示されるか確認する。\n(おまけ) Team Blog Hub の実際の運用 社内で話し合った結果、master ブランチはオリジナルリポジトリの最新内容を反映する場所として、release ブランチは実際のデプロイに利用するブランチとして運用することにしました。\nmaster ブランチと分離して、自分たちの会社の独自改修については release ブランチに取り込むことで、本家リポジトリに PR が出しやすいという利点があります。また、自分たちのペースで本家リポジトリの最新の master ブランチの取り込み作業ができるという利点もあります。\nブランチを分離しておくだけで、最新の本家リポジトリの新機能の取り込みが楽になる恩恵が受けられるようになり、かつ自社の独自改修の内容についても安心して release ブランチへコミットできるようになるためオススメです。\nおわりに Team Blog Hub を用いることで、簡単にカッコいいデザインで会社のテックブログを構築できました。OSS として公開してくださっている catnose さんには本当に感謝です 🙏🏻\n無理なく継続更新され続ける自社テックブログとして、ありがたく Team Blog Hug を活用させていただきつつ、PR チャンスがあれば積極的に狙っていきたいと考えています 🔫\n参考リンク  catnose99/team-blog-hub: RSS based blog starter kit for teams Introduction to Vercel - Vercel Documentation Vercel Action · Actions · GitHub Marketplace  ","permalink":"https://nikaera.com/archives/vercel-github-actions/","summary":"はじめに 最近 catnose99 さんの チーム個々人のテックブログを RSS で集約するサイトを作った（Next.js） を利用させていただく形で 会社のテックブログ を構築しました。\nhttps://tech.kadinche.com/\nTeam Blog Hub が Next.js で開発されているので、デプロイ先は Vercel に決めました。その際に記事を自動的に更新するために、GitHub Actions で定期的にビルドを回すようにしました。また、独自ドメインの設定も行いました。\n本記事では、Vercel プロジェクトを GitHub Actions でビルドしてデプロイするための手順についてまとめていきます。またおまけとして、Vercel での独自ドメインの設定方法、及び弊社の Team Blog Hub の運用方針についても書いていきます。\nなお、記事は下記の前提で書いておりますのでご注意くださいませ。\n Vercel でサインアップ済み  サインアップ済みでない方はこちらからサインアップ可能です   Vercel でプロジェクト作成済み  プロジェクト未作成の方はこちらから作成可能です   Vercel の GitHub 連携でプロジェクトへのリポジトリの紐付け済み  動作環境  Node.js v15.6.0 Vercel CLI 21.1.0  GitHub Actions で Vercel へデプロイする準備を行う まずは GitHub Actions で Vercel へのビルド \u0026amp; デプロイを行うための環境を整える必要があります。そのためには、各種必要となる情報を予め Vercel から取得しておく必要があります。","title":"📔 Vercel の定期デプロイを GitHub Actions で実現する"},{"content":"はじめに 最近 Rust を勉強するため、Actix web で Bloggimg という Web アプリケーションを作りました。その際、セッション管理のために Cookie を利用したのですが、その際の手順及び設定方法についてまとめておきます。\n本記事では Rust や Actix web のインストール方法については説明しません。Mac であれば brew install rustup して rustup-init した後、PATH に $HOME/.cargo/bin を追加するだけで大丈夫なはずです。詳細なインストール手順については 公式サイト をご参照ください。\n開発環境については VSCode の Rust Plugin がオススメです。Rustup で Rust をインストールしている場合、設定から Rustup の PATH を $HOME/.cargo/bin/rustup にするだけで利用可能です。設定手順の詳細はこちらをご参照ください。\n動作環境  Mac mini (M1, 2020)  Rust 1.49 Actix web 3 Serde 1.0    # Cargo.toml [package] name = \u0026#34;cookie_test\u0026#34; version = \u0026#34;0.1.0\u0026#34; authors = [\u0026#34;nikaera\u0026#34;] edition = \u0026#34;2018\u0026#34; # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] actix-web = \u0026#34;3\u0026#34; serde = { version = \u0026#34;1.0\u0026#34;, features = [\u0026#34;derive\u0026#34;] } Actix web で Cookie をセットする サーバー側で Cookie を設定するため、HTTP レスポンスヘッダーに Set-Cookie を含める形でセッション情報をクライアントへ渡します。その際、最低でも Cookie の属性に HttpOnly と Secure、SameSite=Strict は設定します。実際の Cookie を設定するための Actix web でのサンプルコードは下記になります。\nuse std::env; use actix_web::{App, HttpServer}; use actix_web::cookie::{Cookie, SameSite}; use actix_web::{get, web, Error, HttpRequest, HttpResponse}; use serde::{Deserialize}; /// Cookie に設定するキー /// 今回は cookie_test をキーとして使用する /// const KEY: \u0026amp;str = \u0026#34;cookie_test\u0026#34;; /// 存在していれば、HTTP Request ヘッダーから Cookie 文字列を取得する関数 /// /// # Arguments /// * `req` - actix_web::HttpRequest /// /// # Return value /// * Option\u0026lt;String\u0026gt; - key=value; key1=value1;~ のような Cookie の文字列 /// fn get_cookie_string_from_header(req: HttpRequest) -\u0026gt; Option\u0026lt;String\u0026gt; { let cookie_header = req.headers().get(\u0026#34;cookie\u0026#34;); if let Some(v) = cookie_header { let cookie_string = v.to_str().unwrap(); return Some(String::from(cookie_string)); } return None; } /// 存在していれば、特定のキーで Cookie に設定された値を取得するための関数 /// /// # Arguments /// * `key` - Cookie から取り出したい値のキー /// * `cookie_string` - get_cookie_string_from_header 関数で取得した Cookie の文字列 /// /// # Return value /// * Option\u0026lt;String\u0026gt; - Cookie に設定されている値を取得する /// fn get_cookie_value(key: \u0026amp;str, cookie_string: String) -\u0026gt; Option\u0026lt;String\u0026gt; { // 取得した Cookie 文字列を ; で分割してループで回す  let kv: Vec\u0026lt;\u0026amp;str\u0026gt; = cookie_string.split(\u0026#39;;\u0026#39;).collect(); for c in kv { // Cookie 文字列をパースして key で指定した値とマッチしたキーが存在するかチェックする  match Cookie::parse(c) { Ok(kv) =\u0026gt; { if key == kv.name() { // key で指定した値とマッチしたキーが存在していたら、その値を取得する  return Some(String::from(kv.value())); } } Err(e) =\u0026gt; { println!(\u0026#34;cookie parse error. -\u0026gt; {}\u0026#34;, e); } } } return None; } /// 特定のキーで環境変数から値を取得するための関数 /// /// # Arguments /// * `key` - 環境変数から取り出したい値のキー /// /// # Return value /// * String - 環境変数の値を文字列として取得する /// fn get_env(key: \u0026amp;str) -\u0026gt; String { match env::var(key) { Ok(value) =\u0026gt; return value, Err(e) =\u0026gt; println!(\u0026#34;ENV: ERR {:?}\u0026#34;, e), } return String::new(); } /// 環境変数に設定された HTTPS の値が 1 か判定する /// Cookie の属性に Secure を付与するか判定するのに使用する /// /// # Return value /// * bool - Secure 属性を付与するか判定するための真偽値 /// fn is_https() -\u0026gt; bool { return get_env(\u0026#34;HTTPS\u0026#34;) == \u0026#34;1\u0026#34;; } /// Cookie に設定する値を扱う HTTP Query の定義 #[derive(Deserialize)] pub struct CookieQuery { pub value: String, } /// Cookie を設定するために用意したルート /// /// # Example /// /// 例えば GET /cookie?value=test にアクセスした場合、 /// Cookie に cookie_test=test が設定されるようになる /// #[get(\u0026#34;/cookie\u0026#34;)] async fn set_cookie(query: web::Query\u0026lt;CookieQuery\u0026gt;) -\u0026gt; Result\u0026lt;HttpResponse, Error\u0026gt; { // 設定したい Cookie を作成する  // その際に Secure, HttpOnly, SameSite=Strict 属性を付与する  let cookie = Cookie::build(KEY, \u0026amp;query.value) .secure(is_https()) .http_only(true) .same_site(SameSite::Strict) .finish(); // 作成した Cookie を HTTP Response の Set-Cookie ヘッダーに含めることで、  // HTTP Response を受け取ったクライアントに Cookie をセットさせる  return Ok(HttpResponse::Ok() .header(\u0026#34;Set-Cookie\u0026#34;, cookie.to_string()) .body(\u0026#34;\u0026#34;)); } /// KEY で指定した Cookie が存在すれば、その値を返却する /// KEY で指定した Cookie が存在しなければ、空の文字列を返却する #[get(\u0026#34;/\u0026#34;)] async fn index(req: HttpRequest) -\u0026gt; Result\u0026lt;HttpResponse, Error\u0026gt; { let cookie_string = get_cookie_string_from_header(req); if let Some(s) = cookie_string { if let Some(v) = get_cookie_value(KEY, s) { return Ok(HttpResponse::Ok().body(v)); } } return Ok(HttpResponse::Ok().body(\u0026#34;\u0026#34;)); } #[actix_web::main] async fn main() -\u0026gt; std::io::Result\u0026lt;()\u0026gt; { HttpServer::new(|| { App::new() .service(set_cookie) .service(index) }) .bind(\u0026#34;0.0.0.0:8080\u0026#34;)? .run() .await } ザッとインラインコメントで説明していますが、\n最も重要な set_cookie 関数について簡単に説明します。\nActix web には Cookie クラスが存在します。この Cookie クラスは Cookie 文字列を生成したり、パースしたりするのに役立ちます。set_cookie 関数では、Cookie を生成するための関数 Cookie::build を利用しています。\nCookie::build 関数を利用することで、メソッドチェインで Cookie の値や属性を設定できます。作成した Cookie は to_string 関数を使用することで文字列として出力できます。出力した Cookie 文字列を HTTP レスポンスヘッダーに Set-Cookie として設定すれば Cookie を設定できます。\n動作検証 今回用意した Actix web のサンプルコードには 2 つのエンドポイントを用意しました。\n   URI 説明     GET /cookie value クエリで HttpOnly な Cookie を設定する   GET / GET /cookie で設定した Cookie を確認する    cargo run で Actix web のサンプルを起動した後に、ブラウザで http://localhost:8080/cookie?value=sample にアクセスしてみます。またその際に HTTP レスポンスヘッダーを確認したいため、開発者ツールを開いておきます。\nHTTP レスポンスヘッダーに Set-Cookie が含まれていることを確認する\nSet-Cookie が含まれていることが確認できたら正常に Cookie が設定されているか確認します。\nHTTP リクエストヘッダーの Cookie に cookie_test=sample が存在していることを確認する\n実際にブラウザーにも Cookie が正しく設定されているか、開発者ツールで確認する\n正常に Cookie がセットされていることが確認できれば作業完了です。Cookie の属性に Secure を設定した場合の動作検証は、環境変数に HTTPS=1 をセットして cargo run で可能です。\nおわりに Actix web で割と汎用的に使えそうな知識として Cookie の設定方法について、メモ的な記事を書いてみました。引き続き、Rust への理解を深めるために Bloggimg の開発を進めながら学習を進めていきます 🧑‍🎓\n本記事の内容がセキュリティの観点から適切でない場合等はコメントでご指摘いただけますと幸いです。\n参考リンク  Install Rust - Rust Programming Language Rust - Visual Studio Marketplace VSCode で Rust インストールしたのに「Rustup not available」が出るとき (備忘録) - TAKOYAKING’s blog Set-Cookie - HTTP | MDN actix/actix-web: Actix Web is a powerful, pragmatic, and extremely fast web framework for Rust. actix_web::http::Cookie - Rust ブラウザー開発者ツールとは？ - ウェブ開発を学ぶ | MDN  ","permalink":"https://nikaera.com/archives/cookie-rust-actix-web/","summary":"はじめに 最近 Rust を勉強するため、Actix web で Bloggimg という Web アプリケーションを作りました。その際、セッション管理のために Cookie を利用したのですが、その際の手順及び設定方法についてまとめておきます。\n本記事では Rust や Actix web のインストール方法については説明しません。Mac であれば brew install rustup して rustup-init した後、PATH に $HOME/.cargo/bin を追加するだけで大丈夫なはずです。詳細なインストール手順については 公式サイト をご参照ください。\n開発環境については VSCode の Rust Plugin がオススメです。Rustup で Rust をインストールしている場合、設定から Rustup の PATH を $HOME/.cargo/bin/rustup にするだけで利用可能です。設定手順の詳細はこちらをご参照ください。\n動作環境  Mac mini (M1, 2020)  Rust 1.49 Actix web 3 Serde 1.0    # Cargo.toml [package] name = \u0026#34;cookie_test\u0026#34; version = \u0026#34;0.","title":"📔 Actix web で HttpOnly な Cookie を設定する"},{"content":"はじめに Actix web で Web アプリケーションを作ったのですが、技術勉強も兼ねていたので、デプロイ先も今まで試したことがないものを試そうとしていました。そこで、日頃業務でも AWS を利用しているということもあり、去年末に発表された AWS Lightsail Containers をデプロイ先に採用しました。\nAWS Lightsail Containers へのデプロイ自体は非常に簡単でした。また、デプロイにあたり Rust の Docker イメージ作成のやり方も学べました。今回はそのあたりの手順をまとめる形で記事として書き残しておくことにしました。\nActix web の Docker イメージを作成する 開発したアプリケーションでは React でフロントエンド開発をしていて、ビルドしたものを Actix web の public フォルダに配置する形で公開しています。そのため、下記の Dockerfile ではマルチステージビルドを利用しておりますが、本質的には FROM rust:1.49 以降の記述が Actix web に関するものとなります。\n# React ビルド用のイメージFROMnode:14.15.4-alpine3.10 as client_builderARG REACT_APP_API_URLARG REACT_APP_GYAZO_AUTH_URLARG REACT_APP_GA_UNIVERSAL_IDWORKDIR/clientCOPY ./client/package*.json .RUN yarn installADD ./client .RUN yarn build# Actix web ビルド用のイメージFROMrust:1.49# Actix web にアクセスするためのポートを公開するEXPOSE8080# Actix web プロジェクトのフォルダをイメージに追加するWORKDIR/serverADD ./server .# プロジェクトフォルダ内で `cargo install` してビルドを生成するRUN cargo install --path .# 不要になったファイル群を削除するRUN ls | grep -v -E \u0026#39;templates\u0026#39; | xargs rm -r# React ビルド用のイメージでビルドした内容を Actix web ビルド用イメージに追加するCOPY --from=client_builder /client/build ./buildRUN mkdir tmp# `cargo install` コマンドで生成したビルドを実行して Actix web を起動する# 下記のコマンド名称は Cargo.toml 内の [package.name] に準ずるCMD [\u0026#34;bloggimg-server\u0026#34;]また、Docker ビルド時のオプション管理を楽にするため、Docker Compose を利用しました。単一の Docker イメージをビルドする際にも利用しておくことで、後々コンテナを追加して連携させたいときにも即座に対応できたりでオススメです。\n# docker-compose.yml # context に Actix web プロジェクトのパスを指定する # args に Docker ビルド時に利用したい ARGS の値を環境変数で設定する # image に Docker の \u0026lt;イメージ名:タグ名\u0026gt; を指定する (今回は Docker Hub にデプロイする想定) # env_file に開発/動作検証時に利用したい dotenv ファイルを指定する # ports にポートマッピングの設定を書く version: \u0026#34;3.8\u0026#34; services: app: build: context: ./ args: - REACT_APP_API_URL=${REACT_APP_API_URL} - REACT_APP_GYAZO_AUTH_URL=${REACT_APP_GYAZO_AUTH_URL} - REACT_APP_GA_UNIVERSAL_ID=${REACT_APP_GA_UNIVERSAL_ID} image: n1kaera/bloggimg:v1.0.0 env_file: - ./server/.env ports: - 8080:8080 上記を自分の Actix web プロジェクトに応じて改変し docker-compose up して動作検証します。動作検証ができ次第、docker-compose build を実行して Docker イメージをビルドします。ビルドに成功したら次は Docker Hub にイメージを push します。\nDocker Hub にビルドしたイメージを push する 今回は AWS Lightsail Containers で使用するイメージの管理に Docker Hub を利用します。Docker Hub へ push する前に docker login --username=\u0026lt;Docker Hub のユーザ名\u0026gt; コマンドで Docker Hub へのログインを済ませておきます。\nその後 docker-compose push コマンドで Docker イメージを Docker Hub に push します。\nDocker Hub のページから、正常に Docker イメージが push できていそうか確認する\nDocker イメージの push が成功していることを確認できたら、残りは AWS Console 上での作業になります。\nAWS Console から Lightsail Containers Service を作成する AWS Console にログイン後、Lightsail サービス を選択して Lightsail サービスのトップページへ遷移します。遷移したら Containers タブを選択し、Create container services ボタンから Container Service を作成します。\nAWS Console へログイン後 Lightsail のページに遷移して、Containers タブを選択する\nContainers タブを選択すると出てくる、Create container services ボタンをクリックする\nCreate container services ボタンをクリックした遷移先の画面で、リージョンやキャパシティ (Micro であれば 3 ヶ月間のみ無料で利用可能) 等を選択して、名称を入力します。今回は最初にデプロイのための準備をすでに済ませているので、Container Service を作成するついでにデプロイ設定も行います。\nデプロイ設定は Set up your first deployment の項目から行うことが可能です。\nSet up deployment の部分をクリックして、デプロイの設定項目を表示する\nDocker Hub イメージを利用してデプロイする際に必要な設定項目を入力する\nコンテナのヘルスチェックのための情報を入力する\nすべての情報入力が完了したら Create container services ボタンをクリックする\n遷移後の画面下部の Deployment versions からデプロイ状況の確認が行える\n正常にデプロイできていれば Deployment versions の項目が Active になる\nデプロイが完了したら Public domain が発行されているはずなので、正常にアクセスして Web アプリケーションが利用できそうか確認します。Public domain は該当する Container Service のトップページから確認できます。\nAWS Lightsail Containers のトップページにある Public domain から動作検証する\n一通りの動作検証を行い、正常にデプロイできていそうか確認する\nこれで作業は完了です。新しい Docker イメージでデプロイし直したい場合は、Deployments タブの Modify your deployment リンクをクリックすれば可能です。\n(おまけ) 独自ドメインで Container Service へアクセス可能にする AWS Lightsail Containers では独自ドメインの紐付け及び、HTTPS 化も簡単に設定できます。Custom domains タブを選択した後、画面下部にある Create certificate リンクをクリックすることで設定画面を表示します。\nCustom domain タブをクリックしてから、Create certificate リンクをクリックする\n各種設定項目の入力が完了したら Create ボタンをクリックする\nドメイン検証のために、CNAME レコードの設定を求められるので各自で設定作業を行う\n正常に CNAME レコードを設定した後、しばらく経つと Status が Valid になる\n上記まで確認したら、Create certificate で設定したドメインの CNAME レコードに Public domain の値を設定しておきます。設定内容が反映され次第、独自ドメインへアクセスすることで HTTPS 経由で Container Service へアクセスできるようになります。\nCustom domains で Container Service で起動しているサービスにアクセスできることを確認する\nおわりに AWS Lightsail Containers を利用して Actix web プロジェクトをデプロイする手順について簡単にまとめてみました。便利ではあるものの、個人開発で利用する分には価格面及び性能面で Lightsail Instance のほうが良いなと現時点では感じてしまいました。\nしかし、日本リージョンが用意されていたりロードバランサーを備えていたり、簡単にスケールさせやすくかつ定額で利用可能なサービスであるというメリットを活かせる場面があれば有効活用できそうだなと感じました。\n参考リンク  Lightsail コンテナ: クラウドでコンテナを実行する簡単な方法 | Amazon Web Services ブログ rust - Docker Hub Enabling custom domains for your Amazon Lightsail distributions | Lightsail Documentation  ","permalink":"https://nikaera.com/archives/aws-lightsail-containers-rust-actix-web/","summary":"はじめに Actix web で Web アプリケーションを作ったのですが、技術勉強も兼ねていたので、デプロイ先も今まで試したことがないものを試そうとしていました。そこで、日頃業務でも AWS を利用しているということもあり、去年末に発表された AWS Lightsail Containers をデプロイ先に採用しました。\nAWS Lightsail Containers へのデプロイ自体は非常に簡単でした。また、デプロイにあたり Rust の Docker イメージ作成のやり方も学べました。今回はそのあたりの手順をまとめる形で記事として書き残しておくことにしました。\nActix web の Docker イメージを作成する 開発したアプリケーションでは React でフロントエンド開発をしていて、ビルドしたものを Actix web の public フォルダに配置する形で公開しています。そのため、下記の Dockerfile ではマルチステージビルドを利用しておりますが、本質的には FROM rust:1.49 以降の記述が Actix web に関するものとなります。\n# React ビルド用のイメージFROMnode:14.15.4-alpine3.10 as client_builderARG REACT_APP_API_URLARG REACT_APP_GYAZO_AUTH_URLARG REACT_APP_GA_UNIVERSAL_IDWORKDIR/clientCOPY ./client/package*.json .RUN yarn installADD ./client .RUN yarn build# Actix web ビルド用のイメージFROMrust:1.49# Actix web にアクセスするためのポートを公開するEXPOSE8080# Actix web プロジェクトのフォルダをイメージに追加するWORKDIR/serverADD ./server .# プロジェクトフォルダ内で `cargo install` してビルドを生成するRUN cargo install --path .","title":"📔 AWS Lightsail Containers に Actix web をデプロイする"},{"content":"はじめに いつもブログ記事に載せるキャプチャ画像の編集 \u0026amp; アップロード先として Gyazo を利用させていただいているのですが、日々使っている中で不満に感じる点もちょくちょく出てくるようになってきました。\nそのため、3 連休を用いて Rust の勉強がてら Bloggimg というウェブアプリケーションを作ってみました。ソースコードは MIT ライセンスで GitHub のリポジトリにアップしております。ちなみに最初は Gyazo for Blog という名称で開発をしていたため、本記事内のスクショには Gyazo for Blog という文字列が出てきますが、現在は Bloggimg という名称になっております。。\nBloggimg を開発したのは、ブログ記事を書く際に利用する画像のアップロードから加工、マークダウンとして利用するまでのフローを最適化したかったからです。 ブログ記事を書く際に、記事内で用いるスクショ画像の加工や、そのアップロードにすごく時間を取られてしまうなーと日頃から感じていたのでそれを解決したかったのです。✅\n開発中に得た知見等については別途技術記事として書いて残す予定です。\n考えていたこと 今回 Bloggimg の開発を行うに当たり、考えていた点は下記になります。\n 画像の編集ツールは引き続き Gyazo に用意されているものを使う  既に最高に使いやすい 👑   キャプチャ画像をアップロードする際に、自動的に特定のコレクションに紐付けるようにする  技術記事毎にコレクションを分けて管理しているため、技術記事を書いている最中にアップするキャプチャ画像は全て特定のコレクションにまとまっていて欲しい   ワークスペースのようなツールを目指し、ブログを書く時だけに使える機能を開発する  例えば、ワンクリックで画像マークダウンの記述がコピーできたり、画像のアップロードをし直しやすくするため画像削除がお手軽に出来るよう削除ボタンに即アクセス出来るようにしたり\u0026hellip;    特にアップした画像を 自動的に特定のコレクションに紐付けるようにする については本記事で紹介しているウェブアプリケーションを作成するキッカケとなった点なので外せない点でした。\n使い方 Bloggimg の使い方についてご紹介いたします。\nログインする Bloggimg を利用するためには、まず Gyazo アカウントでログインして頂く必要がございます。トップページの右上にあるログインボタンから Gyazo アカウントでログインします。\n1. トップページ右上に配置されたログインボタンから Gyazo アカウント認証を行う\n2. Gyazo アカウント認証が正常に完了したら、再度トップページを開く\n3. トップページを開いた時に Gyazo にアップした直近の画像が確認できるはずです\nログアウトする ウェブアプリケーションからログアウトするには、ログイン後にトップページ右上に表示される ログアウト ボタンをクリックすることでログアウトできます。\nログイン後にトップページ右上に表示される ログアウト ボタンをクリックしてログアウトする\n画像ファイルをアップロードする 画像は一枚でも複数枚でもアップロードすることが可能です。画像アップロードの方法としてドラッグ \u0026amp; ドロップとファイル選択ダイアログを用意しております。\n画面中央の点線枠内に画像ファイルをドラッグ \u0026amp; ドロップするか、点線枠内をクリックしてファイル選択ダイアログから選択することで画像をアップロードできる\n画像ファイルをアップロードする際に自動でコレクションを紐付ける Gyazo トップページ左端にコレクションリストが表示されているので、画像を紐づけたいコレクションを選択します。新たにコレクションを作成する場合はコレクションリスト上部にある コレクションを作成 ボタンをクリックします。\n1. コレクションリストの中から画像を紐づけたいコレクションを選択する\n2. コレクションを選択後に遷移した先の URL 末尾のコレクション ID をコピーする\n3. トップページの最上部に 2. で控えていたコレクション ID をペーストする\n上記までのステップが完了し、正しくコレクション ID が入力できていれば、次回以降のファイルアップロード時に自動で指定したコレクションに画像が紐づくようになります。\nアップロードした画像ファイルを編集する 画像ファイルのアップロード時や 画像の再読み込み ボタンをクリックすることで、最新 20 件の画像リストを画面最下部にロードされます。画像リストの各項目ではプレビュー、編集、削除、マークダウンのコピーを行うことが可能です。\n画像リストの各項目の機能概要図\nプレビュー サムネ画像をクリックすることで、Gyazo にアップした元画像をプレビューすることが可能です。サムネ画像では画像の判別がしにくい場合に詳細を確認するための機能となります。\nアップした画像の詳細を確認するためにプレビュー機能を利用する\n編集 編集は該当画像の Gyazo ページにて行えるように、タイトルをクリックすることで Gyazo ページを別タブで開きます。\n別タブで開いた Gyazo ページから画像の編集作業を行う\n削除 画像の削除は 画像の削除 ボタンをクリックすることで、削除を行うための画面に遷移します。削除しようとしている画像で間違いないか確認後、削除を行うという手順になっております。\nGyazo から選択した画像を削除する\nマークダウンのコピー マークダウンをコピー ボタンをクリックすることで、クリップボードにマークダウン形式で該当画像を表示するための記述をコピーすることができます。具体的には下記のような記述がコピーされます。\nブログを書く先がマークダウン形式での記述に対応していれば、そのままペーストするだけで画像を表示することが可能です。\n![スクリーンショット 2021-01-11 17.10.11.png](https://i.gyazo.com/a3e219b5efb8494103432b369ee99534.png) おわりに この記事を書くのにも実際に Bloggimg を用いましたが、個人的に作業効率は上がったように感じました。ブログを書くという用途に Gyazo を利用されている方のお役に立てれば幸いです。\nまた、今後は下記の機能実装を進めていく予定です。\n 画像アップ時に自動でアスペクト比を維持した状態で画像のリサイズを自動で行う機能 画像アップ時のタイトルの接頭辞が指定できるようにする機能 編集した画像が自動的にコレクションに紐づく機能  心残りな点として編集した画像をコレクションに紐付ける機能は API でできなかったため、現在手動で行う必要があります。。Gyazo の API \u0008 がコレクションの紐づけにも対応したら対応したいと考えています ✅    ","permalink":"https://nikaera.com/archives/bloggimg-first-release/","summary":"はじめに いつもブログ記事に載せるキャプチャ画像の編集 \u0026amp; アップロード先として Gyazo を利用させていただいているのですが、日々使っている中で不満に感じる点もちょくちょく出てくるようになってきました。\nそのため、3 連休を用いて Rust の勉強がてら Bloggimg というウェブアプリケーションを作ってみました。ソースコードは MIT ライセンスで GitHub のリポジトリにアップしております。ちなみに最初は Gyazo for Blog という名称で開発をしていたため、本記事内のスクショには Gyazo for Blog という文字列が出てきますが、現在は Bloggimg という名称になっております。。\nBloggimg を開発したのは、ブログ記事を書く際に利用する画像のアップロードから加工、マークダウンとして利用するまでのフローを最適化したかったからです。 ブログ記事を書く際に、記事内で用いるスクショ画像の加工や、そのアップロードにすごく時間を取られてしまうなーと日頃から感じていたのでそれを解決したかったのです。✅\n開発中に得た知見等については別途技術記事として書いて残す予定です。\n考えていたこと 今回 Bloggimg の開発を行うに当たり、考えていた点は下記になります。\n 画像の編集ツールは引き続き Gyazo に用意されているものを使う  既に最高に使いやすい 👑   キャプチャ画像をアップロードする際に、自動的に特定のコレクションに紐付けるようにする  技術記事毎にコレクションを分けて管理しているため、技術記事を書いている最中にアップするキャプチャ画像は全て特定のコレクションにまとまっていて欲しい   ワークスペースのようなツールを目指し、ブログを書く時だけに使える機能を開発する  例えば、ワンクリックで画像マークダウンの記述がコピーできたり、画像のアップロードをし直しやすくするため画像削除がお手軽に出来るよう削除ボタンに即アクセス出来るようにしたり\u0026hellip;    特にアップした画像を 自動的に特定のコレクションに紐付けるようにする については本記事で紹介しているウェブアプリケーションを作成するキッカケとなった点なので外せない点でした。\n使い方 Bloggimg の使い方についてご紹介いたします。\nログインする Bloggimg を利用するためには、まず Gyazo アカウントでログインして頂く必要がございます。トップページの右上にあるログインボタンから Gyazo アカウントでログインします。\n1. トップページ右上に配置されたログインボタンから Gyazo アカウント認証を行う","title":"📔 ブログを書く用途に特化した Gyazo のツールを開発してみた"},{"content":"コンテナをホットスタンバイさせるために EC2 でインスタンス起動して cron で ping 飛ばしていたのですが、コスト的に勿体ないなーと思っていました。しかし、「AWS Lambda 使えばいいじゃん」という指摘を受け、確かにってなったので cron で定期実行していた ping 処理を AWS Lambda + EventBridge で置き換えました。\n実は Heroku Scheduler とか使って同様のことをしていた時期もあったのですが、10 分毎しか実行できない制約があったりして使い勝手が悪かったので、後々も使っていけそうな知見な気がしたのでメモがてら記事で残しておくことにしました。\nまず、AWS Console から Lambda サービスを選択して関数を新たに作成します。\n1. AWS Lambda のトップ画面から関数作成のための画面に遷移する\n2. 必要な情報を入力して Lambda の関数を作成する\n関数が作成でき次第、ping 処理を書いていきます。http リクエストを行うためのライブラリとして Node.js の標準モジュール(https) を利用します。\nLambda 関数作成直後の index.js は下記のような記述になっていると思います。\n// index.js exports.handler = async (event) =\u0026gt; { // TODO implement  const response = { statusCode: 200, body: JSON.stringify(\u0026#34;Hello from Lambda!\u0026#34;), }; return response; }; こちらを Node.js の標準モジュール(https) を利用する形で下記のように書き換えます。\n// index.js // \u0026#34;https://www.google.com/\u0026#34; に HTTP リクエストを実行する (ping) const https = require(\u0026#34;https\u0026#34;); const httpRequest = async (url) =\u0026gt; { return new Promise((resolve, reject) =\u0026gt; { const req = https.request(url, (res) =\u0026gt; { let body = \u0026#34;\u0026#34;; res.on(\u0026#34;data\u0026#34;, (chunk) =\u0026gt; { body += chunk; }); res.on(\u0026#34;end\u0026#34;, () =\u0026gt; { console.log(`response: ${body}`); resolve(body); }); }); req.on(\u0026#34;error\u0026#34;, (e) =\u0026gt; { console.error(e); reject(e); }); req.end(); }); }; exports.handler = async (event) =\u0026gt; { const body = await httpRequest(\u0026#34;https://www.google.com/\u0026#34;); return { statusCode: 200, body, }; }; その後、右上にある Deploy ボタンをクリックして関数にソースコードを反映します。実際に関数が意図したとおりに動作するか、Test ボタンをクリックして動作検証してみます。\n1. Test ボタンをクリックします\n2. 動作検証時のパラメーターを入力してテスト環境を作成する\n3. 2. のテスト環境で関数の動作検証を行い正常に実行できていることを確認する\n正常に関数が実行できていること確認できれば、後は定期実行可能にすれば作業完了です。定期実行するためのスケジューラには EventBridge を利用します。Add trigger ボタンから EventBridge を追加します。\n1. Add trigger ボタンからトリガー追加画面に遷移する\n2. EventBridge トリガーを追加して定期実行の設定を行う\n3. EventBridge トリガーの追加が無事に完了したことを確認する\nまた 2. では 1 分毎に実行するスケジュールを設定しましたが、EventBridge の書式 を用いてより複雑なスケジュール設定を行うことも可能です。\n最後に本当に定期実行されていて、関数の実行も正常に行われていそう確認します。Monitoring タブをクリックして、関数の実行状況を確認していきます。\n1. Monitoring タブをクリックする\n2. Lambda 関数が定期実行されていることを確認する\n3. Lambda 関数の実行結果が正しいことも確認する\nこれで作業完了です。お疲れさまでした。\n","permalink":"https://nikaera.com/archives/aws-lambda-cron/","summary":"コンテナをホットスタンバイさせるために EC2 でインスタンス起動して cron で ping 飛ばしていたのですが、コスト的に勿体ないなーと思っていました。しかし、「AWS Lambda 使えばいいじゃん」という指摘を受け、確かにってなったので cron で定期実行していた ping 処理を AWS Lambda + EventBridge で置き換えました。\n実は Heroku Scheduler とか使って同様のことをしていた時期もあったのですが、10 分毎しか実行できない制約があったりして使い勝手が悪かったので、後々も使っていけそうな知見な気がしたのでメモがてら記事で残しておくことにしました。\nまず、AWS Console から Lambda サービスを選択して関数を新たに作成します。\n1. AWS Lambda のトップ画面から関数作成のための画面に遷移する\n2. 必要な情報を入力して Lambda の関数を作成する\n関数が作成でき次第、ping 処理を書いていきます。http リクエストを行うためのライブラリとして Node.js の標準モジュール(https) を利用します。\nLambda 関数作成直後の index.js は下記のような記述になっていると思います。\n// index.js exports.handler = async (event) =\u0026gt; { // TODO implement  const response = { statusCode: 200, body: JSON.stringify(\u0026#34;Hello from Lambda!\u0026#34;), }; return response; }; こちらを Node.","title":"📝 AWS Lambda で cron みたいに定期実行する"},{"content":"はじめに 本記事のカバー画像は Vladimir Fedotov による Unsplash の画像です。\n最近「ちゃんと理解してくれた？」ということを聞かれた時に「理解したけど、そういう行動を取るつもりはない」というと、「理解してないじゃん」と言われることが何回かあったので、その時の対処法及び感じた心情などをこの記事で吐露したいと思います。\n年始からこういう記事を書くのは気が引けたのですが、あまりにも本記事タイトルのような方と多く遭遇したため\u0026hellip;\n理解するということ 「理解する」というのは、相手の言動の意味が正しく分かったということを意味します。 その言動をそのまま自身に取り入れることは意味しません。例えば、犯罪者の行動理由が正しく分かったところで、それを自分の行動規範に取り入れることはしないはずです。\n双方がコミュニケーションを取り正しく分かったという状態になっている前提ですが、「理解したけど、そういう行動を取るつもりはない」という発言を受けてから「理解してないじゃん」という言動を発してしまうのは勘違いも甚だしいです。\n上記はもはや理解した・してないとかの次元ではなく、相手を自分の言動でコントロールしたいがために会話している状態と捉えられても文句は言えません。 一種のマインドコントロールに近いかもしれません。\nちなみにこの傾向は、直接話したいと言ってくる人に多いです。しかも、そういった人に限り相手の都合を考慮せずに何回も同期的なコミュニケーションを取ろうとしてきたり、スグに語気を強めて自分が不満であることを感情でアピールしようとしてきます。また、「絶対」 とか 「必ず」 とか 「必要」 という言葉を不適切に多用する傾向が多いです。\n例えば、突然語気が強くなったり、攻撃的なメッセージの後に「電話して」等送ってくる人らが該当します。更に特筆すべき事項としては 怒ったと思ったらいきなり優しくなる DV の典型的なパターンであることが多いです。\n筆者はコミュニケーションは双方の歩み寄りが前提で成り立つものと考えている ので、そういったやり取りになってしまった場合は残念ですが、親戚だろうが親族だろうが関係なく断絶か、それが出来ない場合は自分から可能な限り距離を置くようにしています。\n受け入れるということ 「受け入れる」というのは、相手の何らかの言動をそのまま自分に取り入れることを意味します。 相手の言動を理解した上で、自分が間違っていたり知識不足であった場合に、それを認めて言動を正しく自身に取り入れるということは視野を広げる上で重要です。\nしかし、注意点として「理解してないけど受け入れる」場合は、自分で何も考えないで言動を取り入れることを意味するため危険です。\n世の中には根気よく話せば伝え方を変える等の工夫をせずとも無条件にこちらを説き伏せることが出来ると思ってたり、一回考えを相手に話しきったらその思いが全て伝わり、勝手にこちらがその思い通りに行動すると思っている人が意外と多いです。\n上記のような人物への対策としては、自分の言動の主導権を他人に握らせないが基本戦略となります。 そういった人物は会話のペースや話の進め方が自分勝手なことが多いので、そこで折れずに反応もせずに自分の意思は持ちつつ、適当にいなすような感じで話を流すと無駄に疲れずに済みます。\n例外として、自分が相手の言動に勝手にレッテルを貼り一方的に理解したつもりで、「受け入れない」という選択肢を取ることはオススメしません。自分の視野を狭める行為だからです。\nつまり、理解するという行為はどんな状況であれ双方が取り組むべきであるが、受け入れる行為はお互いに選択の自由があるという前提を忘れずにいるべきと考えます。特に、こちらが決める権利があるはずの選択肢を、逆も然りですが、どんな人物が相手だろうがコントロールする権利は一切無いのでそのような話が出たときは注意する必要があります。\nいずれにせよ、受け入れるというのは行為は、理解という前提があった上での、主体的な行動の結果であるべきと考えています。\n自分を押し殺して受け入れるということ 前項でも簡単に説明した、「理解してないけど受け入れた」状態のことを指しますが、その場合のメリット・デメリットは下記になると考えています。\n メリット  相手の言動が 自分にとって 100% 正しかった場合 、自分だけでは対処できなかった問題を解決できる可能性がある 自分が納得することはないかもしれないが、 少なくとも相手を納得させることは出来る  あくまでも、少しの間は相手を静かにさせることが出来るかもしれない程度   自分で考える労力を割くことが出来る。物事を真剣に検討するのにはそれなりの労力がかかる  ただし、自分で物事を検討して行動するための方法が身につかない。他者に依存する     デメリット  無意識下に相手の意のままに行動させられる可能性がある  経緯はどうであれ、自身の言動の責任は自身に帰属するため、責任を取る必要がある 自分の言う通りに相手が動いてくれることが嬉しいと感じる人種は存在していて、その人達に面倒を見るといったような名目で監視されるようになる   自分が検討して取り入れたわけではないため、その結果の是非に関わらず学びが薄い  結果を反省して、自分の知見として次に繋げることがしづらい   他人の意見に忖度して物事の決断を行うようになると、主体的に行動を起こすエネルギーが無くなっていく  与えられた選択肢の中から最後に後悔しない決断が出来るのは自分しかいません 選択肢の意味を正しく理解しないまま行動すると、失敗したときのダメージが大きくなります      上記を見ていただければ分かる通り、メリットは非常に薄い上に基本的にデメリットに直結するような内容が多いです。 つまり、メリットに感じる点は刹那的な効用であって、本質的には問題の要因でもあるということです。\nまた、上記の中で最も重要な観点は 他人の意見に忖度して物事の決断を行うようになると、主体的に行動を起こすエネルギーが無くなっていく という点です。筆者はこの点を生きていく上で非常に重要な点の 1 つと考えています。\nその理由として、主体的に行動を起こすエネルギーが無くなると、1 日過ごす際の時間を有効活用できなくなったり、重要な決断をいつまでも先延ばしにしてしまうようになり、人生を楽しむ時間が削られていくと考えているからです。\n要は、面倒だからと言って理解せずに受け入れたり、他人の意見に忖度して決断を行うという行為を繰り返してしまうと、時間を浪費したり、1 日に楽しい時間を過ごせる割合が減っていき幸せの総量が減る ということです。\n理解したを受け入れたと捉える人たちへの対策 まず距離を置くという対策をオススメします。反論や反発はオススメしません。時間を浪費するだけでなく、無駄にダメージを負わされる可能性が高いです。 また、強い言葉で一方的にコミュニケーションを取られた後「反省している」「言い過ぎた」等の言葉で優しくされたとしても決して距離は近づけないでください。もしそれで距離を近づけてしまうと相互依存のような形になっていき粘着されやすくなります。\n内向的な人はただでさえダシにされやすいのに、粘着される人物が増えていくと自分の人生の主導権が握れなくなっていきます。 自力で解決するにしても直接対決は言葉の通じない相手には無意味なので、とにかく距離を置いて接する機会自体を少なくすることが肝要です。\n根本的な対策で言うと、少しでもコミュニケーションに違和感を感じたら早い段階で注意しておくことをオススメします。早い段階で内向型の人が注意すべき人の具体的な特徴を述べると下記があります。\n 発言で分からない点について、詳細な説明を求めた時に攻撃的な反応を取られる こちらの都合関係なく電話等の同期的コミュニケーションを求めてくる  一旦メッセージしてくれたら後で見るよと伝えた際に雑な反応を取られた場合はアウト   こちらの意思を尊重しないコミュニケーションを求めてくる  例えば 1 人でいる時間を大切に思っていると伝えたときにないがしろにされたり、ディスられた場合はアウト    筆者個人の経験から言うと、上記に 1 つでも当てはまる人物は大体アウトでした。\nおわりに ネガティブな出来事もただ愚痴を言うのではなく、その対策を知見として記事に出力することで、自分の考えの整理に役立つだけでなく、俯瞰で認識が正しそうか再確認することができて非常に良かったです。\n特に内向的な人に向けたこういった知見は中々表に出ない印象があるので、引き続き何か知見があれば書き留めて行こうと思いました。\n本記事内容が少しでも他の内向型の人の参考や助けになれれば、これ以上の喜びはありません。\n","permalink":"https://nikaera.com/archives/understanding-an-opinion-is-not-the-same-as-accepting-it/","summary":"はじめに 本記事のカバー画像は Vladimir Fedotov による Unsplash の画像です。\n最近「ちゃんと理解してくれた？」ということを聞かれた時に「理解したけど、そういう行動を取るつもりはない」というと、「理解してないじゃん」と言われることが何回かあったので、その時の対処法及び感じた心情などをこの記事で吐露したいと思います。\n年始からこういう記事を書くのは気が引けたのですが、あまりにも本記事タイトルのような方と多く遭遇したため\u0026hellip;\n理解するということ 「理解する」というのは、相手の言動の意味が正しく分かったということを意味します。 その言動をそのまま自身に取り入れることは意味しません。例えば、犯罪者の行動理由が正しく分かったところで、それを自分の行動規範に取り入れることはしないはずです。\n双方がコミュニケーションを取り正しく分かったという状態になっている前提ですが、「理解したけど、そういう行動を取るつもりはない」という発言を受けてから「理解してないじゃん」という言動を発してしまうのは勘違いも甚だしいです。\n上記はもはや理解した・してないとかの次元ではなく、相手を自分の言動でコントロールしたいがために会話している状態と捉えられても文句は言えません。 一種のマインドコントロールに近いかもしれません。\nちなみにこの傾向は、直接話したいと言ってくる人に多いです。しかも、そういった人に限り相手の都合を考慮せずに何回も同期的なコミュニケーションを取ろうとしてきたり、スグに語気を強めて自分が不満であることを感情でアピールしようとしてきます。また、「絶対」 とか 「必ず」 とか 「必要」 という言葉を不適切に多用する傾向が多いです。\n例えば、突然語気が強くなったり、攻撃的なメッセージの後に「電話して」等送ってくる人らが該当します。更に特筆すべき事項としては 怒ったと思ったらいきなり優しくなる DV の典型的なパターンであることが多いです。\n筆者はコミュニケーションは双方の歩み寄りが前提で成り立つものと考えている ので、そういったやり取りになってしまった場合は残念ですが、親戚だろうが親族だろうが関係なく断絶か、それが出来ない場合は自分から可能な限り距離を置くようにしています。\n受け入れるということ 「受け入れる」というのは、相手の何らかの言動をそのまま自分に取り入れることを意味します。 相手の言動を理解した上で、自分が間違っていたり知識不足であった場合に、それを認めて言動を正しく自身に取り入れるということは視野を広げる上で重要です。\nしかし、注意点として「理解してないけど受け入れる」場合は、自分で何も考えないで言動を取り入れることを意味するため危険です。\n世の中には根気よく話せば伝え方を変える等の工夫をせずとも無条件にこちらを説き伏せることが出来ると思ってたり、一回考えを相手に話しきったらその思いが全て伝わり、勝手にこちらがその思い通りに行動すると思っている人が意外と多いです。\n上記のような人物への対策としては、自分の言動の主導権を他人に握らせないが基本戦略となります。 そういった人物は会話のペースや話の進め方が自分勝手なことが多いので、そこで折れずに反応もせずに自分の意思は持ちつつ、適当にいなすような感じで話を流すと無駄に疲れずに済みます。\n例外として、自分が相手の言動に勝手にレッテルを貼り一方的に理解したつもりで、「受け入れない」という選択肢を取ることはオススメしません。自分の視野を狭める行為だからです。\nつまり、理解するという行為はどんな状況であれ双方が取り組むべきであるが、受け入れる行為はお互いに選択の自由があるという前提を忘れずにいるべきと考えます。特に、こちらが決める権利があるはずの選択肢を、逆も然りですが、どんな人物が相手だろうがコントロールする権利は一切無いのでそのような話が出たときは注意する必要があります。\nいずれにせよ、受け入れるというのは行為は、理解という前提があった上での、主体的な行動の結果であるべきと考えています。\n自分を押し殺して受け入れるということ 前項でも簡単に説明した、「理解してないけど受け入れた」状態のことを指しますが、その場合のメリット・デメリットは下記になると考えています。\n メリット  相手の言動が 自分にとって 100% 正しかった場合 、自分だけでは対処できなかった問題を解決できる可能性がある 自分が納得することはないかもしれないが、 少なくとも相手を納得させることは出来る  あくまでも、少しの間は相手を静かにさせることが出来るかもしれない程度   自分で考える労力を割くことが出来る。物事を真剣に検討するのにはそれなりの労力がかかる  ただし、自分で物事を検討して行動するための方法が身につかない。他者に依存する     デメリット  無意識下に相手の意のままに行動させられる可能性がある  経緯はどうであれ、自身の言動の責任は自身に帰属するため、責任を取る必要がある 自分の言う通りに相手が動いてくれることが嬉しいと感じる人種は存在していて、その人達に面倒を見るといったような名目で監視されるようになる   自分が検討して取り入れたわけではないため、その結果の是非に関わらず学びが薄い  結果を反省して、自分の知見として次に繋げることがしづらい   他人の意見に忖度して物事の決断を行うようになると、主体的に行動を起こすエネルギーが無くなっていく  与えられた選択肢の中から最後に後悔しない決断が出来るのは自分しかいません 選択肢の意味を正しく理解しないまま行動すると、失敗したときのダメージが大きくなります      上記を見ていただければ分かる通り、メリットは非常に薄い上に基本的にデメリットに直結するような内容が多いです。 つまり、メリットに感じる点は刹那的な効用であって、本質的には問題の要因でもあるということです。","title":"📔 理解したを受け入れたと捉える人たちに辟易する"},{"content":"はじめに 本記事のカバー画像は Michal Jarmoluk による Pixabay からの画像です。\n今年は結果的にプライベートと仕事の両面で充実した年にできました。来年の自分が今を振り返れるように、今年始めからの記憶を引っ張り出しながら総括しました。\n今年問わず作ったものは Tech ページに、技術記事については RSS Feeds にまとめてあります。\n出来事 1月  Death Stranding のプラチナ獲得 弊社に面接にいらっしゃったベテラン開発者の方に何でうちに応募してくださったのか聞いたら、僕の Twitter や Qiita アカウントを見てくださり技術力がありそうと判断してくれたからと聞いて爆嬉しかった ambr オフ会参加 (オフィシャルなオフ会に初参加) Quest 用アプリケーションの初リリース体験実績解除  申請時の知見の一部については Qiita 記事 として投下   Android で AR アプリケーションの開発及び、マルチプレイを可能にするバックエンド開発を担当した  2月  Docker で各種モバイル VR 向けの Unity ビルドが出来るようにした フルリモートでアジャイルな開発チームにジョインする (WebView/ReactNative/iOS/Android)  主は ReactNative の iOS/Android のネイティブプラグイン開発 コア機能の実装にのみ注力しパフォーマンスチューニング等々を行っていたためポジション的にはひたすら地味だった    3月  note デビューした  初投稿は 精神衛生を保つため Chrome で Twitter を閲覧している時にフォロワー数を非表示にする っていうやつ   お題が「Home」の web1week に参加した  参加した時に投稿した記事はこちら    4月  会社の Medium ブログ開設 したのと、いくつか記事を寄稿した  Azure Kinect DK の開発環境構築から KinectFusion のサンプルを動かすまで 最短で Magic Leap 1 の開発環境を構築する   色々工夫して iPhone TrueDepth を WebRTC でブラウザに転送して、Three.js で表示する仕組みを実現した  Twitter でシェア してみたら、予想だにしないことに一方的に尊敬していたエンジニアの方々からいいねを貰えてモチベが爆上がりした   CloudFormation と和解。IaC の利便性を完全に理解し始める 今更 SEKIRO にハマりまくる \u0026amp; プラチナトロフィー獲得 デス・ストランディングから学んだこと が現在の自分の考え方の基礎として根付き始める  全ての考え方を 0 or 1 ではなく、グラデーションにハメ込むことが出来るようになった    5月  再びお題が「Like」の web1week に参加した  参加した時に投稿した記事はこちら (あとから確認したら投稿先を間違えていた\u0026hellip;)   Medium に目次が無いことに不満をいただき Chrome プラグインを作成する  プラグインの紹介記事 まで書いてたけど全くインストール数伸びなかった、、けど今みたら 12人ほど使ってくれている人いるぽくて嬉しい   お題が「密」の unity1week に 参戦した  6月  S3 + EventBridge + CloudWatch + CloudFront + MediaLive + MediaPackage + AppSync + Amplify + DynamoDB + Cognito + Lambda + API Gateway + SSM という AWS ガッツリなインフラ構築から、バックエンド開発及び iOS アプリ開発までをおもむろに始めた  まず DynamoDB の仕様にハマる (自分のリサーチ \u0026amp; 勉強不足によるせい\u0026hellip;) つぎに MediaPackage + CloudFront の構築 に苦戦する そしてデバッグが辛くなり Serverless Framework でエラーを検知して Webhook で Slack に通知を飛ばす方法 を実践し始めたりしていた   シェンムー3 のプレイを開始。ワクワクするし美しすぎる町並みに興奮し、しばらくの間深夜までプレイする日々が続く 映画の HELLO WORLD を見て、劇中 3回号泣する  元々は Unity で作られたシーンがあるという記事 を見て興味を持ち見ようと思い立った感じだった    7月  ひたすらトラブルバスターしてた (一番忙しかった気がする)  8月  Ghost of Tsushima にハマりまくる \u0026amp; プラチナトロフィー獲得 DDD 開発の際、DI コンテナ入れたいよねっていう話から TypeScript 環境で利用可能なライブラリを調査して InversifyJS と tsyringe を見つける  最初 InversifyJS を発見してそれで開発をしていたものの、microsoft 製の tsyringe を発見し、「メンテナが大手だしコンストラクタインジェクションだけしか使わないし、こっちのが良くね？」という話になり InversifyJS から tsyringe へのリライト作業を行う\u0026hellip;w   AWS SDK for Go で関数の引数と返り値を Type で定義するっていう考え方は非常に参考になった  引数が *Input という定義で、返り値が *Output という定義で分かりやすく読みやすい   自作 iOS ライブラリの CocoaPods 対応について C++ 周りの linker error に対する解決のためのアドバイスを急ぎ求められたので、共有された情報から自分がハマった経験に照らし合わせてソレっぽい対策案を共有したらガチッとハマって解決でき、経験が生きた感がめっちゃあって、めちゃくちゃ嬉しかった  9月  フライパンでコーヒーの焙煎を始める  チャフの飛散に苦しめられるが、風呂場で作業することで諸々ストレスフリーになる このときはまだ、後ほど焙煎機を購入することになるとは夢にも思わなかったのである\u0026hellip;   Azure を用いた開発に本格的に携わり始める  その際得た知見は Zenn で 本としてまとめた。本来は記事として書くつもりだったが、分量が増えすぎたため記事内容を分割して、本としてまとめた PlayFab CloudFunctions のための関数実装のために Azure でシステム構築していたため、PlayFab にもそれなりに詳しくなる   とある案件から別案件に移る際に、 「えー、〇〇 に行っちゃうんですか。nikaera さんは今後も 〇〇 を一緒にやっていって欲しいのに」 って結構強めに言われたことが未だにめちゃくちゃ嬉しい シンガポール現地のフリーランスの方と仕事を共同で進めることになる。技術に関する事柄やプライベートに関する事柄のやり取り等々、全て英語でコミュニケーションを行わざるを得なくなり、そのおかげで英語でコミュニケーションを取ることに一切抵抗が無くなる  10月  約 10 年ぶりに私用携帯を HTC EVO から iPhone 12 mini に機種変する  開発用途でなく普段生活で使うことのみを考慮するということであれば iPhone 12 mini は最強にオススメできるスマホです 会社支給のスマホで 7年近くを賄っていたため、特に不便がなかったため\u0026hellip;   AWS Amplify への PR がマージされる  もとは serverless-amplify-simulator の Issue で議論していたのだが、改修すべき内容は amplify-cli にあったのでそちらで PR を提出した 細かくづまづいた点を進捗共有兼ねて Issue で一人投稿しまくっておくと、他の開発者の役にも立てるし自然とその問題に詳しくなっていくし、OSS 活動への取っ掛かりとしては最高なんじゃないかと勝手に思い始める   NPM に初自作ライブラリを公開する  serverless-amplify-auth という Amplify 開発を行う際は必ず行うであろう IAM Policy の制限を Serverless で行うことが出来るようにするやつ    11月  Hugo で自分のブログ(nikaera.com)を GitHub Pages 上に構築する カジュアル面談した人に Qiita のネタ記事 見ましたって言われて嬉しいよりも恥ずかしいが上回った Etsy でアクセサリ販売している方に日本のフリマ事情を詳細にお伝えしたらおまけのプレゼントを送付してくださった  ちなみに購入物は Death Stranding の ドリームキャッチャー   Moonlander が自宅に届きテンション上がって紹介記事を書く  12月  今年学んだ重要なことを 記事 として残しといた (これもある意味総括な気がする) GitHub Profile を充実させる  こんな感じ -\u0026gt; https://github.com/nikaera 更に GitHub Profile を充実させるために Zenn のバッジを作成するサービス を作った   Lapras の技術力スコアが 3.36 になってた  基準とか良く分からないけど純粋に上位 13% に入ったと言われてるのは嬉しかった   いくつか空いてたアドベントカレンダーに参戦した  MediaPackage 用の CloudFront ディストリビューションを AWS SDK で作成する Serverless のプラグインを TypeScript で作成する方法   AWS Lambda を用いた他社製品との連携システムが好評で、去年から今年末まで特に目立った不具合等も起きずに運用できたため、次期開発に繋がりそうとの連絡があり開発者として爆喜ぶ  おわりに 今年は後半からすごい勢いでギアが入ってきた感があり、諸々活動するための足がかりを作れた気がします。身も心も進化したなと思えて成長できたなという充足感は割と高めな 1年だったので、この勢いのまま 2021 年もマイペースに色んなことにチャレンジしていければなーと思っております。\nこの記事を書いている人物のプロフィールは Profile からご確認いただけます。何かございましたら Contact からお気軽にご連絡くださいませ。\nそれではみなさま良いお年を！！😆\n","permalink":"https://nikaera.com/archives/summarize-2020/","summary":"はじめに 本記事のカバー画像は Michal Jarmoluk による Pixabay からの画像です。\n今年は結果的にプライベートと仕事の両面で充実した年にできました。来年の自分が今を振り返れるように、今年始めからの記憶を引っ張り出しながら総括しました。\n今年問わず作ったものは Tech ページに、技術記事については RSS Feeds にまとめてあります。\n出来事 1月  Death Stranding のプラチナ獲得 弊社に面接にいらっしゃったベテラン開発者の方に何でうちに応募してくださったのか聞いたら、僕の Twitter や Qiita アカウントを見てくださり技術力がありそうと判断してくれたからと聞いて爆嬉しかった ambr オフ会参加 (オフィシャルなオフ会に初参加) Quest 用アプリケーションの初リリース体験実績解除  申請時の知見の一部については Qiita 記事 として投下   Android で AR アプリケーションの開発及び、マルチプレイを可能にするバックエンド開発を担当した  2月  Docker で各種モバイル VR 向けの Unity ビルドが出来るようにした フルリモートでアジャイルな開発チームにジョインする (WebView/ReactNative/iOS/Android)  主は ReactNative の iOS/Android のネイティブプラグイン開発 コア機能の実装にのみ注力しパフォーマンスチューニング等々を行っていたためポジション的にはひたすら地味だった    3月  note デビューした  初投稿は 精神衛生を保つため Chrome で Twitter を閲覧している時にフォロワー数を非表示にする っていうやつ   お題が「Home」の web1week に参加した  参加した時に投稿した記事はこちら    4月  会社の Medium ブログ開設 したのと、いくつか記事を寄稿した  Azure Kinect DK の開発環境構築から KinectFusion のサンプルを動かすまで 最短で Magic Leap 1 の開発環境を構築する   色々工夫して iPhone TrueDepth を WebRTC でブラウザに転送して、Three.","title":"📔 2020年の振り返り"},{"content":"Zenn.badge を作成する時に React に highlight.js を組み込もうとしたのですが、若干躓いてしまったので対処法についてメモっておきます。\nReact は既にプロジェクトにインストール済みと仮定します。\n# 一応 React をインストールするためのコマンドは ↓ npm i --save react react-dom まずは highlight.js を NPM or Yarn でインストールします。\n# NPM で highlight.js をインストールする npm i --save highlight.js yarn add highlight.js その後、React ソースコードに highlight.js を組み込みます。 ソースコードの全体像は下記のとおりです。\nimport Head from \u0026#39;next/head\u0026#39; import styles from \u0026#39;../styles/Home.module.css\u0026#39; import React, { useState, useEffect } from \u0026#39;react\u0026#39;; /** highlight.js を import する */ import hljs from \u0026#39;highlight.js/lib/core\u0026#39;; /** シンタックスハイライトしたい言語のみ import として登録する 今回は html をハイライトしたかったので xml を import した デザインは highlight.js/styles/~ を変更することで調整可能 https://highlightjs.org/ のトップページから各種デザインについては確認可能 (コード右下にある style の右側リンククリックで各種デザインのプレビューが可能) */ import xml from \u0026#39;highlight.js/lib/languages/xml\u0026#39;; import \u0026#39;highlight.js/styles/github.css\u0026#39;; hljs.registerLanguage(\u0026#39;xml\u0026#39;, xml); let inputChecker = null; export default function Home() { const [user, setUser] = useState(\u0026#39;nikaera\u0026#39;); const [previewUser, setPreviewUser] = useState(\u0026#39;nikaera\u0026#39;); const [badgeCode, setBadgeCode] = useState(\u0026#39;nikaera\u0026#39;); const [style, setStyle] = useState(\u0026#39;plastic\u0026#39;) /** useEffect のタイミングで hightlight.js の初期化を行う。 called プロパティを false にすることで highlight.js で、 コードが変更された場合でも常にシンタックスハイライトすることが可能 */ useEffect(() =\u0026gt; { hljs.initHighlighting(); hljs.initHighlighting.called = false; }); useEffect(() =\u0026gt; { /** シンタックスハイライトしたいコード input フォームへの入力内容に応じて動的に変わる */ setBadgeCode(` \u0026lt;!-- Like のバッジ --\u0026gt; \u0026lt;a href=\u0026#34;https://zenn.dev/${user}\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://zenn-badge.herokuapp.com/s/${user}/likes?style=${style}\u0026#34; alt=\u0026#34;${user}likes\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Articles のバッジ --\u0026gt; \u0026lt;a href=\u0026#34;https://zenn.dev/${user}/articles\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://zenn-badge.herokuapp.com/s/${user}/articles?style=${style}\u0026#34; alt=\u0026#34;${user}articles\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Followers のバッジ --\u0026gt; \u0026lt;a href=\u0026#34;https://zenn.dev/${user}/followers\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://zenn-badge.herokuapp.com/s/${user}/followers?style=${style}\u0026#34; alt=\u0026#34;${user}followers\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Books のバッジ --\u0026gt; \u0026lt;a href=\u0026#34;https://zenn.dev/${user}/books\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://zenn-badge.herokuapp.com/s/${user}/books?style=${style}\u0026#34; alt=\u0026#34;${user}books\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- Scraps のバッジ --\u0026gt; \u0026lt;a href=\u0026#34;https://zenn.dev/${user}/scraps\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://zenn-badge.herokuapp.com/s/${user}/scraps?style=${style}\u0026#34; alt=\u0026#34;${user}scraps\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt;`); }, [user, style]); const handleChange = (event) =\u0026gt; { if (inputChecker) clearTimeout(inputChecker); inputChecker = setTimeout(() =\u0026gt; { clearTimeout(inputChecker); inputChecker = null; setPreviewUser(event.target.value); }, 1 * 1000); // 1 seconds  setUser(event.target.value); }; const handleSelect = (event) =\u0026gt; { setStyle(event.target.value); } return ( \u0026lt;div className={styles.container}\u0026gt; \u0026lt;Head\u0026gt; \u0026lt;title\u0026gt;Zenn.badge\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; href=\u0026#34;zenn.svg\u0026#34; type=\u0026#34;image/svg+xml\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;description\u0026#34; content=\u0026#34;GitHub Profile に載せるための Zenn の各種スコアバッジを作成するためのウェブサービス\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;keywords\u0026#34; content=\u0026#34;Zenn, GitHub, GitHub Profile, Shields.io\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;author\u0026#34; content=\u0026#34;nikaera\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:url\u0026#34; content=\u0026#34;https://zenn-badge.vercel.app/\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:type\u0026#34; content=\u0026#34;website\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:title\u0026#34; content=\u0026#34;Zenn.badge\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:description\u0026#34; content=\u0026#34;GitHub Profile に載せるための Zenn の各種スコアバッジを作成するためのウェブサービス\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:site_name\u0026#34; content=\u0026#34;Zenn.badge\u0026#34; /\u0026gt; \u0026lt;meta property=\u0026#34;og:image\u0026#34; content=\u0026#34;https://zenn-badge.vercel.app/ogp_image.png\u0026#34; /\u0026gt; \u0026lt;/Head\u0026gt; \u0026lt;main className={styles.main}\u0026gt; \u0026lt;h1 className={styles.title}\u0026gt; Welcome to \u0026lt;a href=\u0026#34;https://zenn-badge.nikaera.vercel.app/\u0026#34;\u0026gt;Zenn.badge!\u0026lt;/a\u0026gt; \u0026lt;/h1\u0026gt; \u0026lt;p className={styles.description}\u0026gt; \u0026lt;a href=\u0026#34;https://zenn.dev/\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;Zenn\u0026lt;/a\u0026gt; のスコアを GitHub 風のバッジに変換するサービスです。 \u0026lt;/p\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3\u0026gt;注意事項 \u0026amp;rarr;\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;下記の事項にご留意ください。\u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;12時間ほどデータがキャッシュされます\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;100%の動作を保証するものではありません\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;APIの利用数制限などにより、表示されない事があります\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;GitHub Profile にご利用ください\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;h3\u0026gt;アカウント名を入力してください \u0026amp;rarr;\u0026lt;/h3\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; value={user} onChange={handleChange} /\u0026gt; \u0026lt;select name=\u0026#34;style\u0026#34; value={style} onChange={handleSelect}\u0026gt; \u0026lt;option value=\u0026#34;plastic\u0026#34;\u0026gt;plastic\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;flat\u0026#34;\u0026gt;flat\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;flat-square\u0026#34;\u0026gt;flat-square\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;for-the-badge\u0026#34;\u0026gt;for-the-badge\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;social\u0026#34;\u0026gt;social\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; { /* pre -\u0026gt; code タグ内に highlight.js で シンタックスハイライトしたい内容を出力する */ } \u0026lt;pre style={{ width: \u0026#39;80vw\u0026#39; }}\u0026gt; \u0026lt;code className=\u0026#34;xml\u0026#34;\u0026gt; {badgeCode} \u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; \u0026lt;h3\u0026gt;プレビュー \u0026amp;rarr;\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;Likes 👍\u0026lt;/h4\u0026gt; \u0026lt;a href={`https://zenn.dev/${previewUser}`}\u0026gt; \u0026lt;img src={`https://zenn-badge.herokuapp.com/s/${previewUser}/likes?style=${style}`} alt={`${previewUser}likes`} /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;h4\u0026gt;Articles 📝\u0026lt;/h4\u0026gt; \u0026lt;a href={`https://zenn.dev/${previewUser}/articles`}\u0026gt; \u0026lt;img src={`https://zenn-badge.herokuapp.com/s/${previewUser}/articles?style=${style}`} alt={`${previewUser}articles`} /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;h4\u0026gt;Followers 👱\u0026lt;/h4\u0026gt; \u0026lt;a href={`https://zenn.dev/${previewUser}/followers`}\u0026gt; \u0026lt;img src={`https://zenn-badge.herokuapp.com/s/${previewUser}/followers?style=${style}`} alt={`${previewUser}followers`} /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;h4\u0026gt;Books 📚\u0026lt;/h4\u0026gt; \u0026lt;a href={`https://zenn.dev/${previewUser}/books`}\u0026gt; \u0026lt;img src={`https://zenn-badge.herokuapp.com/s/${previewUser}/books?style=${style}`} alt={`${previewUser}books`} /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;h4\u0026gt;Scraps 🗑️\u0026lt;/h4\u0026gt; \u0026lt;a href={`https://zenn.dev/${previewUser}/scraps`}\u0026gt; \u0026lt;img src={`https://zenn-badge.herokuapp.com/s/${previewUser}/scraps?style=${style}`} alt={`${previewUser}scraps`} /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;h3\u0026gt;問い合わせ \u0026amp;rarr;\u0026lt;/h3\u0026gt; \u0026lt;div\u0026gt; 下記 GitHub の Issue で \u0026lt;a className={styles.link} href=\u0026#34;https://github.com/nikaera\u0026#34;\u0026gt;nikaera\u0026lt;/a\u0026gt; にメンションを付けてご連絡ください。 \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a className={styles.link} href=\u0026#34;https://github.com/nikaera/zenn-badge\u0026#34;\u0026gt;Zenn.badge\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;footer className={styles.footer}\u0026gt; 制作者:\u0026amp;nbsp;\u0026lt;a className={styles.link} href=\u0026#34;https://zenn.dev/nikaera\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;nikaera\u0026lt;/a\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/div\u0026gt; ) } ","permalink":"https://nikaera.com/archives/react-highlightjs/","summary":"Zenn.badge を作成する時に React に highlight.js を組み込もうとしたのですが、若干躓いてしまったので対処法についてメモっておきます。\nReact は既にプロジェクトにインストール済みと仮定します。\n# 一応 React をインストールするためのコマンドは ↓ npm i --save react react-dom まずは highlight.js を NPM or Yarn でインストールします。\n# NPM で highlight.js をインストールする npm i --save highlight.js yarn add highlight.js その後、React ソースコードに highlight.js を組み込みます。 ソースコードの全体像は下記のとおりです。\nimport Head from \u0026#39;next/head\u0026#39; import styles from \u0026#39;../styles/Home.module.css\u0026#39; import React, { useState, useEffect } from \u0026#39;react\u0026#39;; /** highlight.js を import する */ import hljs from \u0026#39;highlight.js/lib/core\u0026#39;; /** シンタックスハイライトしたい言語のみ import として登録する 今回は html をハイライトしたかったので xml を import した デザインは highlight.","title":"📝 React で highlight.js を適用する方法"},{"content":"この記事は Static Site Generator Advent Calendar 2020 22 日目の記事です。\nはじめに Hugo のウェブサイトに組み込む RSS リーダーを TypeScript で開発してみたいと思い調査したところ、Hugo の最新版には ESBuild が組み込まれていて、非常に手厚く JavaScript の開発環境がサポートされていることが分かりました。 本記事では紹介していませんが Babel も利用できるようです。\nまた、NPM パッケージも利用できるため、普段のウェブ開発と同様の流れで開発ができ、各種ライブラリを用いた開発も非常に楽でした。 今回は Hugo で JavaScript 開発する方法を RSS リーダーの開発を例に上げ、そこで得た知見についても交える形で記事として残しておくことにしました。\nちなみに本記事内容は Hugo で JavaScript 開発する方法に焦点を絞ったものなのですが、ウェブサイトに RSS リーダーを組み込むことに焦点を絞って見たい方は RSS リーダーを Hugo の Data Templates で実装する から見ていただくことをオススメします。\nHugo で JavaScript (React + TypeScript) の開発環境を整える まず、TypeScript のビルドは ESBuild に任せることができるため何も行う必要はありません。 そのため React 開発用パッケージのインストールのみ行えば大丈夫です。\nHugo プロジェクトのルートディレクトリで下記コマンドを実行し、package.json を作成してから、React の開発に必要なパッケージをインストールします。\nnpm init -y npm install --save react react-dom 無事パッケージのインストールが完了したら、早速 TSX ファイルを assets/js/App.tsx に作成してしまいます。\n// assets/js/App.tsx  import * as React from \u0026#34;react\u0026#34;; import * as ReactDOM from \u0026#34;react-dom\u0026#34;; function App() { return \u0026lt;\u0026gt;Hello React!\u0026lt;/\u0026gt;; } ReactDOM.render(\u0026lt;App /\u0026gt;, document.getElementById(\u0026#34;react\u0026#34;)); 上記のコードを見てもらえば分かる通り、レンダリング先に id が react の DOM ノードを指定しています。そのため Hugo 側で該当する DOM ノードを用意する必要があります。その際の HTML テンプレートは下記になります。\n\u0026lt;!-- ... --\u0026gt; \u0026lt;!-- 利用するリソースを指定する --\u0026gt; {{ with resources.Get \u0026#34;js/App.tsx\u0026#34; }} \u0026lt;!-- id が react の div 要素を用意する --\u0026gt; \u0026lt;div id=\u0026#34;react\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;!-- TSX を ESBuild でビルドする際の Hugo のオプションを指定する --\u0026gt; {{ $options := dict \u0026#34;targetPath\u0026#34; \u0026#34;js/app.js\u0026#34; \u0026#34;minify\u0026#34; true \u0026#34;defines\u0026#34; (dict \u0026#34;process.env.NODE_ENV\u0026#34; \u0026#34;\\\u0026#34;development\\\u0026#34;\u0026#34;) }} \u0026lt;!-- TSX のビルドを Hugo のオプションで指定した内容で実行する --\u0026gt; {{ $js := resources.Get . | js.Build $options }} \u0026lt;!-- 一応 SRI を有効化した状態でビルドした JS を読み込む --\u0026gt; {{ $secureJS := $js | resources.Fingerprint \u0026#34;sha512\u0026#34; }} \u0026lt;script src=\u0026#34;{{ $secureJS.Permalink }}\u0026#34; integrity=\u0026#34;{{ $secureJS.Data.Integrity }}\u0026#34; \u0026gt;\u0026lt;/script\u0026gt; {{ end }} \u0026lt;!-- ... --\u0026gt; ちなみに $options で指定している ESBuild でビルド時に指定可能なオプションは Hugo の公式ページ に記載されています。\n上記 HTML の記述を RSS リーダーを埋め込みたいページに追加します。 この状態で該当ページにアクセスすると下記のような表示が確認できるはずです。\nApp.tsx で定義した内容が画面に表示される\nこれで React + TypeScript の開発環境が整いました。\nRSS リーダーを実装する あとは一般的な Web フロントエンド開発の流れで RSS リーダーの開発を進めていくだけです。\nウェブサイトで読み込みたい RSS フィードを準備する RSS フィードを利用する際は必ず提供しているサービスの利用規約をご確認ください。 Qiita 及び Zenn については個人利用かつ自分の情報のみを扱う範囲内であれば利用が許可されているように見受けられました。1\n下準備としてウェブサイトで読み込みたい RSS フィードを事前にダウンロードするためのバッチを作成します。バッチは NPM を利用して作成していきます。NPM を導入したので Hugo で利用する簡易なバッチは JavaScript でサクッと作成していきます。\nまずはスクリプト作成の際に必要となるパッケージを事前にいくつかインストールします。\n# html をテキスト変換にするパッケージと RSS フィードのパーサーをインストールする npm i -D --save html-to-text rss-parser 実際のコードは下記になります。ファイル名末尾が .mjs なのは Top-Level Await を使用したいからです。\n// scripts/update-rss.mjs  import { writeFileSync } from \u0026#34;fs\u0026#34;; import pkg from \u0026#34;html-to-text\u0026#34;; const { htmlToText } = pkg; import Parser from \u0026#34;rss-parser\u0026#34;; const parser = new Parser(); // 自ブログで読み込みたい RSS フィードの情報を設定する const rssFeed = { Zenn: { rss_url: \u0026#34;https://zenn.dev/nikaera/feed\u0026#34;, profile_url: \u0026#34;https://zenn.dev/nikaera\u0026#34;, }, Qiita: { rss_url: \u0026#34;https://qiita.com/nikaera/feed.atom\u0026#34;, profile_url: \u0026#34;https://qiita.com/nikaera\u0026#34;, }, }; try { const jsonFeed = {}; // RSS フィード内の description を 73字で切り取り末尾に ... を付与する関数  const spliceContent = (content) =\u0026gt; `${htmlToText(content).slice(0, 73)}...`; // rssFeed 変数で定義されてる情報\u0008を繰り返し処理する  for (const [site, info] of Object.entries(rssFeed)) { // RSS フィードの URL から必要な情報を取得する  const feed = await parser.parseURL(info.rss_url); // RSS フィードに登録されている項目で必要な情報のみを取得する  const items = feed.items.map((i) =\u0026gt; { return { title: i.title, content: spliceContent(i.content), url: i.link, date: i.pubDate, }; }); // 取得内容は jsonFeed に格納する  const { rss_url, profile_url } = info; jsonFeed[site] = { rss_url, profile_url, items }; } // 最後に jsonFeed に格納された内容を JSON 文字列として static/rss.json に出力する  writeFileSync(\u0026#34;./static/rss.json\u0026#34;, JSON.stringify(jsonFeed)); } catch (err) { console.error(err); } 次に package.json の scripts に登録してコマンドとして実行可能にします。\n{ \u0026#34;scripts\u0026#34;: { \u0026#34;update-rss\u0026#34;: \u0026#34;node ./scripts/update-rss.mjs\u0026#34; } } これで npm run update-rss を実行すれば自ブログで表示する際に用いる JSON ファイルとして RSS フィードの内容を static/rss.json に出力できます。また、JSON ファイルは static フォルダに出力しているため http://localhost:1313/rss.json でアクセスできます。\nnpm run update-rss を実行して出力した rss.json\nhttp://localhost:1313/rss.json にアクセスして出力した rss.json が参照可能なことを確認する\nRSS リーダーを React + TypeScript で実装する 準備が整ったので、早速 RSS リーダーを作成していきます。\n下記は Hugo のテーマの 1 つである hugo-PaperMod の archives テンプレートを利用してページに埋め込むことを想定した RSS リーダーのコードです。\n// assets/js/Rss.tsx  import React, { useMemo, useState } from \u0026#34;react\u0026#34;; import * as superagent from \u0026#34;superagent\u0026#34;; const Rss = (props) =\u0026gt; { const [feed, setFeed] = useState({}); const { name } = props; useMemo(() =\u0026gt; { (async () =\u0026gt; { try { const res = await superagent.get(\u0026#34;/rss.json\u0026#34;); setFeed(res.body[name]); } catch (err) { console.error(err); } })(); }, [name]); if (!(\u0026#34;items\u0026#34; in feed)) return null; return ( \u0026lt;div className=\u0026#34;archive-month\u0026#34;\u0026gt; \u0026lt;h3 className=\u0026#34;archive-month-header\u0026#34;\u0026gt; \u0026lt;a href={feed.profile_url} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt; {name} \u0026lt;/a\u0026gt;{\u0026#34; \u0026#34;} -{\u0026#34; \u0026#34;} \u0026lt;a href={feed.rss_url} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt; RSS \u0026lt;/a\u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;div className=\u0026#34;archive-posts\u0026#34;\u0026gt; {feed.items.map((item) =\u0026gt; { return ( \u0026lt;div className=\u0026#34;archive-entry\u0026#34; key={item.url}\u0026gt; \u0026lt;h3 className=\u0026#34;archive-entry-title\u0026#34;\u0026gt;{item.title}\u0026lt;/h3\u0026gt; \u0026lt;div className=\u0026#34;archive-meta\u0026#34;\u0026gt; {item.date} - {item.content} \u0026lt;/div\u0026gt; \u0026lt;a className=\u0026#34;entry-link\u0026#34; href={item.url} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt; \u0026amp;nbsp; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; ); })} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ); }; export default Rss; 次に assets/js/App.tsx で assets/js/Rss.tsx を読み込み画面に表示できるよう改修します。\n// assets/js/App.tsx  import Rss from \u0026#34;./Rss\u0026#34;; import * as React from \u0026#34;react\u0026#34;; import * as ReactDOM from \u0026#34;react-dom\u0026#34;; function App() { return ( \u0026lt;\u0026gt; \u0026lt;div class=\u0026#34;archive-year\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;archive-year-header\u0026#34;\u0026gt;Tech 🦾\u0026lt;/h2\u0026gt; \u0026lt;Rss name=\u0026#34;Zenn\u0026#34; /\u0026gt; \u0026lt;Rss name=\u0026#34;Qiita\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/\u0026gt; ); } ReactDOM.render(\u0026lt;App /\u0026gt;, document.getElementById(\u0026#34;react\u0026#34;)); これで RSS リーダーを埋め込んだページを閲覧すると下記のような画面が表示されるはずです。\nhugo-PaperMod で archives テンプレートを用いて RSS リーダーを表示したときの画面\nもし他の RSS フィードを追加したい場合は scripts/update-rss.mjs の rssFeed 変数に情報を追加して、App.tsx に \u0026lt;Rss name=\u0026quot;\u0026lt;rssFeed 変数で定義した RSS Feed 名\u0026gt;\u0026quot; /\u0026gt; を定義することで対応できます。\nRSS フィードの内容を自動で更新する npm run update-rss を手元で実行して static/rss.json を更新して公開すれば、最新の RSS フィードの内容をページに反映できる状態ですが、都度手動で更新するのは面倒な作業です。\nそこで今回は GitHub Actions の schedule を用いて static/rss.json の更新を自動化します。\nGitHub Actions のワークフローファイルを作成する 実際のワークフローファイルは下記になります。schedule の項目で設定している内容がワークフローの実行スケジュールになります。今回は半日毎に更新が走るようにしました。\n# .github/workflows/update-rss.yml name: update rss json file on: push: branches: - main # Set a branch name to trigger deployment schedule: - cron: \u0026#34;0 */12 * * *\u0026#34; # 今回は半日に 1回のタイミングで更新するようにした jobs: build: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 with: ref: main submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Use Node.js 14.10.1 uses: actions/setup-node@v1 with: node-version: 14.10.1 - name: Install dependencies run: npm install - name: Update RSS Feeds run: npm run update-rss - name: Commit files run: |git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git add static/rss.json STATUS=$(git status -s) if [ -n \u0026#34;$STATUS\u0026#34; ]; then git commit -m \u0026#34;Update rss.json `date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;`\u0026#34; -a git push origin main fi 上記ワークフローファイルをプロジェクトに追加して、リモートリポジトリにプッシュした後は、ワークフローが実行されるタイミングを待ちます。\n無事にワークフローの実行が完了すると下記のようなコミットが追加されているはずです。\nGitHub Actions が JSON ファイルを更新してコミットしている\nコミットの詳細を見ると正常に JSON ファイルが更新されていることが確認できる\nコミット後 Hugo をビルド \u0026amp; デプロイするとページが更新されていることを確認できる\nこれで Zenn や Qiita 等に記事を書いた際に、都度手動で static/rss.json を更新してページに最新の内容を反映させる作業は必要なくなりました。\n(余談) RSS リーダーを Hugo の Data Templates で実装する ちなみに Hugo には Data Templates という仕組みがあり、これを用いることで実は JavaScript を利用しなくても HTML テンプレートで RSS リーダーを実現できるということを後から知りました。\nそこで最後に Data Template での RSS リーダーの実装方法について記載します。\nまずは、scripts/update-rss.mjs の内容を書き換えます。\n// scripts/update-rss.mjs  import { writeFileSync } from \u0026#34;fs\u0026#34;; import pkg from \u0026#34;html-to-text\u0026#34;; const { htmlToText } = pkg; import Parser from \u0026#34;rss-parser\u0026#34;; const parser = new Parser(); const rssFeed = { Zenn: { rss_url: \u0026#34;https://zenn.dev/nikaera/feed\u0026#34;, profile_url: \u0026#34;https://zenn.dev/nikaera\u0026#34;, }, Qiita: { rss_url: \u0026#34;https://qiita.com/nikaera/feed.atom\u0026#34;, profile_url: \u0026#34;https://qiita.com/nikaera\u0026#34;, }, }; try { const jsonFeed = {}; const spliceContent = (content) =\u0026gt; `${htmlToText(content).slice(0, 73)}...`; for (const [site, info] of Object.entries(rssFeed)) { const feed = await parser.parseURL(info.rss_url); const items = feed.items.map((i) =\u0026gt; { console.log(i); return { title: i.title, content: spliceContent(i.content), url: i.link, date: i.pubDate, }; }); const { rss_url, profile_url } = info; jsonFeed[site] = { rss_url, profile_url, items }; /* 最終的な JSON ファイルの出力先は data フォルダとなり、RSS フィード毎に出力する 例: ./data/Qiita.json, ./data/Zenn.json, etc. */ writeFileSync(`./data/${site}.json`, JSON.stringify(jsonFeed[site])); } } catch (err) { console.error(err); } 上記を実行することで data/Qiita.json や data/Zenn.json にファイルが出力されます。\nHugo の Data Template を用いると data フォルダ内に配置した json, yaml, toml 形式のファイルは Go の HTML テンプレートで読み込めるようになります。\n例えば、data/Qiita.json に配置された JSON ファイルを読み込みたい場合は Go のテンプレートで $Qiita := $.Site.Data.Qiita のような記述でできます。\n次に RSS リーダーを埋め込んでいたページを下記のように書き換えます。\n\u0026lt;!-- ... --\u0026gt; \u0026lt;!-- React 関連の記述を全て削除する --\u0026gt; \u0026lt;!-- {{ with resources.Get \u0026#34;js/App.tsx\u0026#34; }} \u0026lt;div id=\u0026#34;react\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; {{ $options := dict \u0026#34;targetPath\u0026#34; \u0026#34;js/app.js\u0026#34; \u0026#34;minify\u0026#34; true \u0026#34;defines\u0026#34; (dict \u0026#34;process.env.NODE_ENV\u0026#34; \u0026#34;\\\u0026#34;development\\\u0026#34;\u0026#34;) }} {{ $js := resources.Get . | js.Build $options }} {{ $secureJS := $js | resources.Fingerprint \u0026#34;sha512\u0026#34; }} \u0026lt;script src=\u0026#34;{{ $secureJS.Permalink }}\u0026#34; integrity=\u0026#34;{{ $secureJS.Data.Integrity }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} --\u0026gt; \u0026lt;div class=\u0026#34;archive-year\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;archive-year-header\u0026#34;\u0026gt;Tech 🦾\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;archive-month\u0026#34;\u0026gt; \u0026lt;!-- data/Zenn.json の内容を読み込む --\u0026gt; {{ $Zenn := $.Site.Data.Zenn }} \u0026lt;h3 class=\u0026#34;archive-month-header\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ $Zenn.profile_url }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt;Zenn\u0026lt;/a \u0026gt; - \u0026lt;a href=\u0026#34;{{ $Zenn.rss_url }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt;RSS\u0026lt;/a \u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;archive-posts\u0026#34;\u0026gt; \u0026lt;!-- 配列で格納されている記事情報を繰り返し処理で取得する --\u0026gt; {{- range $Zenn.items }} \u0026lt;div class=\u0026#34;archive-entry\u0026#34; key=\u0026#34;{{ .url }}\u0026#34;\u0026gt; \u0026lt;h3 class=\u0026#34;archive-entry-title\u0026#34;\u0026gt;{{ .title }}\u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;archive-meta\u0026#34;\u0026gt;{{ .date }} - {{ .content }}\u0026lt;/div\u0026gt; \u0026lt;a class=\u0026#34;entry-link\u0026#34; aria-label=\u0026#34;{{ .content }}\u0026#34; href=\u0026#34;{{ .url }}\u0026#34; target=\u0026#34; _blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; {{- end }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;archive-month\u0026#34;\u0026gt; \u0026lt;!-- data/Qiita.json の内容を読み込む --\u0026gt; {{ $Qiita := $.Site.Data.Qiita }} \u0026lt;h3 class=\u0026#34;archive-month-header\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ $Qiita.profile_url }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt;Qiita\u0026lt;/a \u0026gt; - \u0026lt;a href=\u0026#34;{{ $Qiita.rss_url }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt;RSS\u0026lt;/a \u0026gt; \u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;archive-posts\u0026#34;\u0026gt; \u0026lt;!-- 配列で格納されている記事情報を繰り返し処理で取得する --\u0026gt; {{- range $Qiita.items }} \u0026lt;div class=\u0026#34;archive-entry\u0026#34; key=\u0026#34;{{ .url }}\u0026#34;\u0026gt; \u0026lt;h3 class=\u0026#34;archive-entry-title\u0026#34;\u0026gt;{{ .title }}\u0026lt;/h3\u0026gt; \u0026lt;div class=\u0026#34;archive-meta\u0026#34;\u0026gt;{{ .date }} - {{ .content }}\u0026lt;/div\u0026gt; \u0026lt;a class=\u0026#34;entry-link\u0026#34; aria-label=\u0026#34;{{ .content }}\u0026#34; href=\u0026#34;{{ .url }}\u0026#34; target=\u0026#34; _blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; \u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; {{- end }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- ... --\u0026gt; また GitHub Actions のワークフローを用いて RSS フィードの情報を更新していた場合は、.github/workflows/update-rss.yml ファイルの更新も必要になります。\n# .github/workflows/update-rss.yml name: update rss json file on: push: branches: - main # Set a branch name to trigger deployment schedule: - cron: \u0026#34;0 */12 * * *\u0026#34; jobs: build: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 with: ref: main submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Use Node.js 14.10.1 uses: actions/setup-node@v1 with: node-version: 14.10.1 - name: Install dependencies run: npm install - name: Update RSS Feeds run: npm run update-rss # Git で追加する内容を data フォルダに変更する # git add static/rss.json -\u0026gt; git add data/ - name: Commit files run: |git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git add data/ STATUS=$(git status -s) if [ -n \u0026#34;$STATUS\u0026#34; ]; then git commit -m \u0026#34;Update data folder `date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;`\u0026#34; -a git push origin main fi これで JavaScript で作成した RSS リーダーから、Hugo の Data Templates を用いて作成した RSS リーダーへ移行できました。\nおわりに Hugo で React + TypeScript 開発を楽にできそうなことが分かり、テンションが上がってしまい、そのままのノリで実際に RSS リーダーを自ブログ向けに作成してみました。\nしかし、本記事内容で RSS リーダーを実装するのであれば、Hugo の Data Templates を利用することがベストなことに後から気づきました。ただ Hugo での JavaScript を用いた開発手法が理解でき勉強になったので結果ヨシとしました。\nHugo での JavaScript 開発環境は相当充実していることが分かったので、また何かアイデアを思いついたら気軽に作って自ブログに取り込んでいきます。今はザックリ WebGL/WebVR とかで何か面白いもの作れそうだなと考えています。\n参考リンク  esbuild - An extremely fast JavaScript bundler Data Templates | Hugo Functions Quick Reference | Hugo JavaScript Building | Hugo Introducing Hooks – React rbren/rss-parser: A lightweight RSS parser, for Node and the browser html-to-text/node-html-to-text: Advanced html to text converter    もし認識に誤りがあればコメント欄等でご教授いただけますと幸いです。 \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nikaera.com/archives/hugo-react-dev/","summary":"この記事は Static Site Generator Advent Calendar 2020 22 日目の記事です。\nはじめに Hugo のウェブサイトに組み込む RSS リーダーを TypeScript で開発してみたいと思い調査したところ、Hugo の最新版には ESBuild が組み込まれていて、非常に手厚く JavaScript の開発環境がサポートされていることが分かりました。 本記事では紹介していませんが Babel も利用できるようです。\nまた、NPM パッケージも利用できるため、普段のウェブ開発と同様の流れで開発ができ、各種ライブラリを用いた開発も非常に楽でした。 今回は Hugo で JavaScript 開発する方法を RSS リーダーの開発を例に上げ、そこで得た知見についても交える形で記事として残しておくことにしました。\nちなみに本記事内容は Hugo で JavaScript 開発する方法に焦点を絞ったものなのですが、ウェブサイトに RSS リーダーを組み込むことに焦点を絞って見たい方は RSS リーダーを Hugo の Data Templates で実装する から見ていただくことをオススメします。\nHugo で JavaScript (React + TypeScript) の開発環境を整える まず、TypeScript のビルドは ESBuild に任せることができるため何も行う必要はありません。 そのため React 開発用パッケージのインストールのみ行えば大丈夫です。\nHugo プロジェクトのルートディレクトリで下記コマンドを実行し、package.json を作成してから、React の開発に必要なパッケージをインストールします。\nnpm init -y npm install --save react react-dom 無事パッケージのインストールが完了したら、早速 TSX ファイルを assets/js/App.","title":"📔 Hugo で React + TypeScript を利用してサクッとウェブサイトに RSS リーダーを追加する"},{"content":"Hugo で設定した外部リンクを開くときは別ウィンドウで開けるようにしたかったので、Hugo のテーマファイルをオーバーライドして対応しました。外部リンクが設定されているときのみ a タグに target=\u0026quot;_blank\u0026quot; rel=\u0026quot;noopener noreferrer\u0026quot; が追加されるようにしました。\nテンプレートファイルは GO の HTML テンプレートで書かれているので、その書式にしたがって a タグの属性を書き換えることで、外部リンクの場合は target=\u0026quot;_blank\u0026quot; rel=\u0026quot;noopener noreferrer\u0026quot; を追加します。\n下記は hugo-PaperMod で、メインメニューの a タグに target=\u0026quot;_blank\u0026quot; rel=\u0026quot;noopener noreferrer\u0026quot; を追加するときのサンプルになります。\n\u0026lt;ul class=\u0026#34;menu\u0026#34; id=\u0026#34;menu\u0026#34; onscroll=\u0026#34;menu_on_scroll()\u0026#34;\u0026gt; \u0026lt;!-- `.Site.Menues.main` の要素数 (メニュー数) ループします --\u0026gt; {{- range .Site.Menus.main }} \u0026lt;!-- 要素内の .URL にアクセスすることで設定されているリンクにアクセスする。 設定されている URL のプレフィクスが https:// or http:// であれば、 絶対リンクが設定されているはずなため、外部リンクが設定されているとみなす。 --\u0026gt; {{- $is_abs_url := or (strings.HasPrefix .URL \u0026#34;https://\u0026#34;) (strings.HasPrefix .URL \u0026#34;http://\u0026#34;) }} \u0026lt;!-- もし外部リンクが設定されていれば、そのまま .URL の内容を出力する。 そうでなければ、内部リンクを language プレフィクスを付与した形で出力する。 --\u0026gt; {{- $menu_item_url := (cond $is_abs_url .URL (printf \u0026#34;%s/\u0026#34; .URL) ) | absLangURL }} \u0026lt;li\u0026gt; \u0026lt;!-- 外部リンクが設定されていたら ($is_abs_url が true なら) a タグに target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; を設定する --\u0026gt; \u0026lt;a href=\u0026#34;{{ $menu_item_url }}\u0026#34; {{- if $is_abs_url }} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34; {{- end}}\u0026gt; \u0026lt;span {{- if eq $menu_item_url $page_url }} class=\u0026#34;active\u0026#34; {{- end }}\u0026gt; {{ .Name }} \u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{- end -}} \u0026lt;/ul\u0026gt; ","permalink":"https://nikaera.com/archives/hugo-open-external-link-in-a-new-window/","summary":"Hugo で設定した外部リンクを開くときは別ウィンドウで開けるようにしたかったので、Hugo のテーマファイルをオーバーライドして対応しました。外部リンクが設定されているときのみ a タグに target=\u0026quot;_blank\u0026quot; rel=\u0026quot;noopener noreferrer\u0026quot; が追加されるようにしました。\nテンプレートファイルは GO の HTML テンプレートで書かれているので、その書式にしたがって a タグの属性を書き換えることで、外部リンクの場合は target=\u0026quot;_blank\u0026quot; rel=\u0026quot;noopener noreferrer\u0026quot; を追加します。\n下記は hugo-PaperMod で、メインメニューの a タグに target=\u0026quot;_blank\u0026quot; rel=\u0026quot;noopener noreferrer\u0026quot; を追加するときのサンプルになります。\n\u0026lt;ul class=\u0026#34;menu\u0026#34; id=\u0026#34;menu\u0026#34; onscroll=\u0026#34;menu_on_scroll()\u0026#34;\u0026gt; \u0026lt;!-- `.Site.Menues.main` の要素数 (メニュー数) ループします --\u0026gt; {{- range .Site.Menus.main }} \u0026lt;!-- 要素内の .URL にアクセスすることで設定されているリンクにアクセスする。 設定されている URL のプレフィクスが https:// or http:// であれば、 絶対リンクが設定されているはずなため、外部リンクが設定されているとみなす。 --\u0026gt; {{- $is_abs_url := or (strings.HasPrefix .URL \u0026#34;https://\u0026#34;) (strings.HasPrefix .URL \u0026#34;http://\u0026#34;) }} \u0026lt;!-- もし外部リンクが設定されていれば、そのまま .URL の内容を出力する。 そうでなければ、内部リンクを language プレフィクスを付与した形で出力する。 --\u0026gt; {{- $menu_item_url := (cond $is_abs_url .","title":"📝 Hugo で外部リンクを target=\"_blank\" で開く方法"},{"content":"はじめに とある事情で MediaPackage のエンドポイント用の CloudFront ディストリビューションを AWS SDK で作成する機会がありました。その際得た知見をソースコードを交えながら備忘録として記事に残しておきます。\n本記事内容で紹介しているソースコードは Gist にも同じ内容でアップしてあります。\nちなみに MediaLive + MediaPackage + CloudFront の構成でインフラ構築したい場合は、CloudFormation が MediaPackage にも対応したので CloudFormation の利用を推奨します。\n本記事内容はあくまでも何らかの事情で、後から CloudFront ディストリビューションを MediaPackage エンドポイントに紐づけたいケース等で参考になると思われます。\n実装内容 作成したソースコードの内容は下記になります。 最下部の createDistributionForMediaPackage が本記事タイトルに該当する関数です。\nimport { CloudFront } from \u0026#34;aws-sdk\u0026#34;; import * as url from \u0026#34;url\u0026#34;; import { CreateDistributionWithTagsResult, GetDistributionResult, UpdateDistributionResult } from \u0026#34;aws-sdk/clients/cloudfront\u0026#34;; export class CloudFrontClientForMediaPackage { private cloudFront: CloudFront; constructor() { this.cloudFront = new CloudFront({ region: \u0026#34;ap-northeast-1\u0026#34;, apiVersion: \u0026#39;2020-05-31\u0026#39;, }); } /** * CloudFront ディストリビューションの情報を取得するために利用する * @param id CloudFront ディストリビューションの ID * @return ディストリビューションの情報を取得する */ async getDistribution(id: string): Promise\u0026lt;GetDistributionResult\u0026gt; { const distribution = await this.cloudFront.getDistribution({ Id: id }).promise() return distribution; } /** * CloudFront ディストリビューションの設定内容を取得するために利用する * @param id CloudFront ディストリビューションの ID * @return ディストリビューションの設定内容を取得する */ async getDistributionConfig(id: string): Promise\u0026lt;CloudFront.DistributionConfig\u0026gt; { const config = await this.cloudFront.getDistributionConfig({ Id: id }).promise() return config.DistributionConfig; } /** * CloudFront ディストリビューションを削除する * @param id 削除したい CloudFront ディストリビューションの ID */ async deleteDistribution(id: string) { const distribution = await this.getDistribution(id); await this.cloudFront.deleteDistribution({ Id: id, IfMatch: distribution.ETag }).promise() } /** * CloudFront ディストリビューションを無効化する * @param id 無効化したい CloudFront ディストリビューションの ID * @return 無効化した CloudFront ディストリビューションの情報 */ async disableDistribution(id: string): Promise\u0026lt;UpdateDistributionResult\u0026gt; { const distribution = await this.getDistribution(id); const config = distribution.Distribution.DistributionConfig; config.Enabled = false; return await this.cloudFront.updateDistribution({ Id: id, IfMatch: distribution.ETag, DistributionConfig: config }).promise(); } /** * MediaPackage のエンドポイント用の CloudFront ディストリビューションを作成する * @param id CloudFront ディストリビューションを判別するための ID * @param mediaPackageArn MediaPackage チャンネルの ARN * @param mediaPackageUrl MediaPackage エンドポイントの URL */ async createDistributionForMediaPackage( id: string, mediaPackageArn: string, mediaPackageUrl: string ): Promise\u0026lt;CreateDistributionWithTagsResult\u0026gt; { // 1. url モジュールを用いて URL 文字列をパースする  const mediaPackageEndpoint = url.parse(mediaPackageUrl); /** 2. MediaPackage のエンドポイント URL から FQDN を取得する。 後述する CloudFront ディストリビューションのオリジンのドメイン名としても利用する */ const mediaPackageHostname = mediaPackageEndpoint.hostname; /** 3. MediaPackage のエンドポイント URL のフォーマットは https://\u0026lt;AccountID\u0026gt;.mediapackage.\u0026lt;Region\u0026gt;.amazonaws.com/**** となっているので、 FQDN の先頭部分を文字列分割で取り出すとアカウント ID が取得できる */ const accountId = mediaPackageHostname.split(\u0026#39;.\u0026#39;)[0]; // 4. 後述する CloudFront ディストリビューションのオリジン ID として、アカウント ID を利用する  const targetOriginId = `MP-${accountId}` /** 5. createDistribution ではなく、createDistributionWithTags 関数で、 CloudFront ディストリビューションを作成する。MediaPackage との紐付けにタグを利用するため。 */ return await this.cloudFront.createDistributionWithTags({ DistributionConfigWithTags: { Tags: { Items: [ /** !!!!!重要!!!!! 6. CloudFront ディストリビューションに紐付けたい MediaPackage エンドポイントのチャンネル ARN を mediapackage:cloudfront_assoc で定義する。 mediapackage:cloudfront_assoc を定義することで、 CloudFront ディストリビューションと MediaPackage チャンネルを紐付けることが可能となる。 */ { Key: \u0026#39;mediapackage:cloudfront_assoc\u0026#39;, Value: mediaPackageArn }, { Key: \u0026#39;Id\u0026#39;, Value: id }, { Key: \u0026#39;Product\u0026#39;, Value: \u0026#39;product\u0026#39; }, { Key: \u0026#39;Stage\u0026#39;, Value: \u0026#39;dev\u0026#39; } ] }, DistributionConfig: { CallerReference: new Date().toISOString(), Comment: `Managed by MediaPackage - ${id}`, Enabled: true, /** 7. CloudFront ディストリビューションのオリジンには 2つ設定します。 1つが MediaPackage のエンドポイントに対するものと、 もう 1つが MediaPacakge サービスに対するものです。 基本的には MediaPackage のエンドポイントに対するオリジンを利用します。 例外時に向けるオリジンが MediaPacakge サービスに対するものになります。 */ Origins: { Quantity: 2, Items: [ { DomainName: mediaPackageHostname, Id: targetOriginId, CustomOriginConfig: { HTTPPort: 80, HTTPSPort: 443, OriginProtocolPolicy: \u0026#39;match-viewer\u0026#39; } }, { DomainName: \u0026#39;mediapackage.amazonaws.com\u0026#39;, Id: \u0026#34;TEMP_ORIGIN_ID/channel\u0026#34;, CustomOriginConfig: { HTTPPort: 80, HTTPSPort: 443, OriginProtocolPolicy: \u0026#39;match-viewer\u0026#39; } } ] }, /** 8. CacheBehaviors のいずれにも当てはまらなかった場合の キャッシュの振る舞いを定義します。 MediaPackage は タイムシフト表示機能を使用する際等で、クエリ文字列に start, m, end を利用しています。 そのため、それらの文字列は WhitelistedNames に含め QueryString には true を指定しておきます。 DefaultCacheBehavior に引っかかる挙動は例外的扱いなので、 使用するオリジンは MediaPackage サービスのものを設定します。 */ DefaultCacheBehavior: { ForwardedValues: { Cookies: { Forward: \u0026#39;whitelist\u0026#39;, WhitelistedNames: { Quantity: 3, Items: [ \u0026#39;end\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;start\u0026#39; ] } }, QueryString: true, Headers: { Quantity: 0 }, QueryStringCacheKeys: { Quantity: 0 } }, MinTTL: 6, TargetOriginId: \u0026#34;TEMP_ORIGIN_ID/channel\u0026#34;, TrustedSigners: { Enabled: false, Quantity: 0 }, ViewerProtocolPolicy: \u0026#39;redirect-to-https\u0026#39;, AllowedMethods: { Items: [ \u0026#39;GET\u0026#39;, \u0026#39;HEAD\u0026#39; ], Quantity: 2, }, MaxTTL: 60 }, /** 9. \u0008CloudFront のエラーコード全ての TTL に 1sec を設定します。 MediaPackage のエラーのキャッシュが長時間持続してしまうと、 その間は MediaPackage で正常に配信できているとしても、 復旧できない状態となるからです。 */ CustomErrorResponses: { Quantity: 10, Items: [ { ErrorCode: 400, ErrorCachingMinTTL: 1 }, { ErrorCode: 403, ErrorCachingMinTTL: 1 }, { ErrorCode: 404, ErrorCachingMinTTL: 1 }, { ErrorCode: 405, ErrorCachingMinTTL: 1 }, { ErrorCode: 414, ErrorCachingMinTTL: 1 }, { ErrorCode: 416, ErrorCachingMinTTL: 1 }, { ErrorCode: 500, ErrorCachingMinTTL: 1 }, { ErrorCode: 501, ErrorCachingMinTTL: 1 }, { ErrorCode: 502, ErrorCachingMinTTL: 1 }, { ErrorCode: 503, ErrorCachingMinTTL: 1 } ] }, /** 10. CloudFront ディストリビューションのキャッシュの振る舞いを 2つ定義します。 それぞれの設定内容は基本的に DefaultCacheBehavior で定義したものと同様です。 しかし、利用するオリジンは MediaPackage エンドポイントに向けたものを利用します。 1つは Microsoft Smooth Streaming での配信時に利用する index.ism に対するもので Smooth Streaming を true に設定しています。 もう 1つは上記 Microsoft Smooth Streaming 以外の 全てに当てはまるストリーミングに適用されるものになります。 */ CacheBehaviors: { Quantity: 2, Items: [{ MinTTL: 6, PathPattern: \u0026#39;index.ism/*\u0026#39;, TargetOriginId: targetOriginId, ViewerProtocolPolicy: \u0026#39;redirect-to-https\u0026#39;, AllowedMethods: { Items: [ \u0026#39;GET\u0026#39;, \u0026#39;HEAD\u0026#39; ], Quantity: 2, }, ForwardedValues: { Cookies: { Forward: \u0026#39;whitelist\u0026#39;, WhitelistedNames: { Quantity: 3, Items: [ \u0026#39;end\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;start\u0026#39; ] } }, QueryString: true, Headers: { Quantity: 0 }, QueryStringCacheKeys: { Quantity: 0 }, }, SmoothStreaming: true }, { MinTTL: 6, PathPattern: \u0026#39;*\u0026#39;, TargetOriginId: targetOriginId, ViewerProtocolPolicy: \u0026#39;redirect-to-https\u0026#39;, AllowedMethods: { Items: [ \u0026#39;GET\u0026#39;, \u0026#39;HEAD\u0026#39; ], Quantity: 2, }, ForwardedValues: { Cookies: { Forward: \u0026#39;whitelist\u0026#39;, WhitelistedNames: { Quantity: 3, Items: [ \u0026#39;end\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;start\u0026#39; ] } }, QueryString: true, Headers: { Quantity: 0 }, QueryStringCacheKeys: { Quantity: 0 }, } }] }, PriceClass: \u0026#39;PriceClass_All\u0026#39; } } }).promise() } } createDistributionForMediaPackage で作成したディストリビューションは、公式ページに記載された手順 で作成した CloudFront ディストリビューションと同等のものになります。\n詳細な説明はインラインコメントにて書きましたが、一応補足説明を少し付け加えておきます。\n随所に出てくる Quantity について Quantity には Items で指定する項目の数を入力します。 例えば Headers や QueryStringCacheKeys には Items に何も指定していないため、Quantity に 0 を指定します。\nしかし、AllowedMethods や WhitelistedNames には Items に指定した項目数である 2 や 3 を Quantity に入力しています。Quantity の数と Items の項目数が合わないと、エラーが発生するため、注意が必要です。\nmediapackage:cloudfront_assoc を定義する意味 CloudFront ディストリビューションのタグに mediapackage:cloudfront_assoc で紐付ける MediaPackage のチャンネル ARN を指定することで、MediaPackage コンソールから紐付けられた CloudFront ディストリビューション情報を参照できるようになります。\n試しに紐づけられた MediaPackage のチャンネルのエンドポイント詳細ページに遷移すると、 下記のような画面が確認できるはずです。\nmediapackage:cloudfront_assoc で紐付いた CloudFront ディストリビューションが確認できる\n本記事内のソースコードでは他にも Id, Product, Stage といったタグを定義していますが、MediaPackage とは関係無いものなので削除して問題ありません。\nupdateDistribution を実行する際の注意点 これは今回の記事内容とは直接関係ないのですが、地味にハマったので載せておきます。\nCloudFront では createDistribution の時に要求されるパラメータよりも updateDistribution で要求されるパラメータのほうが多いです。 AWS 公式ページの比較表にある通りです。\nそのため、updateDistribution で設定を一部更新したいだけなのに、とても多くのパラメータを指定する必要があり非常に面倒です。例えば CloudFront ディストリビューションの Enable/Disable を切り替えるだけでも 30 個近いパラメータを指定する必要あります。\n上記の入力の手間を省くのには getDistribution で取得した既存のディストリビューション情報を改変する形で updateDistribution のパラメータを作成すると楽でした。\n今回のソースコードの内容を参照すると disableDistribution が該当します。\n// 1. getDistribution を実行して CloudFront ディストリビューションの情報を取得する const distribution = await this.getDistribution(id); // 2. CloudFront ディストリビューションの設定内容を取得する const config = distribution.Distribution.DistributionConfig; // 3. CloudFront ディストリビューションの Enabled/Disabled を切り替えるオプションを改変する config.Enabled = false; // 4. 3. で改変した内容を updateDistribution で CloudFront ディストリビューションに反映する return await this.cloudFront .updateDistribution({ Id: id, IfMatch: distribution.ETag, DistributionConfig: config, }) .promise(); おわりに ニッチな内容なので、本記事内容を今後利用するかどうかは分かりませんが、一応得た知見を記事として残しておきました。同様のことを行う必要が出てきた方の参考になれれば幸いです。\n参考リンク  Class: AWS.CloudFormation — AWS SDK for JavaScript Amazon CloudFront を MediaPackage に使用する - AWS Elemental MediaPackage Required fields for creating and updating distributions - Amazon CloudFront CloudFront と AWS Media Services によるライブストリーミングビデオの配信  ","permalink":"https://nikaera.com/archives/cloudfront-for-mediapackage/","summary":"はじめに とある事情で MediaPackage のエンドポイント用の CloudFront ディストリビューションを AWS SDK で作成する機会がありました。その際得た知見をソースコードを交えながら備忘録として記事に残しておきます。\n本記事内容で紹介しているソースコードは Gist にも同じ内容でアップしてあります。\nちなみに MediaLive + MediaPackage + CloudFront の構成でインフラ構築したい場合は、CloudFormation が MediaPackage にも対応したので CloudFormation の利用を推奨します。\n本記事内容はあくまでも何らかの事情で、後から CloudFront ディストリビューションを MediaPackage エンドポイントに紐づけたいケース等で参考になると思われます。\n実装内容 作成したソースコードの内容は下記になります。 最下部の createDistributionForMediaPackage が本記事タイトルに該当する関数です。\nimport { CloudFront } from \u0026#34;aws-sdk\u0026#34;; import * as url from \u0026#34;url\u0026#34;; import { CreateDistributionWithTagsResult, GetDistributionResult, UpdateDistributionResult } from \u0026#34;aws-sdk/clients/cloudfront\u0026#34;; export class CloudFrontClientForMediaPackage { private cloudFront: CloudFront; constructor() { this.cloudFront = new CloudFront({ region: \u0026#34;ap-northeast-1\u0026#34;, apiVersion: \u0026#39;2020-05-31\u0026#39;, }); } /** * CloudFront ディストリビューションの情報を取得するために利用する * @param id CloudFront ディストリビューションの ID * @return ディストリビューションの情報を取得する */ async getDistribution(id: string): Promise\u0026lt;GetDistributionResult\u0026gt; { const distribution = await this.","title":"📔 MediaPackage 用の CloudFront ディストリビューションを AWS SDK で作成する"},{"content":"はじめに 最近 ErgoDox EZ からの乗り換えで Moonlander というエルゴノミクスキーボードを使っているのですが、諸々非常に満足しています。コンパクトながら安定感のある打ち心地でカスタマイズ性も高く、試行錯誤しながら自分好みにセットアップして使い勝手を最適化することができます。\nバックライトがあり、デフォで 様々な光り方 が用意されているのも非常にカッコよいです。あまりキーボードのバックライトを気にしたことは今まで無かったのですが、何となく 1日毎にエフェクトを切り替えると良い気分転換になります。なんとなくだけど。。ｗ\nまた、これについてはエルゴノミクスキーボード全般に言える話で Moonlander に限った話では無い気がしますが、体がこわばった姿勢にならなくなり、肩が開いてリラックスした姿勢でタイピングできます。\nそのため、長時間 PC で作業していても、呼吸が浅くなりづらく、肩への負担も少なく感じます。現に筆者はキーボードを変えただけで生活習慣変えた覚えがないにも関わらず、長時間作業しても疲れにくくなりましたし、肩こりになりにくくなりました。\n毎年冬になると肩こりになる体質だったのですが、ErgoDox EZ を採用し始めてから肩こりに悩まされることは無くなりました。\n今回はそんな気に入って仕事プライベート問わず酷使している Moonlander についての紹介記事を書いていこうと思います。\nちなみに筆者はキーコンフィグを軽くカスタマイズしている程度のライトユーザーです。\nセットアップが簡単 Web から簡単にキーコンフィグを設定することが可能です。キーコンフィグ設定のためのサービスは Ergodox EZ Configurator というやつです。名前に Ergodox がついていますが ZSA Technology Labs から購入可能なキーボード全てに対応しています。(Moonlander にも Planck にも対応)\n今回は Moonlander Mark I を利用する\nSearch layouts ボタンをクリックすることで、他の人が既にカスタマイズしたキーコンフィグをダウンロードしたり、そのキーコンフィグを元に自分用のキーコンフィグをカスタマイズすることも可能です。\nちなみにデフォルトのキーコンフィグは Configure ボタンをクリックすると確認することができます。もちろん、そこからキーコンフィグをカスタマイズしていくことも可能です。\nまた、カスタマイズした設定内容は常に Ergodoz EZ Configurator に保存されるので、後から細かくキーコンフィグを修正していくといったことも可能です。 最初のうちは頻繁にキーコンフィグに微修正入れると思うので、めっちゃ便利でした。\nErgodoz EZ Configurator の Search layouts で \u0026ldquo;coding\u0026rdquo; で検索した結果\nちなみに僕が使用しているキーコンフィグは こちら です。他の方々のキーコンフィグと比べると大分シンプルですが、その分初見の方でも扱いやすいコンフィグだと思います多分\u0026hellip;\nキーコンフィグの更新も簡単 キーコンフィグの設定が完了したら Wally というツールを使ってキーコンフィグ設定を実際にキーボードに反映させることが可能です。\n余談ですが、Wally は Go言語で作成されていて GitHub でソースコードが公開されています。\nWally のトップページからツールをダウンロードする\nWally アプリを起動したら、Moonlander を接続していれば認識されている様子が確認できるはずなので、 Ergodox EZ Configurator で生成してダウンロードした bin ファイルを Wally アプリウインドウ上にドラッグ \u0026amp; ドロップします。\nWally を起動してダウンロードした bin ファイルをドラッグ \u0026amp; ドロップする\nその後、リセットボタンの入力を求められるのでキーボード左上の小さな穴内部にあるボタンをクリックするか、アプリ右下の Reset をクリックします。これだけでキーコンフィグを反映させることが可能です。\nちなみにリセットボタンはキーコンフィグで特定のキーに割り当てることも可能です。\nデュアルファンクションキーが便利 例えば Shift や Ctrl キー等について、特定キーを長押しすることで発火させることが出来るような機能です。これはシンプルなのですが、個人的に非常に強力で一度慣れると普通のキーボードにも設定したくなるレベルでした。\nF キーのデュアルファンクションキーの設定内容\n例えば A キーもしくは；キーを長押しすると Shift キーの振る舞いになるよう設定していると、プログラミングしている最中キャメルケースで変数を書く時に、ホームポジションから、一切指を動かさずに楽に英語を大文字入力することが可能です。\nまた L キーもしくは S キーを長押しすると Win/CMD キーが発動する様にしているのですが、そのおかげで Windows では作業中、特定アプリを起動したい時に L キー長押しで出てきた Win メニューから文字列検索でサクッと特定アプリを起動することが出来ます。この間も指は一切動かさずに済みます。\nデュアルファンクションキーは Ergodox EZ Configurator で Modify layout 中に、該当キーをクリックして出てくる、Keys タブから行うことが可能です。\nデュアルファンクションキーの設定方法\n上記のおかげでマウスに触る頻度も低くなり、結果としてキーボードで色々な操作をやり繰りしていると PC での作業効率が非常に上がりました。\nレイヤーを用いることでキータイプの効率を向上させる Moonlander にはレイヤーという概念があり、各種キーの動作をマルっと差し替えることが可能です。例えば レイヤー1 には通常通りの英字入力を割り当てておき、レイヤー2 には特殊文字や記号入力を普段打ちやすい位置に割り当て、レイヤー3 にはネットサーフィン用にマウス操作やブラウザ操作等を割り当てておくことが可能です。\nレイヤーの確認および設定方法\nこの設定を切り替える方法も様々用意されていて、例えば 特定キーを押しっぱなしにしているとレイヤーが切り替わるように出来たり、特定キーを押したらもう一度同一キーをクリックしない限り、そのレイヤーが適用され続けるといったことも可能です。\n設定を切り替えるためのキーコンフィグ (レイヤースイッチ)\nちなみに上記のレイヤー設定の例は筆者が現在利用しているキーコンフィグです。基本レイヤー1が常に使われ続ける設定で、レイヤー2 には親指周辺のキー押しっぱで切り替わるようにしております。レイヤー3 は一度特定キーを押したら反映され続けるようにしていて、ネットサーフィン用の設定を割り当てられています。\nレイヤーは最大 32個まで設定可能らしいですが、筆者はフルで設定されているキーコンフィグには未だ出会ったことがありません\u0026hellip;w\nよく聞かれる質問 エルゴノミクスキーボードを使っていると良く 「何が良いの？」「普通のキーボードが打てなくなるんじゃない？」「タイピング速度は速くなるの？」 等色々聞かれることが多いので、今回はそれらのよく聞かれる質問について自分の回答をまとめてみました。\n Q. どれくらいの期間でエルゴノミクスキーボードが実務でも使えるくらいに習熟した？\nA. 英字は最初から問題なく打てましたが、記号については慣れるまで 2-3週間くらいはかかった記憶があります。 慣れるまでは、自分の設定したキーコンフィグを何回も見返して照らし合わせながら打ってたので尋常じゃなく打つの遅かったです\u0026hellip;\n Q. いつも使ってる普通のキーボード打てなくならない？\nA. 僕は最初会社では通常キーボードで家では ErgoDox EZ で特訓していたのですが、確かに最初のうちは混乱して頭の切り替えまでに 20分くらいは要してました。ですが、大体 1ヵ月程度で勝手にキーボード触る時に頭がそのキーボード用の操作にスイッチされるようになりました。今ではどちらのキーボードでも全く問題なく打ててます。 キーコンフィグを常にライトにしているせいもあるかもしれません。\n Q. キーコンフィグはデフォルトのままでも大丈夫？\nA. デフォだと英字配列になっているのと、一部キーが特殊な振る舞いをするように設定されているので、最初にそこら辺のキーを整理しないと普段使いにはキツいと思います。そのため、無理にデフォ設定を使用するよりも、キーボードが届いたら真っ先に自分用のキーコンフィグを行うことをオススメします。 ちなみに最初のうちはキーコンフィグの設定にめちゃくちゃハマって色々細かくチューニングしまくって時間溶けまくってたのですが、筆者は最終的にシンプルな設定に落ち着きました。\n Q. 肩こり解消にもなるって本当？\nA. 僕は肩こり改善しました。運動は普段一切しないし、通常キーボードで日常業務していた時は常に PC と向かい合っていて慢性的に肩が緊張状態で肩こり気味だったのですが、分割キーボードにしてからは肩こりになったことは通年でほぼありません。\n Q. タイピング速度は速くなる？\nA. タイピング速度が早くなったかは分かりませんが、打ち間違いは減りました。 手が小さいのも関係しているかもしれませんが、基本ホームポジション中心にキーを配置して、配置しきれないキーはレイヤー分けしてホームポジション中心に配置しているので指の動きが常に小さくなったのが関係しているのかもしれません。\nおわりに 最初購入して届いたときは不安でしたが、真っ先にキーコンフィグを普段使っているキーボードと同等にしてから、徐々に自分用のキーコンフィグにカスタマイズしつつ、寿司打 等でキータイプ練習および実際のプログラミングで記号入力の練習をしていれば、大体 1-2週間くらいで実践投入可能になると思います。\n一旦慣れてしまえばタイピングしていても、体がこわばった姿勢にならなくなるので疲れにくくなりますし、肩も開いてリラックスした姿勢でタイピングできるので呼吸もしやすくて、肩への負担も少なく感じます。\nキーボードが分割されているので、恐らくブラインドタッチも身につくと思います。というより身につけざるを得ない\u0026hellip;？？\nちなみに Moonlander のキーコンフィグには他にも便利なキー設定が沢山あり、Hyper キーやバックライトを変更するキーを割り当てたり、チュートリアルが起動するキーなども割り当てることなども可能です。音楽の再生切り替えたりとかも\u0026hellip;w\nこの記事を見てエルゴノミクスキーボードに、少しでもご興味持っていただいた方がいれば、最初は飛び込むのにはちょっと勇気がいるかもしれませんが、、値段もそれなりだし\u0026hellip; ｗ\nこの機会に是非 ErgoDox EZ もしくは Moonlander 等のエルゴノミクスキーボードを試してみて欲しいです！\n","permalink":"https://nikaera.com/archives/introduction-to-moonlander/","summary":"はじめに 最近 ErgoDox EZ からの乗り換えで Moonlander というエルゴノミクスキーボードを使っているのですが、諸々非常に満足しています。コンパクトながら安定感のある打ち心地でカスタマイズ性も高く、試行錯誤しながら自分好みにセットアップして使い勝手を最適化することができます。\nバックライトがあり、デフォで 様々な光り方 が用意されているのも非常にカッコよいです。あまりキーボードのバックライトを気にしたことは今まで無かったのですが、何となく 1日毎にエフェクトを切り替えると良い気分転換になります。なんとなくだけど。。ｗ\nまた、これについてはエルゴノミクスキーボード全般に言える話で Moonlander に限った話では無い気がしますが、体がこわばった姿勢にならなくなり、肩が開いてリラックスした姿勢でタイピングできます。\nそのため、長時間 PC で作業していても、呼吸が浅くなりづらく、肩への負担も少なく感じます。現に筆者はキーボードを変えただけで生活習慣変えた覚えがないにも関わらず、長時間作業しても疲れにくくなりましたし、肩こりになりにくくなりました。\n毎年冬になると肩こりになる体質だったのですが、ErgoDox EZ を採用し始めてから肩こりに悩まされることは無くなりました。\n今回はそんな気に入って仕事プライベート問わず酷使している Moonlander についての紹介記事を書いていこうと思います。\nちなみに筆者はキーコンフィグを軽くカスタマイズしている程度のライトユーザーです。\nセットアップが簡単 Web から簡単にキーコンフィグを設定することが可能です。キーコンフィグ設定のためのサービスは Ergodox EZ Configurator というやつです。名前に Ergodox がついていますが ZSA Technology Labs から購入可能なキーボード全てに対応しています。(Moonlander にも Planck にも対応)\n今回は Moonlander Mark I を利用する\nSearch layouts ボタンをクリックすることで、他の人が既にカスタマイズしたキーコンフィグをダウンロードしたり、そのキーコンフィグを元に自分用のキーコンフィグをカスタマイズすることも可能です。\nちなみにデフォルトのキーコンフィグは Configure ボタンをクリックすると確認することができます。もちろん、そこからキーコンフィグをカスタマイズしていくことも可能です。\nまた、カスタマイズした設定内容は常に Ergodoz EZ Configurator に保存されるので、後から細かくキーコンフィグを修正していくといったことも可能です。 最初のうちは頻繁にキーコンフィグに微修正入れると思うので、めっちゃ便利でした。\nErgodoz EZ Configurator の Search layouts で \u0026ldquo;coding\u0026rdquo; で検索した結果\nちなみに僕が使用しているキーコンフィグは こちら です。他の方々のキーコンフィグと比べると大分シンプルですが、その分初見の方でも扱いやすいコンフィグだと思います多分\u0026hellip;\nキーコンフィグの更新も簡単 キーコンフィグの設定が完了したら Wally というツールを使ってキーコンフィグ設定を実際にキーボードに反映させることが可能です。","title":"📔 Moonlander というエルゴノミクスキーボードのススメ"},{"content":"この記事は Static Site Generator Advent Calendar 2020 10 日目の記事です。\nはじめに Zenn や Qiita, note など様々なウェブサービスで記事を書くにつれて、ふと雑多な内容で自分の好き勝手に記事を書いて公開できる自分のブログが欲しくなりました。そこで、自分のブログを作ろうと思い調査したところ、SSG で作るのが手っ取り早そうだったのと、その中でも一番ラクにウェブサイトが構築できそうな Hugo を採用しました。\nまた、デプロイは簡単に行いたかったので、デプロイ先として GitHub Pages を採用しました。 独自ドメインの割り当てから HTTPS 対応まで無料でできるかつ、使い慣れている GitHub をデプロイ先に使えることが決め手でした。\nHugo で自分のブログを構築して GitHub Pages で公開できるようになったのですが、ブログ内容を更新したり記事を書くたびにビルドしてデプロイをするのが、意外と面倒なことに気づきました。そこで、GitHub Pages action を用いて、ビルドしてデプロイするという作業は自動化しました。\n上記までの作業をすることで、自分のブログを書いたり更新することだけに集中できる環境を整えることができました。ウェブサイトを作りこむ以外は、簡単ないくつかの作業をするだけで Hugo で満足のいく自分のブログを書く環境が整えられたので、その手順についてまとめてみました。\nちなみに本記事の手順で実際に作成した私のブログは下記です。\nhttps://nikaera.com/\nHugo を PC にインストールする 私は Windows には Chocolatey、Mac では Homebrew で Hugo をインストールしました。Chocolatey や Homebrew を利用したインストール方法については 公式サイトの手順 で公開されています。\n# install-with-homebrew.sh # Mac で Homebrew を使って Hugo をインストールする brew install hugo # install-with-chocolatey.ps1 # Windows で Chocolatey を使って Hugo をインストールする choco install hugo -confirm # Sass/SCSS を用いてウェブサイトのデザイン改修を行いたい場合は # 下記で Chocolatey を使って Hugo をインストールする choco install hugo-extended -confirm Hugo で自分のウェブサイトを構築する Hugo プロジェクトを作成する 下記コマンドで Hugo のプロジェクトフォルダを生成できます。\n# create-new-site.sh hugo new site \u0026lt;プロジェクトフォルダのパス\u0026gt; Hugo のコンフィグファイルのデフォルトフォーマットは TOML 形式なのですが、hugo new site \u0026lt;プロジェクトフォルダへのパス\u0026gt; -f json のように -f オプションを付与することで JSON フォーマットでもコンフィグファイルを生成可能です。\n後述しますが、一部の Hugo のテーマはコンフィグファイルのサンプルが JSON ファイルで書かれています。その場合は、新規で設定するコンフィグファイルのフォーマットも JSON で統一しておくと各種設定項目の調整が楽になりそうです。\nもしくはこちらのようなコンバーターを使用したり、こちらのようなウェブのコンバーターを使用して、設定ファイルを JSON から TOML フォーマットに変更しても良さそうです。\nウェブサイトのテーマを探す テーマとは、ウェブサイトのデザインテンプレートのことです。テーマは Hugo Themes で公開されているので、この中から自分の好みを選びます。Hugo ではテーマの内容をカスタマイズしたり差し替えることもできるので、あとから一部デザインを自分好みに修正できます。\nHugo Themes にアクセスした時のページ\nお気に入りのテーマが見つかったら、Download ボタンをクリックしてテーマの GitHub URL を控えておきます。\nKiera というテーマのページ\nHugo プロジェクトにテーマを適用する テーマの適用方法については 公式サイトの手順 で紹介されていますが、Hugo のプロジェクトフォルダのルートで下記コマンドを実行して、コンフィグファイルに theme を追記するだけです。\n# apply-template.sh git clone \u0026lt;テーマの GitHub URL\u0026gt; themes/\u0026lt;テーマ名\u0026gt; --depth=1 echo \u0026#39;theme = \u0026#34;\u0026lt;テーマ名\u0026gt;\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml どのテーマも基本的に適用方法は同じで、例えば私が採用したテーマである PaperMod を適用する場合は下記コマンドを実行することになります。\n# apply-papermod-template.sh git clone https://github.com/adityatelange/hugo-PaperMod themes/hugo-PaperMod --depth=1 echo \u0026#39;theme = \u0026#34;hugo-PaperMod\u0026#34;\u0026#39; \u0026gt;\u0026gt; config.toml これで自分のウェブサイトを作り込んでいくための準備が整いました。\nウェブサイトを作り込む 各テーマには exampleSite というウェブサイト作成の参考になる Hugo のプロジェクトフォルダが存在します。 最初は exampleSite フォルダ内に存在する各種ファイルを自分のプロジェクトにコピー\u0026amp;ペーストして、改変しながらウェブサイトを作り込んでいくとスムーズに作業が進められます。\nexampleSite フォルダは大抵 GitHub プロジェクトのルートに存在しているのですが、GitHub のブランチで管理しているものもあります。 私の採用した PaperMod は GitHub の exampleSite ブランチで管理していました。\n上記のような詳細は Hugo のテーマページに記載があるはずなので、事前に exampleSite フォルダが配置されている場所や設定可能な項目等をチェックしておくことをオススメします。実際のウェブサイトの見た目を確認するには、Hugo のプロジェクトフォルダのルートで下記コマンドを実行します。\nhugo server hugo server 実行中に、ウェブサイトの設定を変更したり記事を追加すると、自動的にビルドが実行されるので常に最新のウェブサイトの見た目を確認できます。また、その際にエラーもコンソールに出力されるので、適宜修正しながらウェブサイトの作成を進めていくことが可能です。\nGitHub にデプロイ環境を整える ウェブサイトの構築が出来ればあとはデプロイするだけです。今回は GitHub Actions でデプロイするため、残りの作業は GitHub 上で進めていきます。一応 GitHub Actions を使わずに手動でデプロイする手順については、Hugo の 公式サイトの手順 にて紹介されています。\nGitHub Actions を用いてデプロイできるようにした際の利点としては下記があります。\n デプロイ時に毎回手動で複数コマンド実行する手間が省ける 自動化することでデプロイ時のコマンドミスを防ぐことが可能 常に統一した Hugo のビルド環境が利用可能  最初のうちは私も公式サイトの手順通りに Mac で手動デプロイしていました。しかし、Windows 環境でデプロイ作業した際、本番環境デプロイ時に SRI 関連のエラーが発生してしまい、ウェブサイトに stylesheet が適用されないバグが発生してしまいました。\n結局原因はよく分からなかったのですが、GitHub Actions 経由でデプロイするようにしたところ直りました。CI 経由でデプロイできるようになると、こういった実行環境の違いによる挙動も気にする必要が無くなります。\n一応 Windows 環境で発生した SRI 関連のバグは Hugo で該当するテンプレートファイルを layouts フォルダを利用して差し替えて、integrity の設定内容を空にすることで、本番環境でも stylesheet が適用できるようになったことは確認しました。詳細はこちら。\n自分のウェブサイトをデプロイするためのリポジトリを作成する GitHub の 公式サイトの手順 に従って、自身のウェブサイトをデプロイするためのリポジトリを作成します。また本記事では、Hugo プロジェクトのリポジトリはデプロイ用リポジトリとは別で扱うため、Hugo プロジェクトのリポジトリも新たに作成します。\n本記事では Hugo プロジェクトのリポジトリ名は hugo-blog という名前に設定した前提で進めていきます。また作成したリモートリポジトリの情報は、下記コマンドで Hugo プロジェクトに登録しておきます。\n# Hugo プロジェクトフォルダのルートで Git リポジトリを作成する git init # Hugo プロジェクトのリポジトリ情報を登録しておく (hugo-blog の GitHub URL) git remote add origin \u0026lt;Hugo プロジェクトのリポジトリの URL\u0026gt; GitHub Actions で Hugo のビルドからデプロイまでを自動化するための環境を整える 今回は hugo-blog という Hugo プロジェクトのリポジトリから、デプロイ用リポジトリへデプロイしたいため、まずはデプロイ用リポジトリでデプロイキーを登録します。公式サイトの手順 に従いデプロイキーを登録したら、秘密鍵を hugo-blog リポジトリのシークレットに登録します。\nシークレットは 公式サイトの手順 に従って登録します。今回は秘密鍵をシークレットに登録する際の名前に ACTIONS_DEPLOY_KEY を設定した前提で進めていきます。\n次に GitHub Pages action を導入して Hugo のビルドから公開までを自動化する環境をセットアップします。\nhugo-blog リポジトリに GitHub Pages action の Getting started を参考にワークフローファイルを追加します。- name: Deploy の項目のみデプロイ用リポジトリへデプロイするために設定内容を変更します。\n# .github/workflows/gh-pages.yml name: github pages on: push: branches: - main # Set a branch name to trigger deployment jobs: deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#34;0.78.2\u0026#34; - name: Build run: hugo --minify # - name: Deploy # uses: peaceiris/actions-gh-pages@v3 # with: # github_token: ${{ secrets.GITHUB_TOKEN }} # publish_dir: ./public - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: nikaera/nikaera.github.io publish_branch: main cname: nikaera.com - name: Deploy の項目で設定した各種パラメータは下記になります。\n   キー 説明     deploy_key デプロイ時に使用する秘密鍵   external_repository デプロイ先のリモートリポジトリ   publish_branch デプロイ先のリモートリポジトリのブランチ   cname 設定するカスタムドメイン名    deploy_key にはシークレットに登録した秘密鍵を設定します。external_repository には Hugo をビルドした際のデプロイ先リポジトリを \u0026lt;ユーザ名\u0026gt;/\u0026lt;リポジトリ名\u0026gt; のフォーマットで指定します。publish_branch はデプロイ先として使用するブランチ名になります。cname には自分が設定したいドメイン名を指定します。cname の 詳細 はこちらからご確認いただけます。\nカスタムドメインで設定した内容で DNS 設定を書き換える GitHub Pages にカスタムドメインを利用する際は、該当するドメインの DNS レコードの設定で CNAME レコードもしくは A レコードを設定する必要があります。公式サイトの手順 に従って設定します。\nまた、カスタムドメインの設定後は特別な理由がない限りは、デプロイ用リポジトリで HTTPS 強制の設定 をしておくことオススメします。\nちなみに GitHub Pages で利用している証明書は Let\u0026rsquo;s Encrypt のものになります。\n設定作業はこれで完了です。あとは実際に Hugo プロジェクトを更新後、hugo-blog リポジトリに push することでビルドからデプロイまで GitHub Actions で行われるようになったかを確認していきます。\nHugo プロジェクトの更新時に自動でデプロイが行われるか確認する Hugo プロジェクトには既に hugo-blog リポジトリの GitHub URL が origin として設定されているはずなので、下記で実際に hugo-blog リポジトリへ push してみます。\n# Hugo プロジェクトを hugo-blog リポジトリに push する git add -A git commit -m \u0026#34;initial commit\u0026#34; git push origin main 次は実際に GitHub リポジトリの Actions タブから、GitHub Pages action のワークフローが実行されているか確認します。\nGitHub Pages actions の実行に成功した様子\n無事ワークフローの実行に成功したことを確認したら、デプロイ用リポジトリの様子を確認します。\nデプロイ用リポジトリに Hugo の最新ビルドが push されていることを確認する\n最新コミットのメッセージに deploy: \u0026lt;ユーザ名\u0026gt;/hugo-blog@\u0026lt;コミットハッシュ\u0026gt; のコメントが表示されているはずです。コメントのリンクをクリックすると、hugo-blog リポジトリの最新コミットの確認画面に遷移するはずです。\nここまで確認できれば、あとはローカルで hugo server して確認していたウェブサイトと同じように、GitHub Pages 上でもウェブサイトが見えるか実際に確認してみます。\nデプロイ用リポジトリ名が \u0026lt;ユーザ名\u0026gt;/\u0026lt;ユーザ名\u0026gt;.github.io であった場合、https://\u0026lt;ユーザ名\u0026gt;.github.io でウェブサイトにアクセスできます。私の場合は https://nikaera.github.io になります。その際、カスタムドメインにリダイレクトすることも確認できれば、カスタムドメインの設定も正常に反映されています。\nページが正常に表示されていてカスタムドメインでアクセスできた様子\nアクセス時に意図したページが表示されていることが確認できれば大丈夫です。これで自分のウェブサイトを更新したら hugo-blog リポジトリに Hugo プロジェクトを push するだけで自分のウェブサイトを自動更新できるようになりました。\nおわりに 今回は Hugo を例にしましたが、本記事内で紹介した GitHub Pages action は 様々な SSG に対応しています。そのため Hugo ではウェブサイトのカスタマイズに限界が来たなと感じたら Next.js や Gatsby に乗り換えるといったことも可能です。\nまた Hugo では何も考えずとも、マークダウンで記事が書けてビルドも高速なので、手っ取り早く自分のウェブサイトを構築してみたいという用途にはピッタリだと感じました。\n関係ないですが、Hugo でウェブサイト構築する際の知見も記事に含めてしまったせいで、文章量が意図した倍近い量になってしまいました。。簡潔で分かりやすい文章が書けるようにならなきゃ。。\n参考リンク  The world’s fastest framework for building websites | Hugo Complete List | Hugo Themes GitHub Pages について - GitHub Docs GitHub Pages action Install Hugo | Hugo Quick Start | Hugo Host on GitHub | Hugo GitHub Pages | Websites for you and your projects, hosted directly from your GitHub repository. Just edit, push, and your changes are live. デプロイキーの管理 - GitHub Docs 暗号化されたシークレット - GitHub Docs GitHub Pages サイトのカスタムドメインを管理する - GitHub Docs HTTPS で GitHub Pages サイトを保護する - GitHub Docs  ","permalink":"https://nikaera.com/archives/hugo-github-actions-for-github-pages/","summary":"この記事は Static Site Generator Advent Calendar 2020 10 日目の記事です。\nはじめに Zenn や Qiita, note など様々なウェブサービスで記事を書くにつれて、ふと雑多な内容で自分の好き勝手に記事を書いて公開できる自分のブログが欲しくなりました。そこで、自分のブログを作ろうと思い調査したところ、SSG で作るのが手っ取り早そうだったのと、その中でも一番ラクにウェブサイトが構築できそうな Hugo を採用しました。\nまた、デプロイは簡単に行いたかったので、デプロイ先として GitHub Pages を採用しました。 独自ドメインの割り当てから HTTPS 対応まで無料でできるかつ、使い慣れている GitHub をデプロイ先に使えることが決め手でした。\nHugo で自分のブログを構築して GitHub Pages で公開できるようになったのですが、ブログ内容を更新したり記事を書くたびにビルドしてデプロイをするのが、意外と面倒なことに気づきました。そこで、GitHub Pages action を用いて、ビルドしてデプロイするという作業は自動化しました。\n上記までの作業をすることで、自分のブログを書いたり更新することだけに集中できる環境を整えることができました。ウェブサイトを作りこむ以外は、簡単ないくつかの作業をするだけで Hugo で満足のいく自分のブログを書く環境が整えられたので、その手順についてまとめてみました。\nちなみに本記事の手順で実際に作成した私のブログは下記です。\nhttps://nikaera.com/\nHugo を PC にインストールする 私は Windows には Chocolatey、Mac では Homebrew で Hugo をインストールしました。Chocolatey や Homebrew を利用したインストール方法については 公式サイトの手順 で公開されています。\n# install-with-homebrew.sh # Mac で Homebrew を使って Hugo をインストールする brew install hugo # install-with-chocolatey.","title":"📔 Hugo + GitHub Pages + GitHub Actions で独自ドメインのウェブサイトを構築する"},{"content":"はじめに まず、私の誕生日は 11/6 なので本日 12/6 は誕生日からちょうど 1ヶ月後になります。\n誕生日なので何か書くことにした✍️ | なんでもに参加しました！ https://t.co/1NXySfMlak #Crieit #crieit_advent_calendar @crieitcommunityより\n\u0026mdash; 𝚗𝚒𝚔𝚊𝚎𝚛𝚊 (@n1kaera) November 2, 2020  はい。完全にノリで参加しようと投稿予約してから、後日確認して気づきました。。ｗ しかし、その間違えに気づいてから、何を投稿しようかなあと考え始めた時に、ふと「以前の自分だったら絶対ノリでアドベントカレンダーに登録するっていう行動しないよなあ」ということに気づきました。\nそこで、今回は以前の自分と比較して、今の自分がどう変わったのか考察してみることにしました。この記事書いてるときは誕生日からまだ 1週間経ってないし、ちょうど考察するには良い機会かなと思って。\n以前の自分 Photo by Luis Villasmil on Unsplash\n以前はやりたいことがあっても全部後回しにしてしまっていました。何故かと言うと仕事やプライベートも含めて、他に直近でやらなければならない、もしくはそう思いこんでいる TODO があったときに、それらが残っていると次々に心配事や考え事が増えていってしまい、やりたいことに集中出来なくなるからです。\nつまり、やりたいことは本気で集中できるタイミングが来たらやろうと考えていました。\nそのため、読もうと思って買った本を読みたい、買って届いた新作ゲームをやりたい、OSS 活動をしてみたい、ライブラリを自分で作成してみたい等々のやりたいことは全て後回しになっていました。\n何も考えないでサクッと取り組めるプレイ済みのゲームを遊んだり、気に入っている Youtube の動画を何度も繰り返し見たりして時間を浪費していました。\nボーッとしながら何も考えないで過ごす時間や遊ぶ時間は気力を養う上で重要で必要だと認識しているのですが、それだけを余暇に費やすのは少し勿体ないなと感じていました。\nそして、ある時ふと気づきました。多分このままの生活してると連休とか仕事の区切りのタイミングとかでしか、自分がやりたいことに取り組むことはできないなと。。ちなみに、私はいわゆる ON/OFF みたいな切り替えが出来ないタイプなので、「仕事が終わったらスイッチを OFF にしてプライベートを全力で楽しむようにする」とかは無理なタイプです。\nそこで、過去自分が気分がノリ続けて作業できたり、新しいことに挑戦し始めたときのことを思い返しながら、現状改善のために簡単に出来ることからやってみようと 1年前くらい前から行動を起こし始めました。\n試したこと Free-PhotosによるPixabayからの画像\nまず、前提として下記項目はあえて頑張って解決するとかは諦めました。理由は中学生くらいの時から「そうだったなあ」って感じていることだったので潔く諦めた感じです。\n 仕事プライベート関係なく、やりたくない面倒な TODO を意識してしまい無駄に疲れる  ふとした瞬間に思い出して不快になるかつ、それらの TODO は後回しにしてしまいがちなので、更に不快になる機会が増えてきて、結果疲れ続ける   やるべき必要がある TODO が増えてくると最優先で迅速に全部片付けたくなる  それらが意識の端にあると心を休めた状態で日々を過ごすことが難しくなってきて、結果疲れ続ける    上記の解決を諦めても、やりたいことが自然と出来る環境が作れるようにしようとしました。\n仕事プライベート問わず TODO の総量を減らしていく TODO が増えてくることによって、やるべき TODO も必然的に増えてきます。最初は優先度の低い TODO だったのに、作業していくうちにそこから更にやるべき他の TODO が生まれてくることもあります。\nそのため、そもそも仕事でもプライベートでも優先度とか関係なく TODO を日常的に増やさない努力を行いました。 割とお願いされたら何でも引き受けてしまいがちだったので、そこは常に意識するようにしました。\n直感的に自分に向いて無いと思ったり、自分が請け負う必要が無い TODO は極力減らすか、その内容を簡素にしてもらいました。 常に多少の余裕が感じられる範囲で TODO を抑え込むようにすることで無駄に TODO を意識して気力を消耗し続けることは少なくなっていきました。\nやりたくない TODO は疲れない範囲で対処していく もちろん、それでも 出来ればやりたくないけど、やらないといけない TODO は出てきました。\nただ、やりたくない TODO は大抵やりたくない何かを自分が勝手に想像していることも多いです。つまり、TODO の具体的な内容が明らかになっていない中で勝手にその作業内容を想像してしまい嫌になっているケースです。\nその場合は、作業内容を具体化する作業を行うことで解決できる可能性があります。やりたくないと考えていることが、実は絵空事の場合もあるからです。\n例えば、TODO を依頼してきた相手がいる場合、具体的な作業内容を文章にしてもらう、万が一それが出来ないと言われた場合は、自分が不快に感じないレベルで作業して適宜内容を報告することで、相手が方向性を正しいか間違っているかを随時指摘してくれて、次の作業の内容もどんどん具体化してきて TODO の全体像が見えるようになってきました。\n全体像が見えてきてしまえば、あとは適切な粒度に TODO を切り分けてひたすら消化していくだけなので、無駄に意識が TODO に向かってしまい気力が消耗していかないよう、サクサク片づけるように心がけました。\n段々 TODO の扱い方が分かってくると、常に一定以上は自分のために使える時間が作れるようになってきました。\n軽めのやりたいことは時間が出来れば自然にやれてた 意識するまでも無く時間が作れるようになってきてからは、新作ゲームや積読してた本に取り掛かることを意識せず自然にやってました。TODO 意識しちゃって新作楽しんでても心から楽しめないとかいう気持ちから脱することができる状況になったおかげで、自然と新たな行動が起こせるようになったからかもしれません。\n重めのやりたいことには必ず「自分のため」の理由付けを行う 例えばやりたいことを行う理由が「誰かに認められたい」とか「自分の価値を周りに認めてもらいたい」とか世間体を気にした動機になってしまっていると、そのやりたいことは他人が認めてくれて初めて達成となります。つまり、例えやりたいことが達成できたとしても、他人に評価してもらえない限りは未達の状態であり、その結果が気になって気力を消耗し続けます。\n私は上記のパターンに過去何度もハマり、やりたい TODO が達成できたはずなのに、その後 TODO に対する他人からの評価を常に気にし続けなければならず、無駄に気力が消費され続けて全くその後の行動が続いていかないという現象にハマっていました。\n例を言うと、例えば技術記事を書くとしたときに 「世間に新たな価値を提供したい」「誰かの役に立つような内容を意識する」「しっかり内容を書かないと記事にする意味が無い」 という他人から見られることを意識している動機づけは世間体や他者からの評価です。\nそれら一切度外視で、「文章書く練習のために書く」「後々また困りそうな技術的問題を自分が解決できるようにするために書く」「他の記事と内容が重複しても自分の勉強や理解を深めるために記事は書こう」 と自分の行動の動機を他人と切り離すことで、やりたい TODO を完了したときに「達成できた！満足！」という実感が即湧きます。\nやりたい TODO をちゃんと達成できたという実感は次の行動のモチベにも繋がっていくので、一石二鳥です。\nちなみに、やりたいことのモチベは 「なんとなく興味あってやりたいからやってみるけど飽きたら即止めよう」 という意識でも自分の行動の動機としては全然問題無かったです。そんな薄いモチベでも やり始めたら楽しくなってきてこだわり始めてやりたいことの精度や質がグングン上がっていき、継続して取り組める趣味となることも経験しました。\n自分の具体例で言うと、その程度のモチベで始めたことである普段飲むコーヒーの質向上が、結果、自家焙煎とか豆のブレンド比率にこだわり始めたところまで来ていて飽きずに今でも続けられています。\nその結果 Photo by Jorge Gonzalez on Unsplash\nそうして、やりたくない TODO の総量を減らして使える時間が増えていくと、最初のほうは時間を浪費していた時の行動となんら変わりない行動をしてしまうのですが、、ふとした瞬間に、それらの浪費行動に飽きるタイミングが来ます。暇すぎたり飽きに耐えきれなくなってきて、気力も十分に蓄えられてくると、ふとした瞬間に「やろうと思ってたことやってみるか」という感情に僕はなりました。\nむしろ、そういう感情になるまでは、仕事プライベート問わず TODO を減らし続けたり、遠慮なくダラケまくってボーッとしまくって休息する時間が必要なのかもしれません。\nあとは、気力は体力と同じく有限であるが、無くなっている状態が慢性的に続くと、それに気づくことすら出来ない。更に失った気力を回復するのには結構時間がかかる (1週間とかの単位) ということに気づくことが出来ました。\n私は気力を失っているかどうかの判断軸として、何かをやりたいと思えてたはずなのに「今は忙しいからな」とか「途中でやめそう」とか保守的なモードに入って行動が起こせなくなっていたら、気力が失われていると判断しています。 そうなっていたら、休息だけでなく TODO の総量も同時並行で減らすように心がけています。\n「2」がお題の時の web1week の時に参加出来なかったのは、まさにやりたいと思っていたのに、上記の保守的なモードに入っていたので行動に移せず終わってしまった感じでした。\nまた私は前述してきた内容を意識してきた結果、下記の以前からやりたいと思っていたことを自然に達成できました。\n ライブラリを作成して外部に公開できた ボリュームが大きいブログ記事を書くことができた OSS 活動に関わりライブラリの問題を解決することができた 自分のブログを作成して公開することができた  これらは世間体や他人の目、TODO などを気にせず、自分の意志のみにフォーカスして活動できるようになったことが大きく起因していると感じています。\nおわりに My pictures are CC0. When doing composings:によるPixabayからの画像\nまとめると、私にとっては下記が重要だったということになりそうです。\n 仕事やプライベートや大小や重要度関係なく TODO の総量はとにかく減らす  やりたいことがあっても時間や心労が原因でできていないのであれば、まずは自分のために使える時間を増やすことにだけ、ひたすら意識を傾ける   世間体や他者からの評価をモチベの糧にするような行動は決して起こさない  それで一時的に行動できていたとしても評価されないと気力を消費し続ける原因になってしまい、モチベや行動が後に続いていかない   気力の消費には自分で気づくことは難しいので常に余力が持てるよう意識し続ける  何かやりたいことがあるなら、他者に縛られる時間や仕事プライベートにおける TODO の総量を極力減らしていく   無意識に気力を削ぎ取ってくる外部要因は意識的に排除していく (フォロワーの数や SNS での反応等々)  今回 ノリでアドベントカレンダーに参加できたのも、上記が達成できていたからだと思います。(気力が十分にあって、世間体等を気にせず、自分が記事を書いてみたいと思ったから参加できた)\nもちろん、この記事が誰かの参考になれれば物凄く嬉しいは嬉しいのですが。。ｗ\nまた最近は行動を起こすにもインプットの量が足りてないとアウトプットしたいものも無くなってくるなと感じているので、自分が本当にやりたいことの総量を増やして、アウトプットを増やすことで人生を純粋に楽しむためにも、インプットの量を意識的に増やしていっています。\nちなみに私がモチベや記事に書いている内容を意識し始めたキッカケは教育心理学を学ぶ会で購入した理論と事例でわかるモチベーションですので、本記事内容にご興味お持ちいただいた方は是非チェックしてみてください。著者は kawagoi さん です。\n最後にこのような機会を用意してくれた だらさん に感謝して終わりたいと思います。\n","permalink":"https://nikaera.com/archives/birthday-2020/","summary":"はじめに まず、私の誕生日は 11/6 なので本日 12/6 は誕生日からちょうど 1ヶ月後になります。\n誕生日なので何か書くことにした✍️ | なんでもに参加しました！ https://t.co/1NXySfMlak #Crieit #crieit_advent_calendar @crieitcommunityより\n\u0026mdash; 𝚗𝚒𝚔𝚊𝚎𝚛𝚊 (@n1kaera) November 2, 2020  はい。完全にノリで参加しようと投稿予約してから、後日確認して気づきました。。ｗ しかし、その間違えに気づいてから、何を投稿しようかなあと考え始めた時に、ふと「以前の自分だったら絶対ノリでアドベントカレンダーに登録するっていう行動しないよなあ」ということに気づきました。\nそこで、今回は以前の自分と比較して、今の自分がどう変わったのか考察してみることにしました。この記事書いてるときは誕生日からまだ 1週間経ってないし、ちょうど考察するには良い機会かなと思って。\n以前の自分 Photo by Luis Villasmil on Unsplash\n以前はやりたいことがあっても全部後回しにしてしまっていました。何故かと言うと仕事やプライベートも含めて、他に直近でやらなければならない、もしくはそう思いこんでいる TODO があったときに、それらが残っていると次々に心配事や考え事が増えていってしまい、やりたいことに集中出来なくなるからです。\nつまり、やりたいことは本気で集中できるタイミングが来たらやろうと考えていました。\nそのため、読もうと思って買った本を読みたい、買って届いた新作ゲームをやりたい、OSS 活動をしてみたい、ライブラリを自分で作成してみたい等々のやりたいことは全て後回しになっていました。\n何も考えないでサクッと取り組めるプレイ済みのゲームを遊んだり、気に入っている Youtube の動画を何度も繰り返し見たりして時間を浪費していました。\nボーッとしながら何も考えないで過ごす時間や遊ぶ時間は気力を養う上で重要で必要だと認識しているのですが、それだけを余暇に費やすのは少し勿体ないなと感じていました。\nそして、ある時ふと気づきました。多分このままの生活してると連休とか仕事の区切りのタイミングとかでしか、自分がやりたいことに取り組むことはできないなと。。ちなみに、私はいわゆる ON/OFF みたいな切り替えが出来ないタイプなので、「仕事が終わったらスイッチを OFF にしてプライベートを全力で楽しむようにする」とかは無理なタイプです。\nそこで、過去自分が気分がノリ続けて作業できたり、新しいことに挑戦し始めたときのことを思い返しながら、現状改善のために簡単に出来ることからやってみようと 1年前くらい前から行動を起こし始めました。\n試したこと Free-PhotosによるPixabayからの画像\nまず、前提として下記項目はあえて頑張って解決するとかは諦めました。理由は中学生くらいの時から「そうだったなあ」って感じていることだったので潔く諦めた感じです。\n 仕事プライベート関係なく、やりたくない面倒な TODO を意識してしまい無駄に疲れる  ふとした瞬間に思い出して不快になるかつ、それらの TODO は後回しにしてしまいがちなので、更に不快になる機会が増えてきて、結果疲れ続ける   やるべき必要がある TODO が増えてくると最優先で迅速に全部片付けたくなる  それらが意識の端にあると心を休めた状態で日々を過ごすことが難しくなってきて、結果疲れ続ける    上記の解決を諦めても、やりたいことが自然と出来る環境が作れるようにしようとしました。","title":"📔 誕生日のノリでアドベントカレンダーに投稿予約して感じた変化"},{"content":"はじめに 本記事ではタイトルの内容に加えて ~/.ssh/config に対する基本的な理解を深めるため、各種設定項目に関する説明も付随して行います。GitHub を例に挙げてはいるものの、Bitbucket や GitLab、Git サーバへ接続する際にも利用可能な ~/.ssh/config の設定について記載しております。\n~/.ssh/config の書き方 まず GitHub へ接続する ~/.ssh/config の設定を見ていきます。GitHub で認証に使用する SSH キーは登録済みで秘密鍵は ~/.ssh/github へ配置している想定です。\nSSH キーが未登録の場合は 公式サイトの手順 に従って鍵の生成・登録までを行っておきます。\n# ~/.ssh/config Host github.com IdentityFile ~/.ssh/github User git ~/.ssh/config の各種設定項目は下記になります。\n   項目 説明     Host ホスト名を指定する   IdentityFile 接続時に使用する秘密鍵を指定する   User 接続時のユーザ名を指定する    設定を行うことで github.com に SSH 接続する際、ユーザ名に git が指定され、~/.ssh/github に存在する秘密鍵を用いて SSH 接続を試みるようになります。\n早速 github.com 接続時に正しく認証が通っているか確認するため、適当なプライベートリポジトリを git clone してみます。\nプライベートリポジトリを git clone した実行結果 (成功)\n無事に git clone できることが確認できたら成功です。\n接続先に応じて秘密鍵を使い分けたい GitHub で常に同一の秘密鍵を用いて認証を行う際は問題ないのですが、例えば GitHub アカウントをプライベート用と仕事用で使い分けていて、リポジトリ先に応じて秘密鍵を使い分けたいというケースはあると思います。\nその場合は ~/.ssh/config の設定と git のリモートリポジトリへの接続情報を変更することで対応可能です。\n例えばプライベート用の GitHub アカウント A と仕事用の GitHub アカウント B が存在するとします。その際 github-A がホストの時は A の秘密鍵を、github-B がホストの時は B の秘密鍵を利用するための設定は下記になります。\n# ~/.ssh/config # プライベート用の GitHub アカウントで利用する接続設定 Host github-A HostName github.com IdentityFile ~/.ssh/github-A User git # 仕事用の GitHub アカウントで利用する接続設定 Host github-B HostName github.com IdentityFile ~/.ssh/github-B User git 新たに HostName という項目を設定しました。HostName には接続先を指定します。HostName は何も指定しない場合 Host と同じ値が設定されます。\n今回は同一接続先に対してアカウントを切り替えたいので、Host にはアカウント識別可能な値 (ex: github-A, github-B, etc.) を、HostName に接続先である github.com を明示的に指定してアカウント毎に設定をわけました。\n次に git のリモートリポジトリへの接続情報を変更します。\n ~/.ssh/config に User を指定している場合は該当 Host へ接続時に自動的にユーザが設定されるため、接続 URL からユーザ名である git は取り除いても問題ありません。\n# 現在のリモートリポジトリ URL git remote -v origin git@github.com:nikaera/private-repository.git (fetch) origin git@github.com:nikaera/private-repository.git (push) # リモートリポジトリ URL を # git@github.com:nikaera/private-repository.git から # github-A:nikaera/private-repository.git に変更する # (ユーザ名の git は変更先から取り除いている) git remote set-url origin github-A:nikaera/private-repository.git # 新たに設定したリモートリポジトリ URL git remote -v origin github-A:nikaera/private-repository.git (fetch) origin github-A:nikaera/private-repository.git (push) この状態で先ほど git clone してきたプライベートリポジトリ内で git ls-remote origin コマンドを実行してみて正常に結果が取得できれば正しく設定できています。\nA の接続設定で git ls-remote origin の実行に成功する\n次に B のアカウント情報で接続を試みます。\n# 現在のリモートリポジトリ URL git remote -v origin github-A:nikaera/private-repository.git (fetch) origin github-A:nikaera/private-repository.git (push) # リモートリポジトリ URL を github-A:nikaera/private-repository.git から # github-B:nikaera/private-repository.git に変更する git remote set-url origin github-B:nikaera/private-repository.git # 新たに設定したリモートリポジトリ URL git remote -v origin github-B:nikaera/private-repository.git (fetch) origin github-B:nikaera/private-repository.git (push) B には先ほどのプライベートリポジトリの読み取り権限が無い状態で、git ls-remote origin コマンドを実行してみると失敗するはずです。\nB の接続設定で git ls-remote origin の実行に失敗する\nこれで今後は git のリモートリポジトリ URL を一度書き換えておくだけで、それぞれ適切な秘密鍵で GitHub 認証する設定ができました。\n単に Git サーバに SSH 接続する際の設定情報としては、上記項目を抑えておけば問題無いです。しかし、他にも設定したほうが良い項目や、場合によっては設定が必要な項目も存在します。\n基本的に設定しておいた方が良い項目 他に設定しておいたほうが良い項目には下記があります。\n# ~/.ssh/config Host github.com IdentityFile ~/.ssh/github User git IdentitiesOnly yes # IdentityFile で指定した秘密鍵でのみ認証を試みる Compression yes # Git でのファイル転送時に圧縮する    項目 説明 型     IdentitiesOnly IdentityFile で指定した鍵ファイルでのみ認証を行うかどうか指定する boolean (yes or no)   Compression 圧縮転送を行うかどうか指定する boolean (yes or no)    IdentityFile で指定したファイル以外で認証が通ってしまう状況だと、意図したアカウントで適切に認証が通せていない可能性が出てきてしまいます。\nそのため、特別な理由がなければ IdentitiesOnly には yes を設定しておいたほうが良いです。\nIdentitiesOnly を no に設定しておくと ssh-add で登録したキーの中からも認証を試みるようになります。\nまた、Compression についても yes に設定しておくことをオススメします。プログラミングを行っているプロジェクトでは、テキストファイルの数が大半を占めるはずなので、圧縮転送を有効にしておいたほうがファイル転送の速度向上を見込めるからです。\nただし Compression は大容量の単一のファイルがアップされているなどして、圧縮効率の悪いファイルを含んでいるプロジェクトの場合は逆に転送速度が低下する恐れもあるので要注意です。\n接続先によっては設定が必要な項目 接続先に応じて設定する必要がありそうな項目には下記があります。\n# ~/.ssh/config Host github.com IdentityFile ~/.ssh/github User git Port 12345 # 接続先のポートが 22 以外の場合指定する    項目 説明 型     Port SSH 接続時のポート番号を設定する number (ex: 22, 22222, etc.)    例えば自前で用意した Git リポジトリに対して接続する場合、SSH のポートがセキュリティ上の理由等でウェルノウンポートの 22 ではない場合がありえます。その場合は Port を明示的に指定することで接続時のポート番号を適切な値に設定しておく必要が出てくるでしょう。\nおわりに 今回は GitHub へ SSH 接続する際の例を元に ~/.ssh/config の設定方法について書きました。~/.ssh/config で Host の設定は行っているものの、それに応じて git のリモートリポジトリ URL を変更すべき場合もあることを明示している記事が少なかったので書きました。\nまた、本記事では Git 接続時の ~/.ssh/config の設定にフォーカスを当てましたが、~/.ssh/config についてはサーバに SSH 接続して作業する際に設定しておくと良い項目等もあります。\n~/.ssh/config の各種設定項目について理解していると頻繁に実行するコマンドのコスト削減ができて便利です。特にこちらの記事はとても参考になりました。\n参考リンク  GitHub に SSH で接続する - GitHub Docs SSH 接続をテストする - GitHub Docs ~/.ssh/config による快適 SSH 環境 - Qiita  ","permalink":"https://nikaera.com/archives/ssh-config-github/","summary":"はじめに 本記事ではタイトルの内容に加えて ~/.ssh/config に対する基本的な理解を深めるため、各種設定項目に関する説明も付随して行います。GitHub を例に挙げてはいるものの、Bitbucket や GitLab、Git サーバへ接続する際にも利用可能な ~/.ssh/config の設定について記載しております。\n~/.ssh/config の書き方 まず GitHub へ接続する ~/.ssh/config の設定を見ていきます。GitHub で認証に使用する SSH キーは登録済みで秘密鍵は ~/.ssh/github へ配置している想定です。\nSSH キーが未登録の場合は 公式サイトの手順 に従って鍵の生成・登録までを行っておきます。\n# ~/.ssh/config Host github.com IdentityFile ~/.ssh/github User git ~/.ssh/config の各種設定項目は下記になります。\n   項目 説明     Host ホスト名を指定する   IdentityFile 接続時に使用する秘密鍵を指定する   User 接続時のユーザ名を指定する    設定を行うことで github.com に SSH 接続する際、ユーザ名に git が指定され、~/.ssh/github に存在する秘密鍵を用いて SSH 接続を試みるようになります。\n早速 github.com 接続時に正しく認証が通っているか確認するため、適当なプライベートリポジトリを git clone してみます。","title":"📔 GitHub 接続時の ~/.ssh/config の書き方"},{"content":"はじめに Serverless Framework を使っていて、度々デプロイ時に手動で設定していた作業内容を自動化したいなと思い、プラグイン作成の知識習得も兼ねてライブラリを作成し NPM で公開してみました。\nhttps://www.npmjs.com/package/serverless-amplify-auth\n今後も開発する可能性はありそうなので Serverless のプラグインを TypeScript で作成する際の手順をまとめておきました。各手順はザックリと紹介しつつ、主にその過程でハマった点や工夫した点に重きをおいて記事を書いていきます。\n動作環境  Node.js 12.19.0 Serverless Framework  Framework Core: 2.10.0 Plugin: 4.1.1 SDK: 2.3.2 Components: 3.3.0    開発環境を整える 本記事の内容を最後まで実践した際の最終的なプロジェクトのディレクトリ構造は下記になります。\ntree -I node_modules -L 2 ./ ./ ├── example # ライブラリの動作検証用のサンプルコードを配置するフォルダ │ ├── handler.js │ ├── package.json │ └── serverless.yml ├── lib # src フォルダ内のファイルをコンパイルした結果を配置するフォルダ (ライブラリとして利用する際に含まれるソースコード群) │ ├── index.js │ └── index.js.map ├── package-lock.json ├── package.json ├── src # Serverless プラグインのソースコードを配置するフォルダ │ └── index.ts └── tsconfig.json 基本的には TypeScript で Serverless Framework の Plugin を書いてみる | Developers.IO の手順をなぞっていくだけで環境構築自体は可能です。そこで、ここでは自分なりに工夫した箇所について記載していきます。\nまずは、開発に必要なパッケージを下記コマンドでまとめてインストールします。\n# TypeScript の開発に必要なパッケージインストール npm i -D typescript # TypeScript の型定義ファイルのインストール npm i -D @types/node @types/serverless # 今回は AWS プロバイダー向けの開発を行うため SDK をインストールする npm i --save aws-sdk TypeScript のコンパイル時に必要となる tsconfig.json は下記のように設定しました。\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es6\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;commonjs\u0026#34;, \u0026#34;moduleResolution\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;strict\u0026#34;: true, \u0026#34;strictBindCallApply\u0026#34;: false, \u0026#34;strictNullChecks\u0026#34;: false, \u0026#34;outDir\u0026#34;: \u0026#34;lib\u0026#34;, \u0026#34;sourceMap\u0026#34;: true }, \u0026#34;include\u0026#34;: [\u0026#34;src/**/*\u0026#34;] } compilerOptions.strict には true を設定しつつ、compilerOptions.strictNullChecks 等には false を設定することで、部分的に TypeScript のコンパイルチェックを外すようにしました。\noutDir には lib を指定することで、コンパイルされた TypeScript ファイルは lib フォルダに出力されるよう設定しました。\ninclude には src/**/* を明示的に指定しており、src フォルダ内の全ファイルをコンパイル対象にしております。\n package.json の内容は部分的に抜粋し、説明が必要そうな項目について説明いたします。 全容を把握したい方は こちら からご確認いただけます。\n{ \u0026#34;main\u0026#34;: \u0026#34;lib/index.js\u0026#34;, \u0026#34;files\u0026#34;: [\u0026#34;lib\u0026#34;], \u0026#34;scripts\u0026#34;: { \u0026#34;build\u0026#34;: \u0026#34;rm -rf lib \u0026amp;\u0026amp; tsc\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; } } main には src/index.ts をコンパイルすると生成される lib/index.js を指定しました。そのため、ライブラリのエントリーポイントは lib/index.js が設定されます。\nfiles には lib フォルダを指定することで、TypeScript をコンパイルした結果のみがライブラリのソースコードとして取り込まれるようになります。\nServerless プラグインの開発を進める 開発環境が整ったところで早速 Serverless Plugin のソースコードを書いていきます。TypeScript のソースコードは src/index.ts に配置します。\nServerless プラグインのプログラムを書く // src/index.ts  import * as Serverless from \u0026#34;serverless\u0026#34;; import { SharedIniFileCredentials, config } from \u0026#34;aws-sdk\u0026#34;; /** * serverless.yml の custom property の型定義 */ interface Variables { value1: string; value2: number; value3: boolean; profile?: string; } export default class Plugin { serverless: Serverless; options: Serverless.Options; hooks: { [event: string]: () =\u0026gt; Promise\u0026lt;void\u0026gt;; }; variables: Variables; /** * プラグインの初期化関数。 * 注意点として、初期化関数内では serverless.yml 内の変数展開が行われないので、 * ${ssm:~} 等で設定した値を呼び出しても、適切に値が設定されない状態で呼び出すことになる。 */ constructor(serverless: Serverless, options: Serverless.Options) { this.serverless = serverless; this.options = options; /** * serverless.service.custom 内の特定プロパティを取得するための記述 * 今回は Serverless のプラグイン名に serverless-typescript を設定したため、 * serverless-typescript 文字列をキーとして指定する。 */ this.variables = serverless.service.custom[\u0026#34;serverless-typescript\u0026#34;]; /** * プラグインがフックする関数を指定する。複数指定することも可能だが、 * 今回は before:package:createDeploymentArtifacts を指定して、 * パッケージングの手前の処理を定義した run 関数でフックする。 */ this.hooks = { \u0026#34;before:package:createDeploymentArtifacts\u0026#34;: this.run.bind(this), }; } /** * before:package:createDeploymentArtifacts 時に実行される関数 */ async run() { /** * プラグイン実行時に必要となるフィールドがセットされていなければ処理をスキップする */ if (!this.variables) { this.serverless.cli.log( `serverless-typescript: Set the custom.serverless-typescript field to an appropriate value.` ); return; } /** * this.serverless.getProvider 関数を用いることで、 * デプロイ時のアカウントの各種情報について取得することが出来る */ const awsProvider = this.serverless.getProvider(\u0026#34;aws\u0026#34;); const region = await awsProvider.getRegion(); const accountId = await awsProvider.getAccountId(); const stage = await awsProvider.getStage(); /** * serverless.yml で指定した値や AWS 情報が取得できているか、 * 確認するために標準出力する */ this.serverless.cli.log( `serverless-typescript values: ${JSON.stringify({ stage: stage, region: region, accountId: accountId, variables: this.variables, })}` ); /** * プラグイン内で処理を実行する際、別の特定 Profile を用いたい際は、 * AWS SDK の SharedIniFileCredentials を用いて切り替えると楽に切替可能。 * その際は process.env.AWS_SDK_LOAD_CONFIG に値を設定しておくこと */ if (this.variables.profile) { process.env.AWS_SDK_LOAD_CONFIG = \u0026#34;true\u0026#34;; const credentials = new SharedIniFileCredentials({ profile: this.variables.profile, }); config.credentials = credentials; } } } module.exports = Plugin; ソースコード内にいくつかコメントを残しましたが、何点か補足の説明をしていきます。\nserverless.service.custom['serverless-typescript'] を呼び出すことで、serverless.yml 内の下記の記述内容を Object として取得できます。\n# serverless.yml custom: # custom.serverless-typescript 内の定義を Object として取得可能 serverless-typescript: value1: \u0026#34;value1\u0026#34; value2: 0 value3: true # profile: default (optional) this.hooks には必要に応じてフックを指定します。フックの書き方については 公式ドキュメント に詳細が記載されています。フックの種類については Gist でまとめてくださっている方がいました。\nthis.serverless.getProvider('aws') を用いることで、デプロイ時にアカウントの各種情報について取得することが出来ます。この記述を利用することで Serverless Pseudo Parameters のようなシンタックスを自身のプラグインに取り込むことが可能になります。 私が作成したプラグインでも serverless.yml で ARN を構築する際に利用していて、index.ts 内で利用しました。\nまた、プラグイン内でデプロイ時とは異なる Profile を使用したいケースもあるかと存じます。それは AWS SDK の SharedIniFileCredentials を用いることで簡易に実装できました。\n 注意点として、SharedIniFileCredentials を用いてプロファイルを切り替える時は、環境変数に AWS_SDK_LOAD_CONFIG=\u0026ldquo;true\u0026rdquo; を設定する必要がありました。 設定しないと ConfigError: Missing region in config というエラーが発生してしまい、プロファイルを切り替えることが出来ませんでした。\n それでは、次にプラグインの動作検証用コードを example フォルダに配置していきます。\nServerless プラグインの動作検証用プログラムを書く example フォルダ内には検証用プロジェクトを作成するので、その前準備として example/package.json を作成します。\n# package.json ファイルを作成する cd example \u0026amp;\u0026amp; npm init -y example/package.json ファイルを作成したら開発用のスクリプトを example/package.json に追記します。\n{ \u0026#34;scripts\u0026#34;: { \u0026#34;prestart\u0026#34;: \u0026#34;cd ../ \u0026amp;\u0026amp; npm run build\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;sls package\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; } } scripts 内の prestart は start スクリプト実行前に実行されるスクリプトです。npm start を実行すると prestart でプラグインの build タスクを実行した後、 Serverless Framework のパッケージングを行うことでプラグインの動作確認が行えます。\n 今回は Serverless の before:package:createDeploymentArtifacts フックを利用しているので、sls package コマンドで動作検証が可能となっています。before:deploy:deploy 等のデプロイ中に実行されるフックを利用する際は sls deploy --noDeploy コマンド等で動作検証を行う必要があります。\n 次に動作検証用の serverless.yml を example フォルダに配置します。\n# serverless.yml service: name: serverless-typescript publish: false # プラグイン内で利用する設定値を定義する custom: serverless-typescript: value1: \u0026#34;value1\u0026#34; value2: 0 value3: true profile: custom_profile provider: name: aws runtime: nodejs12.x region: ap-northeast-1 # プラグインのパスを指定して読み込む plugins: localPath: \u0026#34;../../\u0026#34; modules: - serverless-typescript # 何でも良いので動作検証用の関数を定義する (関数の定義は後述) functions: hello: handler: handler.hello example フォルダ内に handler.js を配置して functions.hello.handler で用いる検証用の関数を定義します。\n// example/handler.js  \u0026#34;use strict\u0026#34;; // 検証用の関数。serverless.yml 内では handler.hello で参照可能 module.exports.hello = (event, context, callback) =\u0026gt; { callback(null, { statusCode: 200, body: \u0026#34;Hello World!\u0026#34;, }); }; 上記作業が完了次第、cd example \u0026amp;\u0026amp; npm start を実行して動作検証してみます。\ncd example \u0026amp;\u0026amp; npm start \u0026gt; example@1.0.0 prestart /Users/nika/Desktop/serverless-typescript/example \u0026gt; cd ../ \u0026amp;\u0026amp; npm run build \u0026gt; serverless-typescript@1.0.0 build /Users/nika/Desktop/serverless-typescript \u0026gt; rm -rf lib \u0026amp;\u0026amp; tsc \u0026gt; example@1.0.0 start /Users/nika/Desktop/serverless-typescript/example \u0026gt; sls package Serverless: Configuration warning at \u0026#39;service\u0026#39;: unrecognized property \u0026#39;publish\u0026#39; Serverless: Serverless: Learn more about configuration validation here: http://slss.io/configuration-validation Serverless: # src/index.ts 内の this.serverless.cli.log の出力内容 # 各種値が正常にセットされていることが確認出来る Serverless: serverless-typescript values: {\u0026#34;stage\u0026#34;:\u0026#34;dev\u0026#34;,\u0026#34;region\u0026#34;:\u0026#34;ap-northeast-1\u0026#34;,\u0026#34;accountId\u0026#34;:\u0026#34;XXXXXXXXXX\u0026#34;,\u0026#34;variables\u0026#34;:{\u0026#34;value1\u0026#34;:\u0026#34;value1\u0026#34;,\u0026#34;value2\u0026#34;:0,\u0026#34;value3\u0026#34;:true,\u0026#34;profile\u0026#34;:\u0026#34;custom_profile\u0026#34;}} Serverless: Packaging service... Serverless: Excluding development dependencies... 標準出力にあるプラグイン内で出力したログから、適切に値が取得出来ていることが確認出来れば OK です。\nAWS Profile の切り替えができるか確認してみる Serverless プラグインでの Profile の切り替えについて、動作検証がまだ出来ていないので確認していきます。\nserverless.yml 内の custom.serverless-typescript.profile に設定箇所は既に用意してあるので、~/.aws/credentials に実在する Profile 名を指定します。\n# serverless.yml (一部抜粋) custom: serverless-typescript: profile: \u0026lt;プラグイン実行時に使用したい Profile 名\u0026gt; 動作検証のため、src/index.ts 内にログ出力の記述を加えます。\n// src/index.ts  import * as Serverless from \u0026#34;serverless\u0026#34;; import { SharedIniFileCredentials, config } from \u0026#34;aws-sdk\u0026#34;; interface Variables { value1: string; value2: number; value3: boolean; profile?: string; } export default class Plugin { serverless: Serverless; options: Serverless.Options; hooks: { [event: string]: () =\u0026gt; Promise\u0026lt;void\u0026gt;; }; variables: Variables; constructor(serverless: Serverless, options: Serverless.Options) { this.serverless = serverless; this.options = options; this.variables = serverless.service.custom[\u0026#34;serverless-typescript\u0026#34;]; this.hooks = { \u0026#34;before:package:createDeploymentArtifacts\u0026#34;: this.run.bind(this), }; } async run() { if (!this.variables) { this.serverless.cli.log( `serverless-typescript: Set the custom.serverless-typescript field to an appropriate value.` ); return; } const awsProvider = this.serverless.getProvider(\u0026#34;aws\u0026#34;); const region = await awsProvider.getRegion(); const accountId = await awsProvider.getAccountId(); const stage = await awsProvider.getStage(); this.serverless.cli.log( `serverless-typescript values: ${JSON.stringify({ stage: stage, region: region, accountId: accountId, variables: this.variables, })}` ); if (this.variables.profile) { process.env.AWS_SDK_LOAD_CONFIG = \u0026#34;true\u0026#34;; const credentials = new SharedIniFileCredentials({ profile: this.variables.profile, }); config.credentials = credentials; // Profile が切り替えられたか確認するためにログを出力する  this.serverless.cli.log( `serverless-typescript profile: ${JSON.stringify(config.credentials)}` ); } } } module.exports = Plugin; 早速 cd example \u0026amp;\u0026amp; npm start を実行して正常に profile が切り替えられていそうか確認してみます。\n# 成功時の実行結果 cd example \u0026amp;\u0026amp; npm start # ... # accessKeyId のフィールドに ~/.aws/credentials 内に存在する値が出力されている Serverless: serverless-typescript profile: {\u0026#34;expired\u0026#34;:false,\u0026#34;expireTime\u0026#34;:null,\u0026#34;refreshCallbacks\u0026#34;:[],\u0026#34;accessKeyId\u0026#34;:\u0026#34;XXXXXXXXXXXXXX\u0026#34;,\u0026#34;profile\u0026#34;:\u0026#34;XXXXXXXXXXXXXX\u0026#34;,\u0026#34;disableAssumeRole\u0026#34;:false,\u0026#34;preferStaticCredentials\u0026#34;:false,\u0026#34;tokenCodeFn\u0026#34;:null,\u0026#34;httpOptions\u0026#34;:null} # ... ちなみに存在しない Profile を指定した場合の出力は下記のようになります。\n# 失敗時の実行結果 cd example \u0026amp;\u0026amp; npm start # ... # accessKeyId のフィールドが存在しない時は Profile が正しく設定出来ていない Serverless: serverless-typescript profile: {\u0026#34;expired\u0026#34;:false,\u0026#34;expireTime\u0026#34;:null,\u0026#34;refreshCallbacks\u0026#34;:[],\u0026#34;profile\u0026#34;:\u0026#34;custom_profile\u0026#34;,\u0026#34;disableAssumeRole\u0026#34;:false,\u0026#34;preferStaticCredentials\u0026#34;:false,\u0026#34;tokenCodeFn\u0026#34;:null,\u0026#34;httpOptions\u0026#34;:null} # ... おわりに 今回初めて Serverless プラグインの開発をしてみて、手軽に出来ることが分かったので自動化出来そうな作業は積極的にプラグイン化していきたいなと感じました。\nプラグイン化した後は Git リポジトリにアップするだけでなく、NPM のパッケージ や GitHub Packages として公開しておくと、後々プラグインを利用する際に便利です。また、公開してライブラリのスタッツを見るのは案外楽しく開発のモチベーションにも繋がるのでオススメです。\n参考リンク  TypeScript で Serverless Framework の Plugin を書いてみる | Developers.IO typescript 導入した private な npm パッケージの作り方 - 30 歳 SIer から WEB エンジニアで奮闘 How To Write Your First Plugin For The Serverless Framework - Part 1  ","permalink":"https://nikaera.com/archives/serverless-typescript-plugin/","summary":"はじめに Serverless Framework を使っていて、度々デプロイ時に手動で設定していた作業内容を自動化したいなと思い、プラグイン作成の知識習得も兼ねてライブラリを作成し NPM で公開してみました。\nhttps://www.npmjs.com/package/serverless-amplify-auth\n今後も開発する可能性はありそうなので Serverless のプラグインを TypeScript で作成する際の手順をまとめておきました。各手順はザックリと紹介しつつ、主にその過程でハマった点や工夫した点に重きをおいて記事を書いていきます。\n動作環境  Node.js 12.19.0 Serverless Framework  Framework Core: 2.10.0 Plugin: 4.1.1 SDK: 2.3.2 Components: 3.3.0    開発環境を整える 本記事の内容を最後まで実践した際の最終的なプロジェクトのディレクトリ構造は下記になります。\ntree -I node_modules -L 2 ./ ./ ├── example # ライブラリの動作検証用のサンプルコードを配置するフォルダ │ ├── handler.js │ ├── package.json │ └── serverless.yml ├── lib # src フォルダ内のファイルをコンパイルした結果を配置するフォルダ (ライブラリとして利用する際に含まれるソースコード群) │ ├── index.js │ └── index.js.map ├── package-lock.json ├── package.json ├── src # Serverless プラグインのソースコードを配置するフォルダ │ └── index.","title":"📔 Serverless のプラグインを TypeScript で作成する方法"},{"content":"Blog 📝  Qiita  軽めの内容の記事を書くのに Qiita を利用しています   Zenn  整理整頓した記事を書くのに Zenn を利用しています    What I made 🔨  Teemo 💕  📔 チャットの短文作成に便利な Chrome 拡張機能を開発してみた   Bloggimg  📔 ブログを書く用途に特化した Gyazo のツールを開発してみた   Zenn.badge  Zenn のスコアを GitHub 風のバッジに変換するサービス   selekted.club  tumblr の音と映像のフィードを連続再生できる Web サービス   立体ホームラン競争  web1week の時に作成した Web で出来る 3D ゲーム   チャレンジカレンダー  web1week の時に作成した 30Days Challenge が作成できる Web サービス   精密適正テスト  unity1week の時に作成したゲーム   バーチャル食べ放題  unity1week の時に作成したゲーム    ","permalink":"https://nikaera.com/portfolio/","summary":"Blog 📝  Qiita  軽めの内容の記事を書くのに Qiita を利用しています   Zenn  整理整頓した記事を書くのに Zenn を利用しています    What I made 🔨  Teemo 💕  📔 チャットの短文作成に便利な Chrome 拡張機能を開発してみた   Bloggimg  📔 ブログを書く用途に特化した Gyazo のツールを開発してみた   Zenn.badge  Zenn のスコアを GitHub 風のバッジに変換するサービス   selekted.club  tumblr の音と映像のフィードを連続再生できる Web サービス   立体ホームラン競争  web1week の時に作成した Web で出来る 3D ゲーム   チャレンジカレンダー  web1week の時に作成した 30Days Challenge が作成できる Web サービス   精密適正テスト  unity1week の時に作成したゲーム   バーチャル食べ放題  unity1week の時に作成したゲーム    ","title":"Portfolio 💼"},{"content":"開発者として 🔨 大学院時代は友人 2 人と一緒にチームを組んで Android/iPhone アプリ開発や Web サービス開発をしていました。また、受託として Android/iPhone アプリ開発を請け負っていました。そのおかげで今でも Objective-C はある程度は空で書けます。\n現在は AWS を用いたバックエンドの開発を主に行っています。様々な技術領域に興味がありゼネラリストの指向性が強いです。実際に人に使ってもらえるプロダクトを作ることや社会貢献可能なプロジェクトに参加することに興味があります。\n開発それ自体を楽しめる気質はあるものの、基本的には開発を課題解決のための手段として捉えています。0 からプロジェクトに関わる様々なことに携わりたいタイプの気質が強いエンジニアです。また、個々が苦手な部分を補いあいながら、遠慮なく改善点を指摘しあえて、楽しく心理的に安全に開発できるチームが好きです。\n個人開発はプロトレベルで自分が作ってみたいものを思いついたら作るみたいな感じでやってます。あとは web1week や unity1week に時間があれば参加しています。作ったものは基本全部 GitHub にアップしています。そこで得た知見は他の人のためにもなれれば嬉しいなということで、Qiita や Zenn にまとめています。\n1 つの会社に所属して個人で技術力を高めていくことに限界を感じているので、開発チームへのお誘い等あれば、是非 Contact からお誘い頂けると嬉しいです。 ひととなりは 個人として からご確認いただけます。\nまた、あまり有効活用できておりませんが Lapras を利用しています。自分の能力値が可視化される感じがゲームやってるみたいで楽しいです。\n個人として 🚶‍♂️ 長男です。人生で大切なことは大体ゲームから学びました。小学生の時から引きこもって仕事できる環境に憧れていました。現在は無事それが叶う環境で働けていて満足しています。ガラスのハートの持ち主で HSP の気質があるかもしれません。\n特に下記の傾向が顕著にあります。\n 動揺するような状況を避けることを、普段の生活で最優先している 仕事をする時、競争させられたり、観察されていると、緊張し、いつもの実力を発揮できなくなる   社会人になってからは、会食恐怖症 の気も出てきましたが、なんとか元気にやってます。このような人間なので、仕事もプライベートもできるだけストレスが無い状態を保つよう心がけています。生きるの大変。\n最近は自己開示して積極的に弱みをさらけ出し、周囲の理解得ることでだいぶ生きやすくはなってきました。更に自己開示の副次的効果として、あまり動揺するようなことも無くなってきました。\n基本的に孤独を愛する傾向にありますが、Death Stranding をプレイしてからは 考え方は若干変わりました 。気の合う大切な仲間との関係性にフォーカスして仲を深くすることを大切にするタイプです。\n好きなこと 🌟 趣味はコンシューマーゲームと音楽とコーヒーで、ベースギターと読書と個人開発も好きです。\nゲームのジャンルは雑多に好きで PlayStation Store とか My Nintendo Store とか酒飲んで流し見しながら、インディーゲーとか含めて頻繁にジャケ買いしたりしています。最近はそのせいで積みゲーが増えているのが悩みです。\n音楽もジャンルは雑多に好きですが、ずっと昔から好きなアーティストに Nikakoi と erast がいます。そう考えると IDM のジャンルが特に好きかもしれないです。気になる音楽のインプット/アウトプットは tumblr で行っています。\nコーヒーは大好きで 1 日に 5 杯とか飲みます。生豆を買って自家焙煎も行っていますが、迂闊にもストックしていた生豆が切れてしまったときは届くまでの繋ぎとして やなか珈琲 とかに豆を買いに行ったりします。つまり、やなか珈琲の豆が好みです。\nベースギターは社会人になってから始めました。ベースギターを始めた動機は近所迷惑にならなそうだったからです。あと裏方が好きなのも関係しているかもしれません。プレイヤーとしては Joe Dart に憧れてます。 エレキギターは高校の時から社会人になるまではやっていました。\n読書はジャンルとしてはハード SF がめさくさ好きです。実用書や技術書も好きで読みますが、長時間読んでも飽きずに永遠に読み続けられるのはハード SF です。今まで読んだ本は 読書メーター にまとめています。\n","permalink":"https://nikaera.com/profile/","summary":"開発者として 🔨 大学院時代は友人 2 人と一緒にチームを組んで Android/iPhone アプリ開発や Web サービス開発をしていました。また、受託として Android/iPhone アプリ開発を請け負っていました。そのおかげで今でも Objective-C はある程度は空で書けます。\n現在は AWS を用いたバックエンドの開発を主に行っています。様々な技術領域に興味がありゼネラリストの指向性が強いです。実際に人に使ってもらえるプロダクトを作ることや社会貢献可能なプロジェクトに参加することに興味があります。\n開発それ自体を楽しめる気質はあるものの、基本的には開発を課題解決のための手段として捉えています。0 からプロジェクトに関わる様々なことに携わりたいタイプの気質が強いエンジニアです。また、個々が苦手な部分を補いあいながら、遠慮なく改善点を指摘しあえて、楽しく心理的に安全に開発できるチームが好きです。\n個人開発はプロトレベルで自分が作ってみたいものを思いついたら作るみたいな感じでやってます。あとは web1week や unity1week に時間があれば参加しています。作ったものは基本全部 GitHub にアップしています。そこで得た知見は他の人のためにもなれれば嬉しいなということで、Qiita や Zenn にまとめています。\n1 つの会社に所属して個人で技術力を高めていくことに限界を感じているので、開発チームへのお誘い等あれば、是非 Contact からお誘い頂けると嬉しいです。 ひととなりは 個人として からご確認いただけます。\nまた、あまり有効活用できておりませんが Lapras を利用しています。自分の能力値が可視化される感じがゲームやってるみたいで楽しいです。\n個人として 🚶‍♂️ 長男です。人生で大切なことは大体ゲームから学びました。小学生の時から引きこもって仕事できる環境に憧れていました。現在は無事それが叶う環境で働けていて満足しています。ガラスのハートの持ち主で HSP の気質があるかもしれません。\n特に下記の傾向が顕著にあります。\n 動揺するような状況を避けることを、普段の生活で最優先している 仕事をする時、競争させられたり、観察されていると、緊張し、いつもの実力を発揮できなくなる   社会人になってからは、会食恐怖症 の気も出てきましたが、なんとか元気にやってます。このような人間なので、仕事もプライベートもできるだけストレスが無い状態を保つよう心がけています。生きるの大変。\n最近は自己開示して積極的に弱みをさらけ出し、周囲の理解得ることでだいぶ生きやすくはなってきました。更に自己開示の副次的効果として、あまり動揺するようなことも無くなってきました。\n基本的に孤独を愛する傾向にありますが、Death Stranding をプレイしてからは 考え方は若干変わりました 。気の合う大切な仲間との関係性にフォーカスして仲を深くすることを大切にするタイプです。\n好きなこと 🌟 趣味はコンシューマーゲームと音楽とコーヒーで、ベースギターと読書と個人開発も好きです。\nゲームのジャンルは雑多に好きで PlayStation Store とか My Nintendo Store とか酒飲んで流し見しながら、インディーゲーとか含めて頻繁にジャケ買いしたりしています。最近はそのせいで積みゲーが増えているのが悩みです。\n音楽もジャンルは雑多に好きですが、ずっと昔から好きなアーティストに Nikakoi と erast がいます。そう考えると IDM のジャンルが特に好きかもしれないです。気になる音楽のインプット/アウトプットは tumblr で行っています。","title":"Profile 👦"},{"content":"","permalink":"https://nikaera.com/contact/","summary":"nikaera への問い合わせページ","title":"Contact 📥"},{"content":"","permalink":"https://nikaera.com/rss_feeds/","summary":"自分の技術記事の RSS Feed","title":"RSS Feeds 🔖"}]